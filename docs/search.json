[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Force-plate signal processing for balance analysis — a practical tutorial\nInteractive Introduction to Image Processing"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Open IntroductionImageProcessing in Colab\n\nThis site can include Jupyter notebooks (.ipynb) in two common ways.\n\nRender the notebook as a standalone page using Quarto\n\nPlace notebooks in a notebooks/ folder (e.g. notebooks/example.ipynb).\nRender it with quarto render notebooks/example.ipynb which will produce an HTML page in _site/ (or the site output folder).\nLink to the produced HTML from your site navigation or pages.\n\nInclude notebook content inside a .qmd page (convert or execute)\n\nYou can include the notebook output by converting it to Quarto-compatible content or by embedding the HTML output.\nExample: in a .qmd file you can use an iframe to embed the rendered notebook HTML:\n\n\n&lt;iframe src=\"notebooks/example.html\" style=\"width:100%;height:800px;border:0\"&gt;&lt;/iframe&gt;\nTips:\n\nUse quarto render to convert .ipynb to HTML (Quarto will execute notebooks when rendering if execution is enabled).\nEnsure any data or resources used by the notebook are committed or in paths resolvable from the site root.\nIf you prefer dynamic execution during site builds, enable execute: true in the notebook’s YAML or in _quarto.yml.\n\nIf you want, I can also add a small example notebooks/example.ipynb and a build script to automatically render notebooks during site build. title: Notebooks title: Notebooks format: html\n\n\nOpen IntroductionImageProcessing in Colab Launch on Binder Open in JupyterLite (if available)\n\n\n\nOpen IntroductionImageProcessing in Colab\n\nThis site can include Jupyter notebooks (.ipynb) in two common ways.\n\nRender the notebook as a standalone page using Quarto\n\nPlace notebooks in a notebooks/ folder (e.g. notebooks/example.ipynb).\nRender it with quarto render notebooks/example.ipynb which will produce an HTML page in _site/ (or the site output folder).\nLink to the produced HTML from your site navigation or pages.\n\nInclude notebook content inside a .qmd page (convert or execute)\n\nYou can include the notebook output by converting it to Quarto-compatible content or by embedding the HTML output.\nExample: in a .qmd file you can use an iframe to embed the rendered notebook HTML:\n\n\n&lt;iframe src=\"notebooks/example.html\" style=\"width:100%;height:800px;border:0\"&gt;&lt;/iframe&gt;\nTips:\n\nUse quarto render to convert .ipynb to HTML (Quarto will execute notebooks when rendering if execution is enabled).\nEnsure any data or resources used by the notebook are committed or in paths resolvable from the site root.\nIf you prefer dynamic execution during site builds, enable execute: true in the notebook’s YAML or in _quarto.yml.\n\nIf you want, I can also add a small example notebooks/example.ipynb and a build script to automatically render notebooks during site build."
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html",
    "href": "notebooks/BalanceSignalProcessingTutorial.html",
    "title": "Force-plate signal processing for balance analysis — a practical tutorial",
    "section": "",
    "text": "Open in Colab"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#convert-measured-voltages-to-forces-and-moments",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#convert-measured-voltages-to-forces-and-moments",
    "title": "Force-plate signal processing for balance analysis — a practical tutorial",
    "section": "Convert measured voltages to forces and moments",
    "text": "Convert measured voltages to forces and moments\nThis section shows how to convert the six measured channel voltages from the force plate into physical forces and moments. We provide the inverted sensitivity matrix (B), the amplifier gain (G), and the input voltage (V_0). The conversion is applied row-wise and vectorized in the code cells that follow.\nWhat this section does - Converts raw channel voltages \\((V_{Fx}, V_{Fy}, V_{Fz}, V_{Mx}, V_{My}, V_{Mz})\\) → forces/moments \\((F_x, F_y, F_z, M_x, M_y, M_z)\\) in SI units. - Stores results as new columns in the existing dataframes for use in plotting and analysis.\nFormula (applied per time-point):\n\\[\\vec{Y} = \\frac{10^6}{G V_0} \\vec{V} B^T\\]\nWhere: - (\\(\\vec{V}\\)) is the 1×6 voltage row vector at a time point. - (\\(B\\)) is the inverted sensitivity matrix (6×6) shown below. - The factor \\(10^6\\) adjusts from volts/millivolts depending on your amplifier/device calibration (kept here to match the provided calibration constants).\nVerification checks (quick ideas) - Compare the mean of the vertical force (\\(F_z\\)) to the subject’s body weight (mass × 9.81 m/s²) — they should be close on average. - Plot raw voltages vs computed forces for one channel to confirm linear scaling. - Check for unreasonable outliers in computed forces (e.g., extremely large or NaN values) and investigate missing/erroneous voltage samples.\nUnits and scaling notes - Make sure the amplifier gain (\\(G\\)) and the input voltage (\\(V_0\\)) reflect your acquisition hardware. If your device uses millivolt outputs with a different amplifier gain, adjust the conversion factor accordingly. - Document the units (N, N·m) and any scaling factors in your analysis record so others can reproduce your results.\nTips - Keep the original voltage columns until you are comfortable with the conversion. The notebook already assigns new columns Fx, Fy, Fz, Mx, My, Mz to df_EO and df_EC. - If units or calibration constants differ for your dataset, update \\(G\\), \\(V_0\\), or \\(B\\) accordingly and re-run the conversion cell.\nWhat’s next - After conversion, visualize the force and moment time-series and inspect their PSDs to plan appropriate cutoffs for filtering.\n\n# Inverted sensitivity matrix, B\nB = np.array([\n    [2.9007,  0.0200, -0.0009, -0.0253, -0.0085,  0.0090],\n    [-0.0067, 2.9024, -0.0520, -0.0366, -0.0149, -0.0341],\n    [0.0046, -0.0229, 11.4206, -0.0055,  0.0055,  0.0026],\n    [-0.0019,  0.0035, -0.0067,  1.4559, -0.0053, -0.0028],\n    [0.0036,  0.0011, -0.0067,  0.0018,  1.1475, -0.0008],\n    [0.0037,  0.0145, -0.0032,  0.0006,  0.0076,  0.6188]\n])\n\nG = 4000 # Gain\nV_0 = 10 # Input voltage\nCF = 1e6/(G*V_0) # Conversion factor\n\n\n# Plotting function\ndef plot_data(df_EO, df_EC, df_cols, plot_titles, y_labels, time_range=(15, 30), figsize=(10, 8), dpi=200, pad=2.0):\n    \"\"\"\n    Plot signals from two dataframes (EO and EC) over a specified time range.\n\n    This function generates six subplots corresponding to force (Fx, Fy, Fz) and moment (Mx, My, Mz) signals,\n    comparing data from two different conditions (EO and EC).\n\n    Args:\n        df_EO (DataFrame): Data containing EO (Eyes Open) condition.\n        df_EC (DataFrame): Data containing EC (Eyes Closed) condition, with the same columns as df_EO.\n        df_cols (list): List of column names to plot from the dataframes.\n        plot_titles (list): List of titles for each subplot.\n        y_labels (list): List of y-axis labels for each subplot.\n        time_range (tuple, optional): Time range (start, end) in seconds for plotting. Defaults to (15, 30).\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n        dpi (int, optional): Dots per inch (DPI) setting for the figure resolution. Defaults to 200.\n        pad (float, optional): Padding for tight_layout to adjust subplot spacing. Defaults to 2.0.\n\n    Raises:\n        ValueError: If the input dataframes do not contain the required columns.\n\n    Example:\n        &gt;&gt;&gt; plot_data(df_EO, df_EC, ['Fx', 'Fy', 'Fz', 'Mx', 'My', 'Mz'], titles, labels, time_range=(10, 25))\n    \"\"\"\n    # Create subplots\n    fig, axs = plt.subplots(6, 1, sharex=True, sharey=False, dpi=dpi, figsize=figsize)\n\n    # Plot data for each subplot\n    for i, (col, title, y_label) in enumerate(zip(df_cols,\n                                         plot_titles,y_labels)):\n        axs[i].plot(df_EO['Time'], df_EO[col], color='tab:blue', alpha=1.0, label='EO')\n        axs[i].plot(df_EC['Time'], df_EC[col], color='tab:orange', alpha=1.0, label='EC')\n        axs[i].legend(bbox_to_anchor=(1.1, 1), loc='upper right')\n        axs[i].set_ylabel(y_label)\n        axs[i].set_title(title)\n\n    # Set x-axis label and limits\n    plt.xlabel('Time [s]')\n    plt.xlim(time_range)\n\n    # Adjust layout\n    plt.tight_layout(pad=pad)\n\n    # Show the plot\n    plt.show()\n\n\n# Get voltages as array-like\nV_EO = df_EO[['VFx', 'VFy', 'VFz', 'VMx', 'VMy', 'VMz']].to_numpy()\n# Compute forces and moments in a vectorized manner\nY_EO = CF*(V_EO@B.T)\n# Assign computed values back to DataFrame\ndf_EO[['Fx', 'Fy', 'Fz', 'Mx', 'My', 'Mz']] = Y_EO\n# Get voltages as array-like\nV_EC = df_EC[['VFx', 'VFy', 'VFz', 'VMx', 'VMy', 'VMz']].to_numpy()\n# Compute forces and moments in a vectorized manner\nY_EC = CF*(V_EC@B.T)\n# Assign computed values back to DataFrame\ndf_EC[['Fx', 'Fy', 'Fz', 'Mx', 'My', 'Mz']] = Y_EC\n\nY_cols = ['Fx','Fy','Fz','Mx','My','Mz']\nplot_titles = ['Fx','Fy','Fz','Mx','My','Mz']\ny_labels = ['$F_x$ [N]','$F_y$ [N]','$F_z$ [N]','$M_x$ [N-m]','$M_y$ [N-m]','$M_z$ [N-m]']\n\nplot_data(df_EO=df_EO,df_EC=df_EC,df_cols=Y_cols,plot_titles=plot_titles,y_labels=y_labels,time_range=(0,15),dpi=100,figsize=(10,8))"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#filtering-and-selecting-cutoff-frequencies-practical-guide",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#filtering-and-selecting-cutoff-frequencies-practical-guide",
    "title": "Force-plate signal processing for balance analysis — a practical tutorial",
    "section": "Filtering and selecting cutoff frequencies — practical guide",
    "text": "Filtering and selecting cutoff frequencies — practical guide\nThis section demonstrates how different low-pass cutoff choices affect moment signals and how to apply a zero-phase Butterworth filter to all force/moment channels.\nWhat you’ll see - Example plots that compare a too-low cutoff (over‑smoothed), a too-high cutoff (insufficient noise removal), and a data-driven cutoff determined via cumulative power from the PSD. - Code that uses filter_timeseries_data(...) to compute per-channel cutoffs (or accept a single custom cutoff) and perform zero‑phase filtering with filtfilt when possible.\nHow to choose cutoffs - Too low: removes physiologically relevant sway (underestimates variability). - Too high: keeps noise that inflates spectral/variance metrics. - Data-driven: compute cutoff from a chosen cumulative power threshold (e.g., 95–99%). Use spectral_analysis(..., threshold=...) to inspect PSD/cumulative plots and select a threshold.\nRecommended filtering workflow (concise) 1. Compute PSD and cumulative power for a representative segment of each channel. 2. Pick a threshold (start at 95%), compute cutoff per channel, and inspect a few filtered traces. 3. If multiple channels consistently suggest similar cutoffs, you may choose to use a common cutoff for simplicity; otherwise preserve per-channel cutoffs. 4. Apply filtering to the full dataset and save the cutoffs used alongside filtered outputs.\nCommon practical options - Notch filtering: if mains interference (50/60 Hz) is present and strong, consider a narrow notch filter before low-pass filtering. - Detrending: remove linear drift before PSD computation if low-frequency trends dominate the spectrum. - Short recordings: filtfilt padding rules may not be satisfied; the function falls back to lfilter — document this and avoid phase-sensitive measures in that case.\nChecking results - Plot raw vs filtered traces and quantize differences (RMS reduction) to ensure you’re not removing expected signal content. - Compare sway metrics (range, speed) before and after filtering to understand the filter’s impact.\nTip - Save per-channel cutoff frequencies (and the threshold used to compute them) in a small JSON or CSV so your analysis is reproducible.\n\n# Create shortened df to only include signals between 15 and 30 seconds...\nf_c1_EC = 0.5 # (Hz) Too low cutoff frequency\nf_c2_EC = 450 # (Hz) Too high cutoff frequency\n\nfs = 1000 # ENSURE THAT THIS MATCHES THE DATA SAMPLING RATE!\nf_c3_EC = spectral_analysis(df_EC['My'],sampling_freq=fs, threshold=95)\n\nprint(type(pd.DataFrame(df_EC['My'])))\n\n# Filter data using too low of a cutoff frequency\ndf_c1 = filter_timeseries_data(pd.DataFrame(df_EC['My']),sampling_freq=fs,custom_cutoff_frequency=f_c1_EC)\n# Filter data using too high of a cutoff frequency\ndf_c2 = filter_timeseries_data(pd.DataFrame(df_EC['My']),sampling_freq=fs,custom_cutoff_frequency=f_c2_EC)\n# Filter data using an appropriate cutoff frequency\ndf_c3 = filter_timeseries_data(pd.DataFrame(df_EC['My']),sampling_freq=fs,custom_cutoff_frequency=f_c3_EC)\n\n# Plot the data showing the effects of using the different cutoff frequencies\nfig, axs = plt.subplots(3, 1, sharex=True, sharey=False, dpi=100, figsize=(10, 8))\n\ntime = df_EC['Time']\n\n# Subplot 1: too low cutoff\naxs[0].plot(time, df_EC['My'], color='gray', linestyle='-', alpha=0.8, label='Original $M_y$ (raw)')\naxs[0].plot(time, df_c1['My'], color='tab:blue', alpha=1.0, label=f'Filtered (fc={f_c1_EC:.2f} Hz)')\naxs[0].set_title(\"Cutoff Freq Too Low\")\naxs[0].legend(loc='upper right')\n\n# Subplot 2: too high cutoff\naxs[1].plot(time, df_EC['My'], color='gray', linestyle='-', alpha=0.8, label='Original $M_y$ (raw)')\naxs[1].plot(time, df_c2['My'], color='tab:green', alpha=1.0, label=f'Filtered (fc={f_c2_EC:.2f} Hz)')\naxs[1].set_title(\"Cutoff Freq Too High\")\naxs[1].legend(loc='upper right')\n\n# Subplot 3: appropriate cutoff (data-driven)\naxs[2].plot(time, df_EC['My'], color='gray', linestyle='-', alpha=0.8, label='Original $M_y$ (raw)')\naxs[2].plot(time, df_c3['My'], color='tab:orange', alpha=1.0, label=f'Filtered (fc={f_c3_EC:.2f} Hz)')\naxs[2].set_title(\"Appropriate Cutoff Frequency (spectral_analysis)\")\naxs[2].legend(loc='upper right')\n\naxs[-1].set_xlabel('Time [s]')\naxs[1].set_ylabel('Eyes Closed $M_y$ [N-m]')\n\n# Zoom to the 15-30 s window as intended (if available)\ntry:\n    axs[0].set_xlim(5, 15)\nexcept Exception:\n    pass\n\nplt.tight_layout(pad=2.0)\nplt.show()\n\ndf_EO_filtered = filter_timeseries_data(df_EO, sampling_freq=fs)\ndf_EC_filtered = filter_timeseries_data(df_EC, sampling_freq=fs)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#center-of-pressure-cop-calculation-and-stabilogram-plotting",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#center-of-pressure-cop-calculation-and-stabilogram-plotting",
    "title": "Force-plate signal processing for balance analysis — a practical tutorial",
    "section": "Center of Pressure (CoP) — calculation and stabilogram plotting",
    "text": "Center of Pressure (CoP) — calculation and stabilogram plotting\nThis section explains what the Center of Pressure (CoP) is, why it matters for balance analysis, how to compute it from filtered force/moment signals, and how to visualize stabilograms for EO and EC conditions.\nWhat is the CoP? - The CoP is the point location on the force‑plate surface where the resultant ground reaction force acts. It is computed from vertical force and plate moments and represents the net pressure centroid under the foot/feet. - Physically, CoP tracks how the neuromuscular system shifts load to maintain balance; it is a directly measured, high‑temporal‑resolution proxy for postural control dynamics. - CoP is not the same as the body Center of Mass (CoM). CoM represents mass distribution of the body and typically requires motion capture and modeling; CoP is measured directly by the force plate.\nWhy CoP is useful - Sensitive to corrective actions: fast corrective sways and stabilizing adjustments produce characteristic CoP excursions and velocities. - Trial‑level and summary metrics derived from CoP (range, speed, excursion) are widely used to quantify postural stability and to compare conditions or groups (e.g., EO vs EC). - Easy to compute and reproducible across labs when calibration and filtering are documented.\nWhat is a stabilogram? - A stabilogram is a 2D time‑series plot of CoP coordinates (ML vs AP) that shows the trajectory of CoP over the trial. It is the primary visualization for postural sway. - Interpret visually: - Large cloud/trajectory = more sway / less stability. - Long, smooth excursions suggest slow drift; dense, jittery paths indicate higher-frequency corrective actions or noise. - Complement stabilograms with scalar summaries (range, mean speed, RMS) and spectral analysis to separate slow and fast behavior.\nComputation (per time point):\n\nGiven the measured vector \\(\\vec{Y} = (F_x, F_y, F_z, M_x, M_y, M_z)\\) and the plate shear‑center offset \\(z_0\\) (in meters), the CoP coordinates are computed as the displayed equations:\n\n\\[\n\\begin{aligned}\n\\vec{Y} &= \\big(F_x,\\, F_y,\\, F_z,\\, M_x,\\, M_y,\\, M_z\\big) \\\\\nx_{\\mathrm{CoP}} &= \\frac{M_x + z_0\\,F_y}{F_z} \\\\[6pt]\ny_{\\mathrm{CoP}} &= \\frac{z_0\\,F_x - M_y}{F_z}\n\\end{aligned}\n\\]\n\nImportant notes on signs and conventions: confirm that your plate calibration and sign convention match these formulas (Mx positive → generates positive x_CoP, etc.). Small sign flips in moments/forces will invert CoP axes.\n\nUnits and centering - Keep consistent SI units: forces in N, moments in N·m, z0 in m. Convert CoP from meters to mm for plotting (multiply by 1e3) if desired for readability. - Center stabilograms by subtracting the temporal mean from each CoP axis so the trajectory is centered at (0,0). This removes constant offsets due to subject placement.\nPractical tips and pitfalls - Handle small/zero Fz: mask, drop, or interpolate samples where Fz is near zero (division by very small values yields spurious large CoP). Consider thresholding Fz (e.g., ignore samples where Fz &lt; some small fraction of median Fz). - Filtering: compute CoP from filtered forces/moments (zero‑phase filtering preferred) to avoid adding high‑frequency noise in the stabilogram. Document filter design (type, order, cutoff). - Edge effects: filtfilt requires adequate padding—short recordings may fall back to lfilter (phase shift). Avoid phase‑sensitive measures if filtfilt fallback occurs. - Outlier checks: clip or remove sudden CoP spikes due to artifacts (e.g., transient force clipping).\nPlotting guidance - Use equal axis scales (square aspect ratio) and identical axis limits for EO and EC to allow direct visual comparison. - Add dashed lines at zero to show anatomical midlines; annotate mean CoP and origin. - For long traces, consider color‑coding by time, plotting a density/contour, or downsampling for clarity. - Overlay raw vs filtered CoP (or show both on separate panels) to confirm filtering did not remove relevant low‑frequency features.\nInterpretation (tandem stance example) - Tandem stance typically increases AP sway (narrow fore‑aft base) and may increase average CoP speed; removing vision (EC) commonly further increases excursion and speed. - Use paired comparisons (EO vs EC) and percent change together with stabilograms to report effects.\nNext steps - Compute scalar metrics from the centered CoP (AP/ML excursion ranges, mean CoP speed, mean excursion distance, std dev) and report the filter/cutoff and any sample masking thresholds used for reproducibility. - Important notes on signs and conventions: confirm that your plate calibration and sign convention match these formulas (Mx positive → generates positive x_CoP, etc.). Small sign flips in moments/forces will invert CoP axes.\nUnits and centering - Keep consistent SI units: forces in N, moments in N·m, z0 in m. Convert CoP from meters to mm for plotting (multiply by 1e3) if desired for readability. - Center stabilograms by subtracting the temporal mean from each CoP axis so the trajectory is centered at (0,0). This removes constant offsets due to subject placement.\nPractical tips and pitfalls - Handle small/zero Fz: mask, drop, or interpolate samples where Fz is near zero (division by very small values yields spurious large CoP). Consider thresholding Fz (e.g., ignore samples where Fz &lt; some small fraction of median Fz). - Filtering: compute CoP from filtered forces/moments (zero‑phase filtering preferred) to avoid adding high‑frequency noise in the stabilogram. Document filter design (type, order, cutoff). - Edge effects: filtfilt requires adequate padding—short recordings may fall back to lfilter (phase shift). Avoid phase‑sensitive measures if filtfilt fallback occurs. - Outlier checks: clip or remove sudden CoP spikes due to artifacts (e.g., transient force clipping).\nPlotting guidance - Use equal axis scales (square aspect ratio) and identical axis limits for EO and EC to allow direct visual comparison. - Add dashed lines at zero to show anatomical midlines; annotate mean CoP and origin. - For long traces, consider color‑coding by time, plotting a density/contour, or downsampling for clarity. - Overlay raw vs filtered CoP (or show both on separate panels) to confirm filtering did not remove relevant low‑frequency features.\nInterpretation (tandem stance example) - Tandem stance typically increases AP sway (narrow fore‑aft base) and may increase average CoP speed; removing vision (EC) commonly further increases excursion and speed. - Use paired comparisons (EO vs EC) and percent change together with stabilograms to report effects.\nNext steps - Compute scalar metrics from the centered CoP (AP/ML excursion ranges, mean CoP speed, mean excursion distance, std dev) and report the filter/cutoff and any sample masking thresholds used for reproducibility.\n\nz_0 = -37.645*1e-3 # (m) Shear center relative to geometric center\n\nCOPx_EO = (df_EO_filtered['Mx']+z_0*df_EO_filtered['Fy'])/df_EO_filtered['Fz']\nCOPy_EO = (df_EO_filtered['My']-z_0*df_EO_filtered['Fx'])/df_EO_filtered['Fz']\n\nCOPx_EC = (df_EC_filtered['Mx']+z_0*df_EC_filtered['Fy'])/df_EC_filtered['Fz']\nCOPy_EC = (df_EC_filtered['My']-z_0*df_EC_filtered['Fx'])/df_EC_filtered['Fz']\n\nCOPx_EO_centered = (COPx_EO - COPx_EO.mean())*1e3 # Convert to mm\nCOPy_EO_centered = (COPy_EO - COPy_EO.mean())*1e3 # Convert to mm\n\nCOPx_EC_centered = (COPx_EC - COPx_EC.mean())*1e3 # Convert to mm\nCOPy_EC_centered = (COPy_EC - COPy_EC.mean())*1e3 # Convert to mm\n\nfig, axs = plt.subplots(1, 2, dpi=100, figsize=(12,6))\n\n# Determine the maximum range for centering around (0,0)\nx_max = np.ceil(1.25*max(abs(COPx_EO_centered.min()), abs(COPx_EO_centered.max()),\n            abs(COPx_EC_centered.min()), abs(COPx_EC_centered.max())))\ny_max = np.ceil(1.25*max(abs(COPy_EO_centered.min()), abs(COPy_EO_centered.max()),\n            abs(COPy_EC_centered.min()), abs(COPy_EC_centered.max())))\n\nmax_range = max(x_max, y_max)  # Ensure square limits\n\n# First subplot (EO)\naxs[0].plot(COPx_EO_centered, COPy_EO_centered, color='tab:blue', label='EO')\naxs[0].set_xlim(-max_range, max_range)\naxs[0].set_ylim(-max_range, max_range)\naxs[0].set_xlabel('Medial/lateral CoP position [mm]')\naxs[0].set_ylabel('Anterior/posterior CoP position [mm]')\naxs[0].set_title('Eyes Open Condition')\naxs[0].axhline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[0].axvline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[0].set_aspect('equal')  # Ensures square aspect ratio\n\n# Second subplot (EC)\naxs[1].plot(COPx_EC_centered, COPy_EC_centered, color='tab:orange', label='EC')\naxs[1].set_xlim(-max_range, max_range)\naxs[1].set_ylim(-max_range, max_range)\naxs[1].set_xlabel('Medial/lateral CoP position [mm]')\naxs[1].set_ylabel('Anterior/posterior CoP position [mm]')\naxs[1].set_title('Eyes Closed Condition')\naxs[1].axhline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[1].axvline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[1].set_aspect('equal')  # Ensures square aspect ratio\n\n# Set overall figure title with large font\nfig.suptitle('Stabilogram Comparison', fontsize=18, fontweight='bold')\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title\nplt.show()"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#balance-metrics-definitions-computation-and-interpretation",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#balance-metrics-definitions-computation-and-interpretation",
    "title": "Force-plate signal processing for balance analysis — a practical tutorial",
    "section": "Balance metrics — definitions, computation, and interpretation",
    "text": "Balance metrics — definitions, computation, and interpretation\nThis section computes commonly used scalar summary metrics from CoP time series and explains how to interpret them in the context of balance tasks.\nMetrics computed in the notebook - AP excursion range: max(CoP_x) - min(CoP_x) (mm) - ML excursion range: max(CoP_y) - min(CoP_y) (mm) - Average CoP speed: mean( sqrt( (dx/dt)^2 + (dy/dt)^2 ) ) (mm/s) - Mean excursion distance: mean( sqrt(x^2 + y^2) ) (mm) - Std dev of excursion distance: std( sqrt(x^2 + y^2) ) (mm)\nImplementation notes - Use centered CoP (subtract temporal mean) so offsets do not inflate range measures. - Compute derivatives using successive differences divided by dt (ensure consistent units: mm vs m). - For average speed, ignore NaN or masked samples (e.g., where Fz was invalid). Consider thresholding small Fz values before computing CoP. - Document sampling rate, filter design (type, order, cutoff) and any NaN handling — metrics are sensitive to filtering and preprocessing.\nHow to interpret each metric (practical guide) - AP / ML excursion ranges - What they measure: trial-wide span of CoP in anterior–posterior and medial–lateral axes. - Larger values → greater gross excursion (more sway or larger shifts of load). Directional changes (AP vs ML) can indicate task-specific instability (e.g., tandem stance increases AP). - Caveats: ranges are sensitive to brief large transients (artifacts). Check stabilograms to confirm excursions reflect behaviour, not noise. - Average CoP speed - What it measures: time-averaged instantaneous CoP velocity; captures how actively the subject is correcting posture. - Larger speed → more frequent/rapid corrective actions or noise. Often more sensitive to subtle balance changes than range. - Caveats: speed increases with trial length variability and with higher-frequency noise; ensure consistent filtering across conditions. - Mean excursion distance - What it measures: average radial distance from center (magnitude of sway). - Larger mean distance → CoP is on average farther from center; useful summary of overall displacement. - Complement with range and speed to distinguish broad drift vs frequent small corrections. - Std dev of excursion distance - What it measures: variability of radial CoP magnitude; indicates consistency vs intermittency of sway. - Larger std → more variable control (bursty corrections or inconsistent posture).\nPractical interpretation rules of thumb - Compare within-subject, within-task (paired comparisons) whenever possible; absolute values depend on posture, footwear, surface, and trial duration. - Use percent change and paired statistics (paired t-test, Wilcoxon) for EO vs EC or pre/post designs — report effect size and confidence intervals, not only p-values. - Visual checks: always inspect stabilograms (trajectory) and raw vs filtered traces to ensure metrics reflect behavior. - Filtering matters: a more aggressive low-pass reduces speed and high-frequency variance while leaving gross ranges less affected. Report filter parameters.\nHandling artifacts and edge cases - Outliers: remove or clip transient spikes before computing range/speed. - Low/zero Fz: mask samples where Fz is near zero (division instability) or interpolate short gaps. - Short trials: filtfilt padding may fail and introduce phase shift; avoid phase-sensitive metrics if lfilter fallback occurs.\n\ndt = 1/fs # Time difference between adjacent time points\n\n# AP (Anterior-Posterior) Excursion Range\nAP_EO = COPx_EO_centered.max() - COPx_EO_centered.min()\nAP_EC = COPx_EC_centered.max() - COPx_EC_centered.min()\n\n# ML (Medio-Lateral) Excursion Range\nML_EO = COPy_EO_centered.max() - COPy_EO_centered.min()\nML_EC = COPy_EC_centered.max() - COPy_EC_centered.min()\n\n# Average CoP Speed\nAvg_CoP_Speed_EO = (np.sqrt(COPx_EO_centered.diff()**2 + COPy_EO_centered.diff()**2)/dt).mean()\nAvg_CoP_Speed_EC = (np.sqrt(COPx_EC_centered.diff()**2 + COPy_EC_centered.diff()**2)/dt).mean()\n\n# Mean Excursion Distance\nMean_Excursion_Distance_EO = np.sqrt(COPx_EO_centered**2 + COPy_EO_centered**2).mean()\nMean_Excursion_Distance_EC = np.sqrt(COPx_EC_centered**2 + COPy_EC_centered**2).mean()\n\n# Standard Deviation of Excursion Distance\nStDev_Excursion_Distance_EO = np.sqrt(COPx_EO_centered**2 + COPy_EO_centered**2).std()\nStDev_Excursion_Distance_EC = np.sqrt(COPx_EC_centered**2 + COPy_EC_centered**2).std()\n\n# Print the calculated metrics\nprint(\"Eyes Open Condition:\")\nprint(\"AP Excursion Range: {:.2f} mm\".format(AP_EO))\nprint(\"ML Excursion Range: {:.2f} mm\".format(ML_EO))\nprint(\"Average CoP Speed: {:.2f} mm/s\".format(Avg_CoP_Speed_EO))\nprint(\"Mean Excursion Distance: {:.2f} mm\".format(Mean_Excursion_Distance_EO))\nprint(\"Standard Deviation of Excursion Distance: {:.2f} mm\".format(StDev_Excursion_Distance_EO))\n\nprint(\"\\nEyes Closed Condition:\")\nprint(\"AP Excursion Range: {:.2f} mm\".format(AP_EC))\nprint(\"ML Excursion Range: {:.2f} mm\".format(ML_EC))\nprint(\"Average CoP Speed: {:.2f} mm/s\".format(Avg_CoP_Speed_EC))\nprint(\"Mean Excursion Distance: {:.2f} mm\".format(Mean_Excursion_Distance_EC))\nprint(\"Standard Deviation of Excursion Distance: {:.2f} mm\".format(StDev_Excursion_Distance_EC))\n\n# Calculate Percent change using (EC - EO) / EO * 100\nAP_percent_change = (AP_EC - AP_EO) / AP_EO * 100\nML_percent_change = (ML_EC - ML_EO) / ML_EO * 100\nAvg_CoP_Speed_percent_change = (Avg_CoP_Speed_EC - Avg_CoP_Speed_EO) / Avg_CoP_Speed_EO * 100\nMean_Excursion_Distance_percent_change = (Mean_Excursion_Distance_EC - Mean_Excursion_Distance_EO) / Mean_Excursion_Distance_EO * 100\nStDev_Excursion_Distance_percent_change = (StDev_Excursion_Distance_EC - StDev_Excursion_Distance_EO) / StDev_Excursion_Distance_EO * 100\n\n# Print the percent differences\nprint(\"\\nPercent change from Eyes Open to Eyes Closed conditions:\")\n\nprint(\"AP Excursion Range: {:+.2f} %\".format(AP_percent_change))\nprint(\"ML Excursion Range: {:+.2f} %\".format(ML_percent_change))\nprint(\"Average CoP Speed: {:+.2f} %\".format(Avg_CoP_Speed_percent_change))\nprint(\"Mean Excursion Distance: {:+.2f} %\".format(Mean_Excursion_Distance_percent_change))\nprint(\"Standard Deviation of Excursion Distance: {:+.2f} %\".format(StDev_Excursion_Distance_percent_change))\n\nEyes Open Condition:\nAP Excursion Range: 25.84 mm\nML Excursion Range: 25.82 mm\nAverage CoP Speed: 29.84 mm/s\nMean Excursion Distance: 6.92 mm\nStandard Deviation of Excursion Distance: 3.72 mm\n\nEyes Closed Condition:\nAP Excursion Range: 26.63 mm\nML Excursion Range: 32.95 mm\nAverage CoP Speed: 53.20 mm/s\nMean Excursion Distance: 7.36 mm\nStandard Deviation of Excursion Distance: 3.70 mm\n\nPercent change from Eyes Open to Eyes Closed conditions:\nAP Excursion Range: +3.08 %\nML Excursion Range: +27.60 %\nAverage CoP Speed: +78.31 %\nMean Excursion Distance: +6.33 %\nStandard Deviation of Excursion Distance: -0.64 %"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work in the Tissue Biomechanics Laboratory, where I investigate propulsion biomechanics and musculoskeletal loading during wheelchair-based exercises enabled by wearable sensors, machine learning, and computer vision techniques. One of my interests is building tools and tutorials to make biomechanics research more accessible by utilizing open-source workflows."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I’m Griffin Sipes — a fourth-year PhD student in Mechanical Engineering at the University of Illinois Urbana-Champaign."
  },
  {
    "objectID": "index.html#featured-sections",
    "href": "index.html#featured-sections",
    "title": "Welcome!",
    "section": "Featured Sections",
    "text": "Featured Sections\n\nTutorials — Step-by-step guides and teaching materials.\nProjects — Portfolio projects."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Welcome!",
    "section": "Quick Links",
    "text": "Quick Links\n\nAbout\nTutorials\nProjects"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Welcome!",
    "section": "Contact",
    "text": "Contact\nIf you’d like to get in touch, find me on"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html",
    "href": "notebooks/IntroductionImageProcessing.html",
    "title": "Interactive Introduction to Image Processing",
    "section": "",
    "text": "Open in Colab"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#why-this-matters",
    "href": "notebooks/IntroductionImageProcessing.html#why-this-matters",
    "title": "Interactive Introduction to Image Processing",
    "section": "Why this matters",
    "text": "Why this matters\nImage processing is everywhere: improving medical images, cleaning up photos, extracting measurements, and powering computer vision systems such as object detectors and trackers. This notebook focuses on the practical building blocks you can reuse across projects."
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#what-youll-learn",
    "href": "notebooks/IntroductionImageProcessing.html#what-youll-learn",
    "title": "Interactive Introduction to Image Processing",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nHow images are represented (pixels, channels, color spaces)\nBasic transforms: resizing, rotating, and cropping\nFilters: blur, sharpen, edge detection, and morphological ops\nHow to inspect image statistics using histograms\nHow to detect objects and faces using modern libraries\nPractical tips for tuning parameters and debugging outputs"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#requirements",
    "href": "notebooks/IntroductionImageProcessing.html#requirements",
    "title": "Interactive Introduction to Image Processing",
    "section": "Requirements",
    "text": "Requirements\nTo run this notebook, you need the following:\n\nPython 3.8+\nJupyter Notebook (or JupyterLab)"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#tips-before-you-start",
    "href": "notebooks/IntroductionImageProcessing.html#tips-before-you-start",
    "title": "Interactive Introduction to Image Processing",
    "section": "Tips before you start",
    "text": "Tips before you start\n\nRun cells sequentially so variables (like img) are available.\nIf an example is slow, reduce sizes (resize to 320x240) for faster iteration.\nUse the provided test_image.jpg or change the filename to your own image."
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#resources-next-steps",
    "href": "notebooks/IntroductionImageProcessing.html#resources-next-steps",
    "title": "Interactive Introduction to Image Processing",
    "section": "Resources & next steps",
    "text": "Resources & next steps\n\nOpenCV documentation: https://docs.opencv.org/\nUltralytics (YOLO) docs: https://docs.ultralytics.com/\nMediaPipe face detection: https://developers.google.com/mediapipe\n\n\nEnvironment Setup\nThis cell sets up the environment for image and video processing. It downloads and imports essential libraries:\n\nos for file operations.\ncv2 (OpenCV) for image and video processing.\nnumpy for numerical operations.\nmatplotlib.pyplot for plotting images and results.\n\nIt also defines file paths for the test image, video, and YOLO model, and prints quick checks to confirm that these files exist and that OpenCV is installed. This ensures all required resources are available before running further image processing tasks.\n\n# Install required packages\n%pip install opencv-python matplotlib numpy ultralytics mediapipe ipython\n\nimport os\nimport cv2\nimport numpy as np\nimport requests\nfrom PIL import Image\n\n# Direct download links for Box files\nIMG_URL = 'https://uofi.box.com/shared/static/4vwmy8d4zutugdq54xj0jm98y2dsv8t0.jpg'\nVIDEO_URL = 'https://uofi.box.com/shared/static/of08em0yjvobx6xoqtxzk02glq9d8e55.mp4'\nYOLO_URL = 'https://uofi.box.com/shared/static/pma36x7cge58b3i18b2o0xqlcqd38iwk.pt'\n\ndef download_file(url, save_path):\n    r = requests.get(url, stream=True)\n    r.raise_for_status()\n    with open(save_path, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n    return save_path\n\n# Download files to /content/\nimg_path = '/content/test_image.jpg'\nvideo_path = '/content/GX010881.MP4'\nyolo_path = '/content/yolo11n.pt'\n\ndownload_file(IMG_URL, img_path)\ndownload_file(VIDEO_URL, video_path)\ndownload_file(YOLO_URL, yolo_path)\n\n# Load image\nimg = cv2.imread(img_path)\nprint('Image loaded:', img is not None)\n\n# Load video\nvideo = cv2.VideoCapture(video_path)\nprint('Video loaded:', video.isOpened())\n\n# Check YOLO model file\nprint('YOLO model downloaded:', os.path.exists(yolo_path))\n\ncv2 version: 4.11.0\nimg exists: True\nvideo exists: True\nmodel file exists: True\n\n\n\n\nLoading and Displaying Images with OpenCV\nOpenCV is a powerful library for image processing in Python. To get started, you need to load an image from disk and display it. Here’s how:\n\nLoading an Image\nUse cv2.imread() to load an image from a file. The image is read as a NumPy array in BGR (Blue, Green, Red) format.\n\n# Step 1: Load and Display an Image\nimport cv2\nfrom matplotlib import pyplot as plt\n\n# Load an image from file\nimg = cv2.imread('test_image.jpg')\n\nif img is not None:\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n    plt.imshow(img_rgb)\n    plt.title('Loaded Image')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not found. Try changing the filename to an image file.')\n\n\n\n\n\n\n\n\n\n\n\nColor Spaces and Channels\nImages are made up of pixels, and each pixel can have one or more channels depending on the color space used. Understanding color spaces and channels is essential for effective image analysis and processing.\n\nWhat is a Color Space?\nA color space is a specific way of representing colors numerically. It defines how pixel values map to actual colors. Common color spaces include RGB, LAB, and HSV.\n\n\nWhat is a Channel?\nA channel is a single component of a color space. For example, in RGB, each pixel has three channels: Red, Green, and Blue. In grayscale images, there is only one channel representing intensity.\n\n\nCommon Color Spaces\n\nRGB (Red, Green, Blue):\n\nEach pixel has three channels: R, G, and B.\nUsed for display and general image processing.\nNot perceptually uniform.\n\n**LAB (Lab*):**\n\nThree channels: L (lightness), a (green–red), b (blue–yellow).\nDesigned for perceptual uniformity.\nUseful for color correction and measuring color differences.\n\nHSV (Hue, Saturation, Value):\n\nThree channels: H (hue), S (saturation), V (value/brightness).\nSeparates color information (hue) from intensity (value).\nUseful for color-based segmentation and filtering.\n\n\n\n\nWhy Channels Matter\n\nChannel manipulation: You can process each channel separately (e.g., enhance brightness, isolate colors).\nVisualization: Viewing individual channels helps understand image structure and color distribution.\nAnalysis: Some algorithms work better on specific channels (e.g., edge detection on intensity, segmentation on hue).\n\nTip:\nChoose the color space and channels that best fit your task. For example, use LAB for brightness/contrast adjustment, HSV for color segmentation, and RGB for visualization.\n\n# Show original image and individual color channels (RGB, LAB, HSV)\nif img is not None:\n    # RGB channels\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    r, g, b = img_rgb[:, :, 0], img_rgb[:, :, 1], img_rgb[:, :, 2]\n    # LAB channels\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l_lab, a_lab, b_lab = cv2.split(lab)\n    # HSV channels\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h_hsv, s_hsv, v_hsv = cv2.split(hsv)\n\n    plt.figure(figsize=(16, 8))\n    # Row 1: RGB\n    plt.subplot(3, 4, 1)\n    plt.imshow(img_rgb)\n    plt.title('RGB (original)')\n    plt.axis('off')\n    plt.subplot(3, 4, 2)\n    plt.imshow(r, cmap='Reds')\n    plt.title('Red channel')\n    plt.axis('off')\n    plt.subplot(3, 4, 3)\n    plt.imshow(g, cmap='Greens')\n    plt.title('Green channel')\n    plt.axis('off')\n    plt.subplot(3, 4, 4)\n    plt.imshow(b, cmap='Blues')\n    plt.title('Blue channel')\n    plt.axis('off')\n\n    # Row 2: LAB\n    plt.subplot(3, 4, 5)\n    plt.imshow(l_lab, cmap='gray')\n    plt.title('LAB L (Lightness)')\n    plt.axis('off')\n    plt.subplot(3, 4, 6)\n    plt.imshow(a_lab, cmap='RdYlGn')\n    plt.title('LAB a (Green-Red)')\n    plt.axis('off')\n    plt.subplot(3, 4, 7)\n    plt.imshow(b_lab, cmap='RdYlBu')\n    plt.title('LAB b (Blue-Yellow)')\n    plt.axis('off')\n    plt.subplot(3, 4, 8)\n    plt.axis('off')  # Empty for layout\n\n    # Row 3: HSV\n    plt.subplot(3, 4, 9)\n    plt.imshow(h_hsv, cmap='hsv')\n    plt.title('HSV H (Hue)')\n    plt.axis('off')\n    plt.subplot(3, 4, 10)\n    plt.imshow(s_hsv, cmap='gray')\n    plt.title('HSV S (Saturation)')\n    plt.axis('off')\n    plt.subplot(3, 4, 11)\n    plt.imshow(v_hsv, cmap='gray')\n    plt.title('HSV V (Value)')\n    plt.axis('off')\n    plt.subplot(3, 4, 12)\n    plt.axis('off')  # Empty for layout\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nGrayscale imagery — why and how\nGrayscale images use a single intensity channel, making them faster to process and easier for many algorithms (e.g., edge detection, thresholding). Converting to grayscale is a common first step in image analysis.\n\nBackground\nMost digital images are captured in color, with each pixel containing multiple values (channels) for red, green, and blue. However, many image processing tasks—such as measuring brightness, detecting edges, or segmenting objects—work best on simpler data. Grayscale images reduce complexity by representing each pixel with a single value for intensity, ranging from black (0) to white (255).\nGrayscale conversion is widely used in medical imaging, document analysis, and computer vision because it: - Removes color distractions, focusing on structure and contrast. - Speeds up processing and reduces memory usage. - Simplifies algorithms that rely on intensity rather than color.\nTip:\nStart with grayscale for tasks like thresholding, edge detection, and morphological operations. Use color only when necessary for segmentation or visualization.\n\n# Convert the loaded image to grayscale and display it\nif img is not None:\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Grayscale Image')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\nImage Resizing and Rotation\nResizing and rotating images are essential preprocessing steps in image analysis and computer vision.\nWhy resize?\n- Reduces memory and computation time, especially for large images or real-time applications. - Enables faster experimentation and model training. - Helps standardize input sizes for neural networks and algorithms.\nWhy rotate?\n- Corrects image orientation for consistent analysis. - Useful for aligning features or objects in medical, satellite, or document images.\nTip:\nWhen enlarging images, use interpolation methods (e.g., bilinear, bicubic) to avoid pixelation and preserve quality. For shrinking, simple nearest-neighbor or area interpolation is often sufficient.\nResizing and rotating are quick ways to optimize your workflow and improve downstream results.\n\n# Resize and rotate examples\nif img is not None:\n    # Use the grayscale image 'gray' if available, otherwise convert\n    small = cv2.resize(gray, (320, 240))\n    rotated = cv2.rotate(small, cv2.ROTATE_90_CLOCKWISE)\n    plt.figure(figsize=(8, 4))\n    plt.subplot(1, 2, 1)\n    plt.imshow(small, cmap='gray')\n    plt.title('Resized (320x240)')\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(rotated, cmap='gray')\n    plt.title('Rotated 90 deg')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nSmoothing / Blur — Purpose, Background, and Sharpening\nBlurring (smoothing) is a fundamental image processing technique that reduces high-frequency noise and small details. It is commonly used before edge detection, thresholding, or segmentation to suppress speckle and minor artifacts, making features easier to analyze.\n\nWhy Blur?\n\nNoise reduction: Removes random pixel fluctuations and small unwanted details.\nPreprocessing: Improves the reliability of subsequent steps like edge detection and binarization.\nVisual effect: Produces a softer, less detailed image.\n\nA Gaussian blur uses a weighted kernel (must be odd-sized, e.g., 3x3, 5x5, 11x11) to average pixel values, giving more weight to the center. Larger kernels produce stronger smoothing.\n\n\nSharpening\nSharpening enhances edges and fine details, making features stand out. It is often used after blurring or on its own to improve image clarity.\n\nHow it works: Sharpening applies a kernel that emphasizes differences between neighboring pixels, boosting contrast at edges.\nCommon method: The Laplacian or unsharp mask filter.\n\nTip:\nUse blurring to clean up noise before analysis. Use sharpening to highlight boundaries and details for visualization or feature extraction.\nExample kernels: - Gaussian blur:\n[[1, 2, 1],      [2, 4, 2],      [1, 2, 1]] / 16 - Sharpening:\n[[ 0, -1,  0],      [-1,  5, -1],      [ 0, -1,  0]]\n\nif img is not None:\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Original Grayscale')\n    plt.axis('off')\n\n    # Gaussian blur with 11x11 kernel and sigma=0\n    blurred = cv2.GaussianBlur(gray, (11, 11), 0)\n    plt.subplot(1, 3, 2)\n    plt.imshow(blurred, cmap='gray')\n    plt.title('Blurred (Gaussian)')\n    plt.axis('off')\n\n    # Sharpening kernel\n    sharpen_kernel = np.array([[0, -1, 0],\n                               [-1, 5, -1],\n                               [0, -1, 0]])\n    sharpened = cv2.filter2D(gray, -1, sharpen_kernel)\n    plt.subplot(1, 3, 3)\n    plt.imshow(sharpened, cmap='gray')\n    plt.title('Sharpened')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nHistograms — Background & Intuition\nAn image histogram is a graphical representation of the distribution of pixel intensities (brightness or color values) in an image. It helps you understand the overall exposure, contrast, and color balance at a glance.\n\nWhat does a histogram show?\n\nX-axis: Pixel intensity values (0–255 for 8-bit images).\nY-axis: Number of pixels at each intensity.\nGrayscale images: One histogram for brightness.\nColor images: Separate histograms for each channel (Red, Green, Blue).\n\n\n\nWhy are histograms useful?\n\nContrast: A wide histogram means high contrast; a narrow one means low contrast.\nBrightness: If the histogram is shifted left, the image is dark; shifted right, it’s bright.\nExposure: Peaks at the ends may indicate underexposure (too dark) or overexposure (too bright).\nColor balance: Comparing channel histograms reveals color casts or imbalances.\n\n\n\nPractical uses\n\nImage enhancement: Adjust brightness/contrast or apply histogram equalization.\nThresholding: Choose thresholds for binarization based on histogram shape.\nQuality control: Detect poor lighting or exposure problems.\n\nHistograms are a simple but powerful tool for diagnosing and improving images in any image processing workflow.\n\n# Intensity and channel histograms\nif img is not None:\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 3, 1)\n    plt.hist(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).ravel(), bins=256)\n    plt.title('Intensity histogram')\n    plt.subplot(1, 3, 2)\n    plt.hist(r.ravel(), bins=256, color='r')\n    plt.title('Red hist')\n    plt.subplot(1, 3, 3)\n    plt.hist(g.ravel(), bins=256, color='g')\n    plt.title('Green hist')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nHistogram-based Image Processing\nHistogram-based image processing uses the distribution of pixel intensities to analyze and enhance images. By examining the histogram, we can adjust brightness, contrast, and exposure, detect features, and segment regions. This approach is essential because it provides a quantitative way to understand image quality and apply targeted corrections, making images more useful for visualization and further analysis.\nCLAHE (Contrast Limited Adaptive Histogram Equalization) is an advanced method for improving image contrast, especially in images with varying lighting or local features. Unlike standard histogram equalization, which adjusts contrast globally, CLAHE works on small regions (tiles) of the image and limits amplification to avoid noise.\n\nStandard Histogram Equalization\n\nGlobal adjustment: Redistributes pixel intensities across the entire image.\nBest for: Images with uniform lighting and global low contrast.\nDrawbacks: Can over-amplify noise and create unnatural effects in areas with little variation.\n\n\n\nCLAHE\n\nLocal adjustment: Applies histogram equalization to small tiles, then combines them.\nContrast limiting: Prevents over-amplification of noise by clipping the histogram.\nBest for: Medical images, uneven lighting, or images with both bright and dark regions.\nAdvantages: Preserves local details, avoids noise amplification, and produces more natural results.\n\nSummary:\nUse standard histogram equalization for quick global contrast enhancement. Use CLAHE for images with local contrast issues, uneven illumination, or when you want to avoid boosting noise.\n\n# Histogram equalization and CLAHE comparison (grayscale, LAB luminance)\nif img is not None:\n    # Prepare processed images\n    gray_eq = cv2.equalizeHist(gray)\n    gray_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    img_clahe = gray_clahe.apply(gray)\n\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    l_eq = cv2.equalizeHist(l)\n    lab_eq = cv2.merge((l_eq, a, b))\n    lab_eq_rgb = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2RGB)\n    l_clahe = gray_clahe.apply(l)\n    lab_clahe = cv2.merge((l_clahe, a, b))\n    lab_clahe_rgb = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n\n    # Figure 1: Images (2 rows x 3 cols)\n    plt.figure(figsize=(18, 8))\n    plt.subplot(2, 3, 1)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Original Grayscale')\n    plt.axis('off')\n    plt.subplot(2, 3, 2)\n    plt.imshow(gray_eq, cmap='gray')\n    plt.title('Histogram Equalized')\n    plt.axis('off')\n    plt.subplot(2, 3, 3)\n    plt.imshow(img_clahe, cmap='gray')\n    plt.title('CLAHE Grayscale')\n    plt.axis('off')\n\n    plt.subplot(2, 3, 4)\n    plt.imshow(img_rgb)\n    plt.title('Original Color (RGB)')\n    plt.axis('off')\n    plt.subplot(2, 3, 5)\n    plt.imshow(lab_eq_rgb)\n    plt.title('Histogram Equalized LAB Luminance')\n    plt.axis('off')\n    plt.subplot(2, 3, 6)\n    plt.imshow(lab_clahe_rgb)\n    plt.title('CLAHE LAB Luminance')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n    # Figure 2: Histograms corresponding to the images (2 rows x 3 cols)\n    plt.figure(figsize=(18, 8))\n    plt.subplot(2, 3, 1)\n    plt.hist(gray.ravel(), bins=256, color='gray')\n    plt.title('Original Grayscale Hist')\n    plt.subplot(2, 3, 2)\n    plt.hist(gray_eq.ravel(), bins=256, color='blue')\n    plt.title('Hist Eq Grayscale Hist')\n    plt.subplot(2, 3, 3)\n    plt.hist(img_clahe.ravel(), bins=256, color='red')\n    plt.title('CLAHE Grayscale Hist')\n\n    plt.subplot(2, 3, 4)\n    # histogram for original LAB luminance channel\n    plt.hist(l.ravel(), bins=256, color='gray', alpha=0.7, label='Luminance')\n    plt.title('Original LAB Luminance Hist')\n    plt.ylim(0, 30000)\n    plt.subplot(2, 3, 5)\n    plt.hist(l_eq.ravel(), bins=256, color='purple')\n    plt.title('Histogram Equalized LAB Luminance')\n    plt.subplot(2, 3, 6)\n    plt.hist(l_clahe.ravel(), bins=256, color='orange')\n    plt.title('CLAHE Lab Luminance')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinarization\nBinarization is the process of converting a grayscale image into a binary image, where each pixel is either black (0) or white (255). This is done by applying a threshold: pixels above the threshold become white, and those below become black.\n\nWhy Use Binarization?\n\nSimplifies analysis: Many image processing tasks (like shape analysis, object counting, and OCR) work better on binary images.\nSeparates foreground from background: Useful for segmenting objects from their surroundings.\nPreprocessing for algorithms: Many algorithms (e.g., contour detection, morphological operations) require binary input.\n\n\n\nHow Does Binarization Work?\n\nChoose a threshold value (e.g., 100).\nCompare each pixel’s intensity to the threshold.\n\nIf the pixel value &gt; threshold, set it to 255 (white).\nIf the pixel value ≤ threshold, set it to 0 (black).\n\n\n\n\nOtsu’s Method — Automatic Threshold Selection\nOtsu’s method is a popular technique for automatically finding the optimal threshold value. It works by: - Analyzing the histogram of pixel intensities. - Finding the threshold that minimizes the variance within each class (foreground and background), or equivalently, maximizes the separation between them.\nAdvantages: - No manual tuning needed. - Works well when the image has a clear bimodal histogram (two peaks: one for background, one for foreground).\nIn practice:\nOtsu’s method is widely used for document scanning, medical imaging, and any scenario where robust, automatic binarization is needed.\n\nif img is not None:\n    # Show histogram\n    plt.figure(figsize=(10, 4))\n    plt.hist(gray.ravel(), bins=256, color='gray')\n    plt.title('Grayscale Histogram')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    # Otsu's thresholding\n    otsu_thresh, binary_img = cv2.threshold(\n        gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    plt.axvline(otsu_thresh, color='red', linestyle='dashed',\n                label=f'Otsu Threshold = {otsu_thresh:.0f}')\n    plt.legend()\n    plt.show()\n    # Show binarized image\n    plt.imshow(binary_img, cmap='gray')\n    plt.title('Binarized Image (Otsu Threshold)')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMorphological Operations\nMorphological operations are image processing techniques that probe and modify the shapes of objects in binary or grayscale images. The two most common operations are erosion and dilation.\n\nWhat is Erosion?\n\nPurpose: Erosion shrinks bright regions and removes small white noise. It is useful for eliminating tiny artifacts, separating objects that are close together, and reducing the size of foreground objects.\nHow it works: Erosion slides a small shape (called a structuring element or kernel) over the image. At each position, if all pixels under the kernel are bright (e.g., white in binary images), the output pixel remains bright; otherwise, it becomes dark. This causes boundaries of bright regions to shrink.\n\n\n\nWhat is Dilation?\n\nPurpose: Dilation expands bright regions and fills small holes or gaps. It is useful for joining broken parts of objects, making features thicker, and connecting nearby objects.\nHow it works: Dilation also slides a kernel over the image. At each position, if any pixel under the kernel is bright, the output pixel becomes bright. This causes boundaries of bright regions to grow outward.\n\n\n\nPractical Use\n\nNoise removal: Erosion followed by dilation (called opening) removes small noise while preserving object shape.\nObject joining: Dilation followed by erosion (called closing) fills small holes and connects nearby objects.\nParameter tuning: The size and shape of the kernel control the strength and direction of the effect. Larger kernels produce stronger changes.\n\nTip: Try different kernel sizes and shapes to see how they affect your image. Use erosion to clean up noise, and dilation to restore or connect features.\n\n# Erosion and dilation on binarized image\nif img is not None:\n    plt.figure(figsize=(15, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(binary_img, cmap='gray')\n    plt.title('Original Binary')\n    plt.axis('off')\n\n    # 15x15 kernel for demo. Adjust size as needed.\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n\n    eroded_bin = cv2.erode(binary_img, kernel, iterations=1)\n    dilated_bin = cv2.dilate(binary_img, kernel, iterations=1)\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(eroded_bin, cmap='gray')\n    plt.title('Erosion (15x15)')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(dilated_bin, cmap='gray')\n    plt.title('Dilation (15x15)')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nEdge Detection\nEdge detection is a fundamental technique in image processing and computer vision. Edges represent boundaries where pixel intensities change sharply, often corresponding to object outlines, texture changes, or surface discontinuities.\n\nWhy Detect Edges?\n\nObject boundaries: Edges help segment objects from the background.\nFeature extraction: Many algorithms use edges to identify shapes, corners, and regions of interest.\nImage understanding: Edges simplify images, making it easier to analyze and interpret content.\n\n\n\nHow Does Edge Detection Work?\nEdge detectors analyze local intensity changes in an image. They highlight pixels where the difference between neighboring values is large. Common approaches include: - Gradient-based methods: Compute the rate of change (gradient) in intensity. Examples: Sobel, Prewitt, Roberts. - Canny edge detector: A multi-stage algorithm that smooths the image, finds gradients, applies non-maximum suppression, and uses double thresholding to select strong and weak edges.\n\n\nCanny Edge Detector\nThe Canny method is popular because it produces clean, thin edges and reduces noise. It uses two thresholds: - Low threshold: Detects weak edges. - High threshold: Detects strong edges. Edges connected to strong edges are kept; isolated weak edges are discarded.\nTip: Adjust thresholds to control edge sensitivity. Lower values reveal more edges (including noise); higher values show only the most prominent boundaries.\nEdge detection is a key step for tasks like segmentation, tracking, and recognition.\n\n# Compare Sobel gradient (simpler) with Canny edge detection (more complex)\nif img is not None:\n    # Sobel gradient (X and Y)\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n    sobel_mag = cv2.magnitude(sobelx, sobely)\n    sobel_mag = cv2.convertScaleAbs(sobel_mag)\n\n    # Canny edge detection\n    edges_canny = cv2.Canny(gray, 200, 400)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(sobel_mag, cmap='gray')\n    plt.title('Sobel Gradient Magnitude')\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(edges_canny, cmap='gray')\n    plt.title('Canny Edge Detection')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nObject Detection and Face Detection\nModern image processing leverages deep learning models to automatically detect and localize objects and faces in images and videos. Two popular approaches are YOLO (You Only Look Once) for general object detection and MediaPipe Face Detection for fast, lightweight face localization.\n\nYOLO Object Detection\n\nYOLO is a family of real-time object detectors that predict bounding boxes and class labels for multiple objects in a single pass through the image.\nHow it works: YOLO divides the image into a grid and simultaneously predicts bounding boxes and class probabilities for each cell.\nAdvantages: Fast, accurate, and suitable for real-time applications such as video analysis, robotics, and surveillance.\nUse cases: Detecting people, vehicles, animals, and everyday objects in images or video frames.\n\n\n# Step 3: Object Detection with YOLO\nfrom ultralytics import YOLO\n\n# The YOLO model loaded above (yolo_model) is a pre-trained deep learning object detector from the Ultralytics YOLO family.\n# YOLO (\"You Only Look Once\") models are designed for real-time object detection in images and videos.\n# They predict bounding boxes and class labels for multiple objects in a single forward pass.\n# The model file 'yolo11n.pt' is a specific version/size of YOLO.\n\nyolo_model = YOLO('yolo11n.pt')\n\n# Load the first frame of the video file and use it as the image\nvideo_path = 'GX010881.MP4'  # Change to your video file name if needed\ncap = cv2.VideoCapture(video_path)\nret, frame = cap.read()\ncap.release()\nif ret:\n    img = frame\n    # Run YOLO detection on the first frame\n    results = yolo_model(img)\n    img_detected = img.copy()\n    for result in results:\n        for box in result.boxes:\n            class_id = int(box.cls[0])\n            confidence = float(box.conf[0])\n            class_name = yolo_model.names[class_id]\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\n            if confidence &gt; 0.2:\n                cv2.rectangle(img_detected, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(img_detected, f'{class_name} {confidence:.2f}',\n                            (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n    img_detected_rgb = cv2.cvtColor(img_detected, cv2.COLOR_BGR2RGB)\n    plt.imshow(img_detected_rgb)\n    plt.title('YOLO Object Detection (First Video Frame)')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Could not load first frame from video.')\n\n\n\nMediaPipe Face Detection\n\nMediaPipe is a framework from Google for building cross-platform ML pipelines. Its face detection model is designed for real-time applications and works well on both close-up and wide-angle scenes.\nHow it works: The model uses machine learning to find faces and estimate their bounding boxes, returning confidence scores for each detection.\nAdvantages: Lightweight, fast, and robust to variations in pose and lighting.\nUse cases: Face tracking, privacy blurring, selfie enhancement, and interactive applications.\n\n\n# Step 4: Face Detection with MediaPipe\nimport mediapipe as mp\n\n# The MediaPipe Face Detection model is a lightweight, real-time face detector from Google.\n# It uses machine learning to find faces and estimate their bounding boxes in images or video frames.\n# The model is fast and works well for both close-up selfies and wider scenes, depending on the model_selection parameter.\n# It outputs detection objects with bounding box coordinates and confidence scores.\nmp_face_detection = mp.solutions.face_detection\n\nif img is not None:\n    # Tip: model_selection=0 is tuned for close-up faces (e.g., selfies); model_selection=1 is for wider scenes.\n    # Tip: increase min_detection_confidence (0.0-1.0) to reduce false positives at the cost of missing faint faces.\n    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        results = face_detection.process(img_rgb)\n        img_faces = img_rgb.copy()\n        h_img, w_img = img_faces.shape[:2]\n        if results.detections:\n            for detection in results.detections:\n                bboxC = detection.location_data.relative_bounding_box\n                x = int(bboxC.xmin * w_img)\n                y = int(bboxC.ymin * h_img)\n                w = int(bboxC.width * w_img)\n                h = int(bboxC.height * h_img)\n                cv2.rectangle(img_faces, (x, y),\n                              (x + w, y + h), (255, 0, 0), 2)\n            plt.imshow(img_faces)\n            plt.title('MediaPipe Face Detection')\n            plt.axis('off')\n            plt.show()\n            # Note: each detection object has a score (detection.score). You can filter detections by inspecting that value.\n            print(f'Faces detected: {len(results.detections)}')\n        else:\n            print('No faces detected.')\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\nVideo Processing: Object and Face Detection with Privacy Blurring\nThe next cell defines and runs a function called process_video that processes a video file frame-by-frame. For each frame, it performs:\n\nObject Detection (YOLO):\nUses the YOLO model to detect people in the frame and draws green rectangles around them.\nFace Detection (MediaPipe):\nUses MediaPipe to detect faces, draws blue rectangles around each detected face, and applies a blur to the face region for privacy protection.\nInteractive Display:\nThe processed frames are displayed interactively in the notebook, allowing you to watch the detection and blurring in real time.\nRobust Face Tracking:\nIf a face is missed in a frame, the function keeps the last known face location for a few frames to maintain privacy blurring even during brief detection failures.\n\nThis workflow demonstrates how to combine deep learning-based object detection and lightweight face detection for privacy-aware video analysis in Python.\n\nfrom IPython.display import display, clear_output\nimport contextlib\nimport os\nfrom ultralytics import YOLO\n\n\ndef _get_valid_ksize(fw, fh, base=51):\n    ksize = min(fw, fh, base)\n    ksize = max(ksize, 3)\n    if ksize % 2 == 0:\n        ksize += 1\n    return int(ksize)\n\n\ndef process_video(video_path, max_frames=30, max_missed=5):\n    yolo_model = YOLO('yolo11n.pt')\n    mp_face_detection = mp.solutions.face_detection\n    cap = cv2.VideoCapture(video_path)\n    last_face_boxes = []\n    missed_frames = 0\n    frames_shown = 0\n    fig, ax = plt.subplots(figsize=(10, 6))\n    img_plot = None\n    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n        while cap.isOpened() and frames_shown &lt; max_frames:\n            ret, frame = cap.read()\n            if not ret:\n                print('End of video or cannot read the frame.')\n                break\n            # YOLO person detection (suppress output)\n            with open(os.devnull, 'w') as fnull, contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n                results = yolo_model(frame, verbose=False)\n            for result in results:\n                for box in result.boxes:\n                    class_id = int(box.cls[0])\n                    confidence = float(box.conf[0])\n                    class_name = yolo_model.names[class_id]\n                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n                    if confidence &gt; 0.5 and class_name == 'person':\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (0, 255, 0), 2)\n                        cv2.putText(frame, f'Person {confidence:.2f}', (\n                            x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n            # MediaPipe face detection\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            results = face_detection.process(rgb_frame)\n            h_img, w_img = frame.shape[:2]\n            current_face_boxes = []\n            if results.detections:\n                for detection in results.detections:\n                    bboxC = detection.location_data.relative_bounding_box\n                    x = int(bboxC.xmin * w_img)\n                    y = int(bboxC.ymin * h_img)\n                    w = int(bboxC.width * w_img)\n                    h = int(bboxC.height * h_img)\n                    x1 = max(x, 0)\n                    y1 = max(y, 0)\n                    x2 = min(x + w, w_img)\n                    y2 = min(y + h, h_img)\n                    current_face_boxes.append((x1, y1, x2, y2))\n            if current_face_boxes:\n                last_face_boxes = current_face_boxes\n                missed_frames = 0\n            else:\n                missed_frames += 1\n            # Draw blue rectangle, add text, and blur face for each detected face\n            if last_face_boxes and missed_frames &lt;= max_missed:\n                for (x1, y1, x2, y2) in last_face_boxes:\n                    face_roi = frame[y1:y2, x1:x2]\n                    fh, fw = face_roi.shape[:2]\n                    ksize = _get_valid_ksize(\n                        fw, fh, base=51 + 10 * missed_frames)\n                    if face_roi.size &gt; 0 and ksize &gt; 1:\n                        blurred_face = cv2.GaussianBlur(\n                            face_roi, (ksize, ksize), 0)\n                        frame[y1:y2, x1:x2] = blurred_face\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (255, 0, 0), 2)\n                        cv2.putText(frame, 'Face', (x1, y1-10),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n            # Update the image plot\n            clear_output(wait=True)\n            if img_plot is None:\n                img_plot = ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                ax.axis('off')\n            else:\n                img_plot.set_data(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                fig.canvas.draw_idle()\n            display(plt.gcf())\n            frames_shown += 1\n    cap.release()\n\n\nprocess_video('test_video.MP4')  # Change to your video file name"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Conducting community-based evaluations of adapted exercise modes using wearable sensors and musculoskeletal modeling to assess musculoskeletal safety and participant enjoyment.\nData collection combines graded exercise testing on a custom wheelchair ergometer (following ACSM protocols) with personalized workouts performed in community settings.\nThe study recruits participants across a broad age range and varied activity backgrounds (athletic and non‑athletic) to improve generalizability.\nPrimary outcomes include shoulder loading and propulsion metrics derived from musculoskeletal modeling and sensor data, alongside self-reported enjoyment to compare exercise modes.\nPresentation: Evaluation of propulsion biomechanics during exercise with a suspended-wheel everyday wheelchair (American Society of Biomechanics Annual Meeting, 2024)"
  },
  {
    "objectID": "projects.html#evaluation-of-adapted-exercise-modes",
    "href": "projects.html#evaluation-of-adapted-exercise-modes",
    "title": "Projects",
    "section": "",
    "text": "Conducting community-based evaluations of adapted exercise modes using wearable sensors and musculoskeletal modeling to assess musculoskeletal safety and participant enjoyment.\nData collection combines graded exercise testing on a custom wheelchair ergometer (following ACSM protocols) with personalized workouts performed in community settings.\nThe study recruits participants across a broad age range and varied activity backgrounds (athletic and non‑athletic) to improve generalizability.\nPrimary outcomes include shoulder loading and propulsion metrics derived from musculoskeletal modeling and sensor data, alongside self-reported enjoyment to compare exercise modes.\nPresentation: Evaluation of propulsion biomechanics during exercise with a suspended-wheel everyday wheelchair (American Society of Biomechanics Annual Meeting, 2024)"
  },
  {
    "objectID": "projects.html#smartwatch-based-wheelchair-propulsion-monitoring",
    "href": "projects.html#smartwatch-based-wheelchair-propulsion-monitoring",
    "title": "Projects",
    "section": "Smartwatch-Based Wheelchair Propulsion Monitoring",
    "text": "Smartwatch-Based Wheelchair Propulsion Monitoring\n\nDeveloping a novel method to predict manual wheelchair propulsion kinetics using only smartwatch sensor data. This work addresses the limitations of gold-standard devices (e.g., the SMARTWheel), which are no longer widely available and are restricted to laboratory use. The system aims to predict all six components of hand reaction loads (three forces and three moments) and to enable calculation of common wheelchair propulsion metrics. This pilot study is evaluating whether consumer-grade wearables can provide accurate, field-deployable measures of biomechanical loading to support injury prevention and rehabilitation for manual wheelchair users.\n\nPresentation: Predicting manual wheelchair propulsion kinetics using smartwatch data: a pilot study (MRC Annual Symposium, Washington University in St. Louis, 2025)"
  },
  {
    "objectID": "projects.html#handcycling-force-prediction-from-wearable-sensors",
    "href": "projects.html#handcycling-force-prediction-from-wearable-sensors",
    "title": "Projects",
    "section": "Handcycling Force Prediction from Wearable Sensors",
    "text": "Handcycling Force Prediction from Wearable Sensors\n\nInvestigated the feasibility of predicting continuous hand reaction forces during handcycling using only arm segment kinematics. This work compared multiple machine learning architectures including temporal convolutional networks (TCN), residual networks, and ensemble methods against traditional statistical approaches. The TCN model achieved correlation coefficients up to r=0.97 for in-plane forces, enabling replacement of instrumented crank handles in future studies.\n\nPublication: Kinematics-Based Predictions of External Loads during Handcycling (Sensors, 2024)\nPresentation: Estimating hand reaction forces from arm segment accelerations during handcycle propulsion using machine learning (Congress of the International Society of Biomechanics, 2023)"
  },
  {
    "objectID": "projects.html#flexible-multi-camera-markerless-motion-capture-pipeline",
    "href": "projects.html#flexible-multi-camera-markerless-motion-capture-pipeline",
    "title": "Projects",
    "section": "Flexible Multi-Camera Markerless Motion Capture Pipeline",
    "text": "Flexible Multi-Camera Markerless Motion Capture Pipeline\n\nDeveloping a scalable motion capture system that works with any N≥2 consumer cameras to achieve research-grade 3D motion tracking. The complete pipeline includes:\n\nCamera Calibration: Estimating intrinsic parameters for each camera using Zhang’s method with planar calibration targets\nLens Distortion Correction: Applying radial and tangential distortion models to correct image artifacts\nCamera Pose Estimation: Solving the Perspective-n-Point (PnP) problem to determine extrinsic camera parameters\nKeypoint Tracking: Using deep learning-based keypoint detection (DeepLabCut) to track anatomical landmarks across multiple views\n3D Reconstruction: Triangulating 2D keypoints into accurate 3D coordinates using multi-view geometric constraints"
  },
  {
    "objectID": "projects.html#machine-learning-for-equine-ground-reaction-force-estimation",
    "href": "projects.html#machine-learning-for-equine-ground-reaction-force-estimation",
    "title": "Projects",
    "section": "Machine Learning for Equine Ground Reaction Force Estimation",
    "text": "Machine Learning for Equine Ground Reaction Force Estimation\n\n\nEstimated equine ground reaction forces using kinematic and anthropometric data with machine learning models.\nDeveloped statistical approaches to predict loading during walking and trotting, providing non-invasive methods for veterinary biomechanical assessment.\nPublication: Statistical approaches for estimating forelimb ground reaction forces in foals during walking and trotting (Journal of Biomechanics, 2025 — Submitted)"
  },
  {
    "objectID": "projects.html#finite-element-analysis-of-equine-bone-fracture-risk",
    "href": "projects.html#finite-element-analysis-of-equine-bone-fracture-risk",
    "title": "Projects",
    "section": "Finite Element Analysis of Equine Bone Fracture Risk",
    "text": "Finite Element Analysis of Equine Bone Fracture Risk\n\n\nAnalyzed high fracture risk regions of equine bone using subject-specific finite element models.\nCreated detailed models from CT data to understand stress distribution in fracture-prone areas of the third metacarpal.\nPresentation: Bone quality differences in fracture-prone regions of the equine third metacarpal (Orthopaedic Research Society, 2021)"
  }
]