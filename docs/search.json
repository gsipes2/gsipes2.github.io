[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial\nInteractive Introduction to Image Processing"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Open IntroductionImageProcessing in Colab\n\nThis site can include Jupyter notebooks (.ipynb) in two common ways.\n\nRender the notebook as a standalone page using Quarto\n\nPlace notebooks in a notebooks/ folder (e.g. notebooks/example.ipynb).\nRender it with quarto render notebooks/example.ipynb which will produce an HTML page in _site/ (or the site output folder).\nLink to the produced HTML from your site navigation or pages.\n\nInclude notebook content inside a .qmd page (convert or execute)\n\nYou can include the notebook output by converting it to Quarto-compatible content or by embedding the HTML output.\nExample: in a .qmd file you can use an iframe to embed the rendered notebook HTML:\n\n\n&lt;iframe src=\"notebooks/example.html\" style=\"width:100%;height:800px;border:0\"&gt;&lt;/iframe&gt;\nTips:\n\nUse quarto render to convert .ipynb to HTML (Quarto will execute notebooks when rendering if execution is enabled).\nEnsure any data or resources used by the notebook are committed or in paths resolvable from the site root.\nIf you prefer dynamic execution during site builds, enable execute: true in the notebook’s YAML or in _quarto.yml.\n\nIf you want, I can also add a small example notebooks/example.ipynb and a build script to automatically render notebooks during site build. title: Notebooks title: Notebooks format: html\n\n\nOpen IntroductionImageProcessing in Colab Launch on Binder Open in JupyterLite (if available)\n\n\n\nOpen IntroductionImageProcessing in Colab\n\nThis site can include Jupyter notebooks (.ipynb) in two common ways.\n\nRender the notebook as a standalone page using Quarto\n\nPlace notebooks in a notebooks/ folder (e.g. notebooks/example.ipynb).\nRender it with quarto render notebooks/example.ipynb which will produce an HTML page in _site/ (or the site output folder).\nLink to the produced HTML from your site navigation or pages.\n\nInclude notebook content inside a .qmd page (convert or execute)\n\nYou can include the notebook output by converting it to Quarto-compatible content or by embedding the HTML output.\nExample: in a .qmd file you can use an iframe to embed the rendered notebook HTML:\n\n\n&lt;iframe src=\"notebooks/example.html\" style=\"width:100%;height:800px;border:0\"&gt;&lt;/iframe&gt;\nTips:\n\nUse quarto render to convert .ipynb to HTML (Quarto will execute notebooks when rendering if execution is enabled).\nEnsure any data or resources used by the notebook are committed or in paths resolvable from the site root.\nIf you prefer dynamic execution during site builds, enable execute: true in the notebook’s YAML or in _quarto.yml.\n\nIf you want, I can also add a small example notebooks/example.ipynb and a build script to automatically render notebooks during site build."
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html",
    "href": "notebooks/BalanceSignalProcessingTutorial.html",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "",
    "text": "Open in Colab"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#prerequisites",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#prerequisites",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic Python experience (pandas, numpy, matplotlib).\nFamiliarity with time series concepts (sampling rate, basic plotting) is helpful but not required — the notebook introduces essential signal‑processing ideas as we go."
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#learning-objectives",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#learning-objectives",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nLoad and inspect force‑plate time series (voltages).\nConvert voltages to forces/moments using a calibration matrix and amplifier settings.\nUse power spectral density (PSD) to choose data‑driven low‑pass filter cutoffs (cumulative power).\nApply zero‑phase Butterworth filtering.\nCompute Center of Pressure (CoP), visualize stabilograms, and calculate scalar sway metrics (AP/ML range, mean CoP speed, excursion statistics)."
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#notebook-flow",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#notebook-flow",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Notebook flow",
    "text": "Notebook flow\n\nEnvironment & helper functions (imports, plotting, download).\nData loading and basic inspection.\nSpectral methods to select filter cutoffs and filtering helpers.\nConvert voltages → forces/moments and compute CoP.\nVisualize stabilograms and compute balance metrics; compare conditions (e.g., eyes open vs closed)."
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#data-note",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#data-note",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Data note",
    "text": "Data note\n\nExample files in this notebook use tandem‑stance (one foot in front of the other) balance tests under two conditions (eyes‑open vs eyes‑closed) to illustrate typical contrasts. Replace the example URLs/files with your own data as needed."
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#reproducibility-reporting-reminders",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#reproducibility-reporting-reminders",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Reproducibility & reporting reminders",
    "text": "Reproducibility & reporting reminders\n\nRecord sampling rate, filter design (type, order), and cutoff selection method when sharing results.\n\n\nEnvironment setup — what this cell does and why it matters\nThis small section ensures the Python environment has the packages used in the notebook and documents why each is needed. Key points:\nNotes - On Colab the cell uses %pip install so dependencies are available\nTakeaway: Make sure imports succeed before continuing; the rest of the notebook assumes these packages are available.\n\n# Environment setup (Colab-friendly)\n%pip install numpy pandas scipy matplotlib requests ipywidgets --quiet\n\n# Standard library imports\nimport os  # For interacting with the operating system (e.g., file paths)\nimport io  # For handling streams and file-like objects\nfrom io import StringIO  # For working with string buffers as file-like objects\nimport logging\nfrom typing import Any, Dict, List, Optional, Sequence, Union\nimport requests\n\n# Third-party library imports\nimport numpy as np  # For numerical computations and array operations\nimport pandas as pd  # For data manipulation and analysis using DataFrames\nfrom scipy.signal import welch, butter, filtfilt, lfilter  # For signal processing (spectral density, filtering)\nimport matplotlib.pyplot as plt  # For creating plots and visualizations\nimport requests  # For making HTTP requests\nimport ipywidgets  # For creating interactive widgets in Jupyter notebooks\nimport ipywidgets as widgets  # IPython widgets with an alias for convenience\nimport IPython.display  # For rendering rich outputs (e.g., images, HTML)\n\n_LOGGER = logging.getLogger(__name__)\n\n# Jupyter-specific magic commands\n%matplotlib inline\n\n# Set default plot parameters settings\nplt.style.use('default')\nplt.rcParams['axes.titlesize'] = 16\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 'large'\nplt.rcParams['ytick.labelsize'] = 'large'\nplt.rcParams['font.family'] = 'Sans'\nplt.rcParams['mathtext.fontset'] = 'stix'\nplt.rcParams['figure.figsize'] = 8, 6\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['figure.autolayout'] = True\nplt.rcParams['axes.grid'] = True\n\n\n# Streaming download helper (same pattern as IntroductionImageProcessing.ipynb)\ndef download_file(url, save_path):\n    \"\"\"Download a file via streaming and save to disk. Returns the save_path.\"\"\"\n    r = requests.get(url, stream=True)\n    r.raise_for_status()\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            if chunk:\n                f.write(chunk)\n    return save_path\n\n# Example usage (replace with your Box static links):\n# download_file('https://uofi.box.com/shared/static/FILEID.txt', '/content/eo_data.txt')\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nDownload & load EO/EC data (what this cell does)\n\nDownloads two example force‑plate text files (EO/EC) from the provided URLs and saves them to local paths (colab-friendly: /content/eo_data.txt, /content/ec_data.txt).\nReads the files into pandas DataFrames using pd.read_csv(...) with:\n\ndelimiter='\\t', skiprows=6, and column names ['Time','VFx','VFy','VFz','VMx','VMy','VMz'].\n\nCoerces all columns to numeric (pd.to_numeric(..., errors='coerce')) so non‑numeric tokens become NaN.\nPrints the first rows of each DataFrame to allow a quick header/alignment check.\n\nNotes / things to check or change if using your own data - Replace EO_URL / EC_URL with your direct file links (Box direct‑download or other static URLs). - Adjust skiprows, delimiter, and names to match your file format if headers or metadata differ.\nAfter running, inspect: - df_EO.head(), df_EC.head() for column alignment, - df_EO.info(), df_EC.info() for dtypes and NaNs, - how many NaNs were created; decide to interpolate, fill, or drop before spectral/filters. - Files are saved under /content/ (Colab); change paths for a different environment.\n\nEO_URL = 'https://uofi.box.com/shared/static/elqg597eogilxkqc94kg1wjwfjzwtyzy.txt'\nEC_URL = 'https://uofi.box.com/shared/static/0a1pbfhmtxadpstrw8cdd07ziz26tar2.txt'\n\ndef download_file(url, save_path):\n    r = requests.get(url, stream=True)\n    r.raise_for_status()\n    with open(save_path, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n    return save_path\n\n# Local paths to save the downloaded files (Colab-friendly /content/)\neo_path = '/content/eo_data.txt'\nec_path = '/content/ec_data.txt'\n# Download files (streaming) and then read with pandas\ndownload_file(EO_URL, eo_path)\ndownload_file(EC_URL, ec_path)\n# Adjust these read_csv args to match your file format (delimiter, header rows, column names)\ncols = ['Time', 'VFx', 'VFy', 'VFz', 'VMx', 'VMy', 'VMz']\n# Read files robustly and coerce numeric types\ndf_EO = pd.read_csv(eo_path, delimiter='\\t', skiprows=6, names=cols, engine='python')\ndf_EC = pd.read_csv(ec_path, delimiter='\\t', skiprows=6, names=cols, engine='python')\n\n# Ensure all columns are numeric where possible (non-numeric -&gt; NaN)\nfor df in (df_EO, df_EC):\n    for c in cols:\n        df[c] = pd.to_numeric(df[c], errors='coerce')\n\nEyes Open Dataframe:\n\n    Time       VFx       VFy       VFz       VMx       VMy       VMz\n0  0.000  0.012344 -0.087344  2.551563 -1.753438  0.204531 -0.745938\n1  0.001  0.014687 -0.088281  2.552812 -1.759375  0.201094 -0.749375\n2  0.002  0.013281 -0.088281  2.553438 -1.759375  0.203437 -0.750938\n3  0.003  0.015000 -0.088438  2.552188 -1.758125  0.199844 -0.752344\n4  0.004  0.010156 -0.089844  2.551875 -1.761719  0.200937 -0.749531\n\n\nEyes Closed Dataframe:\n\n    Time       VFx       VFy       VFz       VMx       VMy       VMz\n0  0.000  0.001406 -0.084531  2.547188 -1.909531  0.104844 -0.735156\n1  0.001  0.000625 -0.087656  2.549375 -1.912813  0.105313 -0.734844\n2  0.002 -0.001250 -0.087656  2.547813 -1.913281  0.105625 -0.735938\n3  0.003 -0.002188 -0.087031  2.546250 -1.908906  0.106094 -0.731250\n4  0.004 -0.005469 -0.086406  2.549375 -1.909375  0.107813 -0.736875\n\n\n\n\nWhat is a force plate?\nA force plate (force platform) is a rigid instrumented plate that measures the forces and moments exchanged between a subject (or object) and the ground. It converts mechanical loads on its sensing elements (usually strain gauges or load cells) into electrical signals that are recorded as time‑series. Force plates are widely used in biomechanics, gait analysis, balance/postural control, ergonomics, and sports science.\n\n\nWhat it measures (primary signals)\n\nForces along three orthogonal axes:\n\nFx, Fy — shear (horizontal) forces (medial–lateral, anterior–posterior)\nFz — vertical ground reaction force\n\nMoments about the same three axes:\n\nMx, My, Mz — moments/torques (N·m)\n\n\nMany systems output raw voltages for the six channels (VFx, VFy, VFz, VMx, VMy, VMz) which must be converted to physical units using the calibration/sensitivity matrix and amplifier gain (as in this notebook).\n\n\nTypical outputs and data format\n\nTime series sampled at a fixed sampling rate (fs), e.g., 100–2000 Hz.\nPer-sample vectors: [Fx, Fy, Fz, Mx, My, Mz] (or voltages to convert).\nCommon file/column names: Time, VFx/VFx, VFy/Fy, VFz/Fz, VMx/Mx, VMy/My, VMz/Mz.\nUnits after conversion: Forces in newtons (N), moments in N·m; CoP usually reported in meters or mm.\n\n\n\nKey practical notes / pitfalls\n\nCalibration: raw voltages must be converted using the device’s sensitivity matrix and amplifier gain; check units carefully.\nSign conventions: verify force/moment sign conventions and plate origin/coordinate axes (affects CoP sign/direction).\nNoise & filtering: high‑frequency noise is common; apply appropriate low‑pass filtering (zero‑phase filtfilt preferred) and document cutoff/Nyquist.\nArtifacts: check for clipping, spikes, drifts; inspect PSD and time traces before automated processing.\nSampling: ensure fs is recorded and used consistently for filtering, differentiation (speed), and spectral analysis.\n\n\n\nTypical applications\n\nPostural/balance tests (eyes open/closed, tandem stance)\nGait and running kinetics (ground reaction forces)\nJump testing, landing mechanics\nClinical and rehabilitation assessments\n\n\n\nLet’s see what our force‑plate data looks like\nQuick checks to run now: - Inspect the first rows of EO/EC to confirm column alignment and Time indexing (df_EO.head(), df_EC.head()). - Draw diagnostic voltage traces to spot spikes, clipping, or offsets (use the plot_voltage_data(...) helper).\n\nprint(\"Eyes Open Dataframe:\\n\")\nprint(df_EO.head())\nprint(\"\\n\")\nprint(\"Eyes Closed Dataframe:\\n\")\nprint(df_EC.head())\n\nEyes Open Dataframe:\n\n    Time       VFx       VFy       VFz       VMx       VMy       VMz  \\\n0  0.000  0.012344 -0.087344  2.551563 -1.753438  0.204531 -0.745938   \n1  0.001  0.014687 -0.088281  2.552812 -1.759375  0.201094 -0.749375   \n2  0.002  0.013281 -0.088281  2.553438 -1.759375  0.203437 -0.750938   \n3  0.003  0.015000 -0.088438  2.552188 -1.758125  0.199844 -0.752344   \n4  0.004  0.010156 -0.089844  2.551875 -1.761719  0.200937 -0.749531   \n\n         Fx        Fy          Fz         Mx        My         Mz  \n0  1.691807 -7.492642  728.781669 -64.231260  5.374828 -11.761739  \n1  1.864985 -7.553040  729.139203 -64.447059  5.275992 -11.815882  \n2  1.762143 -7.553160  729.317995 -64.447298  5.343027 -11.839789  \n3  1.886389 -7.561766  728.960631 -64.401112  5.240377 -11.862005  \n4  1.537110 -7.662102  728.872340 -64.532107  5.271112 -11.819274  \n\n\nEyes Closed Dataframe:\n\n    Time       VFx       VFy       VFz       VMx       VMy       VMz  \\\n0  0.000  0.001406 -0.084531  2.547188 -1.909531  0.104844 -0.735156   \n1  0.001  0.000625 -0.087656  2.549375 -1.912813  0.105313 -0.734844   \n2  0.002 -0.001250 -0.087656  2.547813 -1.913281  0.105625 -0.735938   \n3  0.003 -0.002188 -0.087031  2.546250 -1.908906  0.106094 -0.731250   \n4  0.004 -0.005469 -0.086406  2.549375 -1.909375  0.107813 -0.736875   \n\n         Fx        Fy          Fz         Mx        My         Mz  \n0  1.022489 -7.110280  727.538129 -69.898703  2.507629 -11.615878  \n1  0.966270 -7.337180  728.164786 -70.018846  2.520398 -11.612384  \n2  0.830318 -7.333592  727.718632 -70.035494  2.529457 -11.629300  \n3  0.760869 -7.294226  727.271674 -69.876284  2.543203 -11.556364  \n4  0.521829 -7.247805  728.163108 -69.893502  2.591798 -11.643391  \n\n\n\n\nPlotting helper — how to use these diagnostic plots\nThe plotting helper draws voltage traces for all force (Fx,Fy,Fz) and moment (Mx,My,Mz) channels for EO and EC. Use it as a first pass to: - Check relative amplitudes and variability between conditions (e.g., increased sway in EC). - Spot spikes, clipping, or missing segments that must be handled before filtering. - Select representative time windows for spectral analysis and PSD estimation (avoid start/end transients).\nPractical tip: plot short windows (10–30 s) to inspect detail, and longer windows to assess overall signal\n\n# Plotting function\ndef plot_voltage_data(df_EO, df_EC, time_range=(15, 30), figsize=(10, 8), dpi=100, pad=2.0):\n    \"\"\"\n    Plot voltage data from two dataframes (EO and EC) over a specified time range.\n\n    This function generates six subplots corresponding to force (Fx, Fy, Fz) and moment (Mx, My, Mz) voltage signals, comparing data from two different conditions (EO and EC).\n\n    Args:\n        df_EO (DataFrame): Data containing EO (Eyes Open) condition, with columns 'Time', 'VFx', 'VFy', 'VFz', 'VMx', 'VMy', 'VMz'.\n        df_EC (DataFrame): Data containing EC (Eyes Closed) condition, with the same columns as `df_EO`.\n        time_range (tuple, optional): The time range (start, end) in seconds for plotting. Defaults to (15, 30).\n        figsize (tuple, optional): The figure size as (width, height). Defaults to (10, 8).\n        dpi (int, optional): Dots per inch (DPI) setting for the figure resolution. Defaults to 100.\n        pad (float, optional): Padding for `tight_layout` to adjust subplot spacing. Defaults to 2.0.\n\n    Raises:\n        ValueError: If the input dataframes do not contain the required voltage columns.\n\n    Example:\n        &gt;&gt;&gt; plot_voltage_data(df_EO, df_EC, time_range=(10, 25))\n    \"\"\"\n    # Create subplots\n    fig, axs = plt.subplots(6, 1, sharex=True, sharey=False, dpi=dpi, figsize=figsize)\n\n    # Plot data for each subplot\n    for i, (col, title) in enumerate(zip(['VFx', 'VFy', 'VFz', 'VMx', 'VMy', 'VMz'],\n                                         ['Fx voltage', 'Fy voltage', 'Fz voltage', 'Mx voltage', 'My voltage', 'Mz voltage'])):\n        axs[i].plot(df_EO['Time'], df_EO[col], color='tab:blue', alpha=1.0, label='EO')\n        axs[i].plot(df_EC['Time'], df_EC[col], color='tab:orange', alpha=1.0, label='EC')\n        axs[i].legend(bbox_to_anchor=(1.1, 1), loc='upper right')\n        axs[i].set_title(title)\n\n    # Set x-axis label and limits\n    plt.xlabel('Time [s]')\n    plt.xlim(time_range)\n\n    # Set y-axis label for the middle subplot\n    axs[2].set_ylabel('Voltage [V]')\n    fig.align_ylabels()\n\n    # Adjust layout\n    plt.tight_layout(pad=pad)\n\n    # Show the plot\n    plt.show()\n\n# Here we will only plot between time t=5s and t=15s\nplot_voltage_data(df_EO, df_EC, time_range=(5, 15), figsize=(10, 8), dpi=100, pad=2.0)\n\n\n\n\n\n\n\n\n\n\nSpectral analysis helper — interpretation and practical use\nThis helper uses Welch’s method to compute a power spectral density (PSD) and finds a cutoff frequency where the cumulative power exceeds a chosen percentage (e.g., 95–99%). Use it to pick data‑driven low‑pass cutoffs.\nKey points: - Low-frequency peaks often correspond to physiologically meaningful sway; high-frequency broadband tails are usually noise. - Choosing a cumulative power threshold keeps most signal energy while excluding high-frequency noise — common thresholds are 95%–99%. - For short signals, PSD estimates can be noisy; increase segment length (if available) to improve resolution.\nDiagnostics (use plot=True): - Check the PSD (semilogy) for dominant components and the cumulative power curve for a clear knee where energy levels off. - Inspect the vertical line indicating the selected cutoff — it should follow main low-frequency peaks but precede broadband noise.\nTakeaway: prefer a visually justified, data-driven cutoff rather than an arbitrary frequency; the helper provides both the numeric cutoff and diagnostic plots to justify it.\n\n# This a helper function for determining cutoff frequencies using cumulative power spectrum thresholding\ndef spectral_analysis(\n    data: Union[np.ndarray, pd.Series, pd.DataFrame, Sequence[float]],\n    sampling_freq: float,\n    nperseg: Optional[int] = None,\n    window: str = \"hann\",\n    threshold: float = 99,\n    plot: bool = False,\n) -&gt; float:\n    \"\"\"Estimate a cutoff frequency from the power spectral density of a 1-D signal.\n\n    The function computes a Welch PSD and finds the frequency at which the\n    cumulative power (from low to high frequency) exceeds ``threshold`` percent\n    of the total power.\n\n    Parameters\n    ----------\n    data:\n        1-D signal to analyze. Accepts a NumPy array, pandas Series or a 1-column\n        DataFrame. Passing a multi-column DataFrame will raise ``ValueError``.\n    sampling_freq:\n        Sampling frequency of the signal in Hz.\n    nperseg:\n        Length of each segment used by Welch's method. If ``None`` defaults to\n        the signal length. Larger values give better frequency resolution but\n        fewer averages.\n    window:\n        Window name passed to ``scipy.signal.welch`` (default: ``'hann'``).\n    threshold:\n        Percentage (0-100) of cumulative power to use as the cutoff. Default is\n        99 (i.e. when 99% of the power is reached).\n    plot:\n        If True, display diagnostic plots using matplotlib. Specifically,\n        ``spectral_analysis`` shows two stacked subplots: the PSD (semilogy)\n        and the cumulative power (%) with a vertical dashed line at the\n        detected cutoff frequency. The function also logs the frequency\n                vector shape and the selected cutoff index at DEBUG level when\n                plotting. Note that matplotlib must be available and a suitable\n                display/backend present for figures to appear.\n\n        Notes on threshold\n        ------------------\n        - ``threshold`` must be within (0, 100]. Passing values outside this\n            range raises ``ValueError``. ``threshold`` is interpreted as a\n            percentage of cumulative power (0-100).\n\n    Returns\n    -------\n    float\n        The cutoff frequency in Hz where the cumulative power first exceeds\n        ``threshold``. If the threshold is never exceeded (e.g., threshold &gt; 100\n        or numerical issues), the function returns the lowest frequency ``f[0]``.\n\n    Raises\n    ------\n    ValueError\n        If ``data`` is empty, if ``sampling_freq`` is not positive, or if a\n        multi-column DataFrame is provided.\n\n    Notes\n    -----\n    - The function does not currently handle NaNs; callers should pre-process\n      the signal (interpolate or drop) before calling.\n    - The return type is a Python float (converted from NumPy types when\n      necessary).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.linspace(0, 1.0, 1000, endpoint=False)\n    &gt;&gt;&gt; x = np.sin(2*np.pi*5*t)\n    &gt;&gt;&gt; spectral_analysis(x, sampling_freq=1000, threshold=90)\n    5.0\n    \"\"\"\n\n    # Validate threshold\n    if not (0 &lt; float(threshold) &lt;= 100):\n        raise ValueError(\"threshold must be between 0 (exclusive) and 100 (inclusive)\")\n\n    # Convert input to 1D numpy array\n    if isinstance(data, pd.DataFrame):\n        if data.shape[1] == 1:\n            data = data.iloc[:, 0]\n        else:\n            raise ValueError(\n                \"spectral_analysis expects a 1D signal, not a multi-column DataFrame.\")\n    if isinstance(data, pd.Series):\n        data = data.values\n    elif isinstance(data, list):\n        data = np.array(data)\n\n    if len(data) == 0:\n        raise ValueError(\"Input data is empty\")\n\n    if sampling_freq &lt;= 0:\n        raise ValueError(\"Sampling frequency must be positive\")\n\n    if nperseg is None:\n        nperseg = len(data)\n\n    f, Pxx = welch(data, fs=sampling_freq, nperseg=nperseg, window=window)\n\n    # Calculate cumulative power spectrum\n    cumulative_power = np.cumsum(Pxx)\n\n    # Normalize cumulative power to get a percentage\n    cumulative_power_percent = cumulative_power / cumulative_power[-1] * 100\n\n    # Find the index where the cumulative power levels off\n    cutoff_index = np.argmax(\n        cumulative_power_percent &gt; threshold\n    )  # Use the specified threshold\n\n    if plot == True:\n        # Plot the results\n        plt.figure(figsize=(12, 6))\n\n        # Plot Power Spectral Density\n        plt.subplot(2, 1, 1)\n        plt.semilogy(f, Pxx)\n        plt.title('Power Spectral Density')\n        plt.xlabel('Frequency (Hz)')\n        plt.ylabel('Power/Frequency (dB/Hz)')\n        plt.grid(True)\n\n        # Plot Cumulative Power Spectrum\n        plt.subplot(2, 1, 2)\n        plt.plot(f, cumulative_power_percent)\n        plt.axvline(x=f[cutoff_index], color='r', linestyle='--',\n                    label=f'Cutoff Frequency: {f[cutoff_index]:.2f} Hz')\n        plt.title('Cumulative Power Spectrum')\n        plt.xlabel('Frequency (Hz)')\n        plt.ylabel('Cumulative Power (%)')\n        plt.legend()\n        plt.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n    _LOGGER.debug(\"spectral_analysis: f.shape=%s, cutoff_index=%s\", f.shape, cutoff_index)\n\n    return f[cutoff_index]\n\n\n\nFiltering helper — summary and expected behavior\nThis helper centralizes realistic filtering choices: cutoff estimation, NaN handling, and application of a zero‑phase Butterworth filter when possible.\nMain behaviors: - Accepts lists, NumPy arrays, pandas Series/DataFrames and preserves the input shape/type on return where practical. - NaN handling: 'raise' (error), 'interpolate' (linear/interpolate + edge fill), or 'fill' (constant). Interpolate is a reasonable default for isolated missing samples. - Cutoff selection: custom_cutoff_frequency forces a fixed cutoff; otherwise per‑channel cutoffs are estimated by spectral_analysis using a cumulative power threshold. - filtfilt (zero‑phase) is preferred; for very short signals the code falls back to lfilter and logs a warning (this introduces phase shift).\nVerification checklist after filtering: - Plot raw vs filtered traces to ensure smoothing without removing expected low‑frequency structure. - Confirm cutoff frequencies are below Nyquist and in a sensible range for your task (e.g., &lt; fs/2).\nWhat is the Nyquist frequency (brief)? - Definition: Nyquist frequency = fs / 2, where fs is the sampling rate (Hz).\nExample: if fs = 1000 Hz, Nyquist = 500 Hz. - Why it matters: the Nyquist frequency is the highest frequency that can be represented without aliasing according to the Nyquist–Shannon sampling theorem. Frequencies above Nyquist will fold (alias) into lower frequencies and corrupt your signal. - Practical implication for filtering: Never design a low‑pass cutoff &gt;= Nyquist. In practice, clamp cutoffs safely below Nyquist (the helper uses 95% of Nyquist when necessary).\n\n# This is a helper function for filtering timeseries data\ndef filter_timeseries_data(\n    data: Union[List[float], np.ndarray, pd.DataFrame, pd.Series],\n    sampling_freq: float,\n    custom_cutoff_frequency: Optional[float] = None,\n    threshold: float = 99,\n    plot: bool = False,\n    rolling_window: int = 10,\n    butter_order: int = 4,\n    nan_policy: str = \"interpolate\",\n    nan_fill_value: Optional[float] = None,\n) -&gt; Union[List[float], np.ndarray, pd.DataFrame, pd.Series]:\n    \"\"\"Apply a low-pass Butterworth filter to time-series data.\n\n    The function accepts lists, 1-D NumPy arrays, 2-D NumPy arrays with shape\n    (n_samples, n_channels), pandas Series, or pandas DataFrame. When\n    ``custom_cutoff_frequency`` is not given, a cutoff frequency is estimated\n    per channel using :func:`spectral_analysis` with the provided ``threshold``.\n\n    Parameters\n    ----------\n    data:\n        Input time-series. Supported types:\n        - ``list`` (treated as 1-D signal, returns ``list``)\n        - ``np.ndarray`` (1-D or 2-D). For 2-D arrays filtering is applied to\n          each column and the resulting array has the same shape.\n        - ``pandas.Series`` (returns ``pandas.Series`` with the same index/name)\n        - ``pandas.DataFrame`` (filters each column and returns a DataFrame\n          preserving index and columns)\n    sampling_freq:\n        Sampling frequency in Hz.\n    custom_cutoff_frequency:\n        If provided, this scalar cutoff frequency (Hz) is used for all\n        channels. Otherwise, the cutoff is estimated independently per channel\n        using ``spectral_analysis`` and the given ``threshold``.\n    threshold:\n        Cumulative power percentage (0-100) used by ``spectral_analysis`` when\n        estimating cutoffs.\n    plot:\n        If True, enables plotting inside the underlying ``spectral_analysis``\n        calls and shows the PSD/cumulative plots. Plotting is only triggered\n        when cutoffs are estimated automatically (i.e., ``custom_cutoff_frequency``\n        is ``None``). For multi-channel inputs this may display one figure per\n        channel and can be slow or produce many windows; set ``plot=False`` to\n        suppress plotting.\n    rolling_window:\n        Integer window size used to apply a simple moving average to any\n        pandas.Series inputs before filtering to reduce very short-term noise.\n    butter_order:\n        Integer order of the Butterworth filter. Defaults to 4 for a\n        4th-order low-pass filter. Increasing the order makes the filter\n        steeper but may introduce more ringing/artifacts.\n    nan_policy:\n        How to handle NaN values in the input data. Options:\n        - ``'raise'`` (default): raise ``ValueError`` if NaNs are present.\n        - ``'interpolate'``: interpolate NaNs along the time axis (linear\n          interpolation). Works for arrays and pandas objects.\n        - ``'fill'``: replace NaNs with ``nan_fill_value`` (must be provided).\n    nan_fill_value:\n        Value used to fill NaNs when ``nan_policy=='fill'``. If None and\n        ``nan_policy=='fill'``, a ValueError is raised.\n        Notes on filtering behavior\n        --------------------------\n        - The filter is designed with ``scipy.signal.butter`` using ``butter_order``\n            and applied with ``filtfilt`` for zero-phase filtering when possible.\n            For very short signals where ``filtfilt`` cannot be used the code falls\n            back to ``lfilter`` and logs a WARNING; this introduces a phase shift\n            relative to zero-phase filtering.\n        - ``butter_order`` is validated to be an integer &gt;= 1.\n\n    Returns\n    -------\n    list | np.ndarray | pandas.Series | pandas.DataFrame\n        Filtered data with the same type and shape as the input. For numpy\n        arrays, columns are treated along axis=1 (i.e., shape (n_samples,\n        n_channels)).\n\n    Raises\n    ------\n    ValueError\n        If ``data`` is not one of the supported types.\n\n    Notes\n    -----\n        - The filter is a Butterworth low-pass filter implemented via\n            ``scipy.signal.butter`` and applied with ``filtfilt`` for zero-phase\n            filtering. The default order is 4 (4th-order) but this can be changed\n            using the ``butter_order`` parameter.\n        - If the estimated or provided cutoff is &gt;= Nyquist (fs/2) it is clamped\n            to 95% of Nyquist. If it is &lt;= 0 it is clamped to 5% of Nyquist. These\n            behaviours are intentional to avoid invalid filter designs.\n            - NaN handling: use the ``nan_policy`` parameter to control behavior.\n                Options are ``'raise'`` (default), ``'interpolate'``, or ``'fill'``.\n                When ``'fill'`` is selected, ``nan_fill_value`` must be provided.\n        - Estimating cutoffs via spectral analysis for many channels may be slow;\n            provide ``custom_cutoff_frequency`` when possible for performance.\n\n    Examples\n    --------\n    Filter a 1-D numpy signal:\n\n    &gt;&gt;&gt; filtered = filter_timeseries_data(np.array(x), sampling_freq=1000)\n\n    Filter a pandas DataFrame (each column filtered independently):\n\n    &gt;&gt;&gt; filtered_df = filter_timeseries_data(df, sampling_freq=200)\n\n    \"\"\"\n    # Accept list, ndarray, DataFrame, Series\n    input_type = None\n    if isinstance(data, list):\n        # Convert list to numpy array\n        data = np.array(data)\n        input_type = 'list'\n    elif isinstance(data, np.ndarray):\n        input_type = 'ndarray'\n    elif isinstance(data, pd.DataFrame):\n        input_type = 'dataframe'\n    elif isinstance(data, pd.Series):\n        input_type = 'series'\n    else:\n        raise ValueError(\n            \"Unsupported data type. Please provide a list, NumPy array, pandas DataFrame, or pandas Series.\")\n\n    fs = float(sampling_freq)\n    # Validate butter_order\n    if not isinstance(butter_order, int) or butter_order &lt; 1:\n        raise ValueError(\"butter_order must be an integer &gt;= 1\")\n\n    # Validate nan handling policy\n    if nan_policy not in (\"raise\", \"interpolate\", \"fill\"):\n        raise ValueError(\"nan_policy must be one of 'raise', 'interpolate', or 'fill'\")\n\n    # Handle NaNs according to policy. For pandas objects use pandas methods;\n    # for numpy arrays use interpolation via numpy where appropriate.\n    if input_type in (\"dataframe\", \"series\"):\n        if nan_policy == \"raise\":\n            if (isinstance(data, pd.DataFrame) and data.isna().values.any()) or (\n                isinstance(data, pd.Series) and data.isna().any()\n            ):\n                _LOGGER.debug(\"NaNs detected in pandas input and nan_policy='raise' -&gt; raising ValueError\")\n                raise ValueError(\"NaN values found in input; set nan_policy to 'interpolate' or 'fill' to handle them\")\n        elif nan_policy == \"interpolate\":\n            # Linear interpolation along the index (time axis)\n            if isinstance(data, pd.DataFrame):\n                data = data.interpolate(axis=0).ffill().bfill()\n                _LOGGER.debug(\"Interpolated NaNs in pandas DataFrame input (axis=0) and applied ffill/bfill for edges\")\n                _LOGGER.info(\"Interpolated NaNs in pandas DataFrame input (nan_policy='interpolate')\")\n            else:\n                data = data.interpolate().ffill().bfill()\n                _LOGGER.debug(\"Interpolated NaNs in pandas Series input and applied ffill/bfill for edges\")\n                _LOGGER.info(\"Interpolated NaNs in pandas Series input (nan_policy='interpolate')\")\n        else:  # fill\n            if nan_fill_value is None:\n                _LOGGER.debug(\"nan_policy='fill' but nan_fill_value is None -&gt; raising ValueError\")\n                raise ValueError(\"nan_fill_value must be provided when nan_policy=='fill'\")\n            data = data.fillna(nan_fill_value)\n            _LOGGER.debug(\"Filled NaNs in pandas input with value=%s\", nan_fill_value)\n            _LOGGER.info(\"Filled NaNs in pandas input with value=%s (nan_policy='fill')\", nan_fill_value)\n    elif input_type in (\"ndarray\", \"list\"):\n        arr = np.asarray(data, dtype=float)\n        if arr.ndim == 1:\n            if np.isnan(arr).any():\n                if nan_policy == \"raise\":\n                    _LOGGER.debug(\"NaNs detected in 1D ndarray and nan_policy='raise' -&gt; raising ValueError\")\n                    raise ValueError(\"NaN values found in input array; set nan_policy to 'interpolate' or 'fill' to handle them\")\n                elif nan_policy == \"interpolate\":\n                    idx = np.arange(arr.size)\n                    mask = np.isfinite(arr)\n                    if not mask.all():\n                        arr = np.interp(idx, idx[mask], arr[mask])\n                        _LOGGER.debug(\"Interpolated NaNs in 1D ndarray input using numpy.interp\")\n                        _LOGGER.info(\"Interpolated NaNs in 1D ndarray input (nan_policy='interpolate')\")\n                else:  # fill\n                    if nan_fill_value is None:\n                        _LOGGER.debug(\"nan_policy='fill' but nan_fill_value is None -&gt; raising ValueError\")\n                        raise ValueError(\"nan_fill_value must be provided when nan_policy=='fill'\")\n                    arr = np.where(np.isnan(arr), nan_fill_value, arr)\n                    _LOGGER.debug(\"Filled NaNs in 1D ndarray input with value=%s\", nan_fill_value)\n                    _LOGGER.info(\"Filled NaNs in 1D ndarray input with value=%s (nan_policy='fill')\", nan_fill_value)\n            data = arr\n        else:\n            # 2D array: process each column\n            if np.isnan(arr).any():\n                if nan_policy == \"raise\":\n                    _LOGGER.debug(\"NaNs detected in 2D ndarray and nan_policy='raise' -&gt; raising ValueError\")\n                    raise ValueError(\"NaN values found in input array; set nan_policy to 'interpolate' or 'fill' to handle them\")\n                elif nan_policy == \"interpolate\":\n                    arr2 = arr.copy()\n                    for i in range(arr2.shape[1]):\n                        col = arr2[:, i]\n                        idx = np.arange(col.size)\n                        mask = np.isfinite(col)\n                        if not mask.all():\n                            arr2[:, i] = np.interp(idx, idx[mask], col[mask])\n                    data = arr2\n                    n_nans = int(np.isnan(arr).sum())\n                    _LOGGER.debug(\"Interpolated NaNs in 2D ndarray input for %d columns\", arr2.shape[1])\n                    _LOGGER.info(\"Interpolated %d NaNs in 2D ndarray input across %d columns (nan_policy='interpolate')\", n_nans, arr2.shape[1])\n                else:\n                    if nan_fill_value is None:\n                        _LOGGER.debug(\"nan_policy='fill' but nan_fill_value is None -&gt; raising ValueError\")\n                        raise ValueError(\"nan_fill_value must be provided when nan_policy=='fill'\")\n                    data = np.where(np.isnan(arr), nan_fill_value, arr)\n                    _LOGGER.debug(\"Filled NaNs in 2D ndarray input with value=%s\", nan_fill_value)\n                    n_nans = int(np.isnan(arr).sum())\n                    _LOGGER.info(\"Filled %d NaNs in 2D ndarray input with value=%s (nan_policy='fill')\", n_nans, nan_fill_value)\n\n    def apply_filter(column, cutoff_frequency):\n        # Ensure cutoff_frequency is within (0, fs/2)\n        nyquist = fs / 2.0\n        if cutoff_frequency &gt;= nyquist:\n            # Set to 95% of Nyquist if above or equal\n            cutoff_frequency = nyquist * 0.95\n        if cutoff_frequency &lt;= 0:\n            # Set to a small positive value if non-positive\n            cutoff_frequency = nyquist * 0.05\n        # Design Butterworth filter with configurable order\n        b, a = butter(N=butter_order, Wn=cutoff_frequency,\n                      btype=\"low\", fs=fs, output=\"ba\")\n\n        # Smooth very short-term noise for pandas Series inputs\n        if isinstance(column, pd.Series):\n            column = column.rolling(window=rolling_window, min_periods=1).mean()\n\n        # Work on numpy array copy for length/pad checks\n        col_arr = np.asarray(column, dtype=float)\n\n        # filtfilt requires length &gt; padlen where padlen = 3*(max(len(a), len(b)) - 1)\n        padlen = 3 * (max(len(a), len(b)) - 1)\n        if col_arr.size &lt;= padlen:\n            # Fallback to lfilter with a warning (introduces phase shift)\n            _LOGGER.warning(\n                \"Signal length %d &lt;= padlen %d for filtfilt (order=%d). Falling back to lfilter which introduces phase shift.\",\n                col_arr.size,\n                padlen,\n                butter_order,\n            )\n            return lfilter(b, a, col_arr)\n\n        # Zero-phase filtering\n        return filtfilt(b, a, col_arr)\n\n    if custom_cutoff_frequency is not None:\n        cutoff_frequency = custom_cutoff_frequency\n\n    if input_type == 'ndarray':\n        if custom_cutoff_frequency is None:\n            _LOGGER.info(\"Automatic cutoff estimation for %d channels (threshold=%s%%)\", data.shape[1], threshold)\n            cutoff_frequencies = [float(spectral_analysis(\n                data=column, sampling_freq=sampling_freq, threshold=threshold, plot=plot)) for column in data.T]\n        else:\n            cutoff_frequencies = [cutoff_frequency] * data.shape[1]\n\n        filtered_data = np.array([apply_filter(column, cf)\n                                 for column, cf in zip(data.T, cutoff_frequencies)]).T\n        try:\n            cutoffs = np.asarray(cutoff_frequencies, dtype=float)\n            _LOGGER.info(\n                \"Filtering complete for %d channels (butter_order=%d). Cutoff summary: min=%.3gHz median=%.3gHz max=%.3gHz\",\n                cutoffs.size,\n                butter_order,\n                np.nanmin(cutoffs),\n                np.nanmedian(cutoffs),\n                np.nanmax(cutoffs),\n            )\n        except Exception:\n            _LOGGER.info(\"Filtering complete (ndarray input).\")\n        return filtered_data\n\n    elif input_type == 'dataframe':\n        if custom_cutoff_frequency is None:\n            _LOGGER.info(\"Automatic cutoff estimation for %d channels (threshold=%s%%)\", len(data.columns), threshold)\n            cutoff_frequencies = {column: float(spectral_analysis(\n                data=data[column], sampling_freq=sampling_freq, threshold=threshold, plot=plot)) for column in data.columns}\n        else:\n            cutoff_frequencies = {\n                column: cutoff_frequency for column in data.columns}\n\n        filtered_data = data.apply(lambda column: apply_filter(\n            column, cutoff_frequencies[column.name]))\n        try:\n            cutoffs = np.asarray(list(cutoff_frequencies.values()), dtype=float)\n            _LOGGER.info(\n                \"Filtering complete for %d channels (butter_order=%d). Cutoff summary: min=%.3gHz median=%.3gHz max=%.3gHz\",\n                cutoffs.size,\n                butter_order,\n                np.nanmin(cutoffs),\n                np.nanmedian(cutoffs),\n                np.nanmax(cutoffs),\n            )\n        except Exception:\n            _LOGGER.info(\"Filtering complete (DataFrame input).\")\n        return filtered_data\n\n    elif input_type == 'series':\n        if custom_cutoff_frequency is None:\n            cutoff_frequency = float(spectral_analysis(\n                data=data, sampling_freq=sampling_freq, threshold=threshold, plot=plot))\n        filtered_data = apply_filter(data, cutoff_frequency)\n        _LOGGER.info(\"Filtering complete for 1 channel (butter_order=%d). Cutoff=%.3g Hz\", butter_order, cutoff_frequency)\n        return pd.Series(filtered_data, index=data.index, name=data.name)\n\n    elif input_type == 'list':\n        # Treat as 1D array\n        if custom_cutoff_frequency is None:\n            cutoff_frequency = float(spectral_analysis(\n                data=data, sampling_freq=sampling_freq, threshold=threshold, plot=plot))\n        filtered_data = apply_filter(pd.Series(data), cutoff_frequency)\n        _LOGGER.info(\"Filtering complete for 1 channel (butter_order=%d). Cutoff=%.3g Hz\", butter_order, cutoff_frequency)\n        return filtered_data.tolist()\n\n\n\nSynthetic signal demo — what to look for\nThis cell creates a noisy sine wave and demonstrates the effect of different low-pass cutoffs.\nLearning points: - How lowpass filtering progressively removes higher frequency content as the cutoff decreases. - What over‑smoothing looks like (important features removed) vs under‑filtering (noise remains). - The importance of choosing a cutoff relative to the signal’s content (here a 20 Hz sine wave).\n\n# Define the original signal parameters\nsampling_rate = 1000  # Hz\nduration = 0.5  # seconds\nfrequency = 20  # Hz\namplitude = 1.0  # Amplitude of the sine wave\nnoise_level = 1.0  # Amplitude of the noise\n\n# Generate time vector\ntime = np.linspace(0, duration, int(sampling_rate * duration), endpoint=False)\n\n# Create sine wave\nsine_wave = amplitude * np.sin(2 * np.pi * frequency * time)\n\n# Generate noise\nnoise = noise_level*np.random.uniform(min(sine_wave), max(sine_wave), len(time))\n\n# Add noise to sine wave\nnoisy_signal = sine_wave + noise\n# Define cutoff frequencies to demonstrate aliasing\ncutoff_frequencies = [400, 200, 40, 10]  # Hz, progressively lower\n\nplt.figure(figsize=(12, 10))\n\ndef lowpass_filter(data, cutoff_freq, sampling_rate, order=4):\n    \"\"\"Applies a low-pass Butterworth filter to the input data.\"\"\"\n    nyquist_freq = 0.5 * sampling_rate\n    normalized_cutoff = cutoff_freq / nyquist_freq\n    b, a = butter(N=4, Wn=cutoff_freq,\n                      btype=\"low\", fs=sampling_rate, output=\"ba\")\n    return filtfilt(b, a, data)\n\nfor i, fc in enumerate(cutoff_frequencies, 1):\n    filtered_signal = lowpass_filter(noisy_signal, fc, sampling_rate)\n    # Plot results\n    plt.subplot(len(cutoff_frequencies), 1, i)\n    plt.plot(time, sine_wave, color='tab:blue', linestyle='-', alpha=0.8, label=\"Original Sine Wave\")\n    plt.plot(time, noisy_signal, color='k', linestyle='-', alpha=0.5, label=\"Noisy Signal\")\n    plt.plot(time, filtered_signal, color='tab:orange', linestyle='-', alpha=1.0, label=f\"Filtered at {fc}Hz\",)\n\n    # Position legend to the right\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Amplitude\")\n    plt.title(f\"Filtered with {fc}Hz cutoff frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nInteractive slider — how to use and what to observe\nThis widget allows you to pick a manual cutoff frequency and immediately see its impact on the filtered voltage signals.\nLearning points: - Observe how the traces become smoother as cutoff decreases and how details are lost when the cutoff is set too low. - Note the update time: filtering multiple channels may take a few seconds after moving the slider. - Use this tool to develop an intuition for a reasonable range of cutoff frequencies before using data-driven spectral methods.\n\n# Use the slider to see how using different cutoff frequencies effects the signal\ncutoff_freq = ipywidgets.widgets.FloatSlider(value=25, min=0.1, max=49.9, step=0.1, description='Cutoff Frequency (Hz):')\n\ndef filter_and_plot(cutoff_freq):\n  filtered_data_EO = filter_timeseries_data(df_EO, 100, custom_cutoff_frequency=cutoff_freq, plot=False)\n  filtered_data_EC = filter_timeseries_data(df_EC, 100, custom_cutoff_frequency=cutoff_freq, plot=False)\n\n  plot_voltage_data(filtered_data_EO, filtered_data_EC, time_range=(5, 15), figsize=(10, 8), dpi=100, pad=2.0)\n\nout = ipywidgets.interactive_output(filter_and_plot, {'cutoff_freq': cutoff_freq})\nprint(\"Note: it may take a minute for the plot to update after moving the slider\")\ndisplay(cutoff_freq, out)\n\nNote: it may take a minute for the plot to update after moving the slider\n\n\n\n\n\n\n\n\n\n\nChoosing a cumulative power threshold\nTo derive a data‑driven low‑pass cutoff, we find the frequency where the PSD cumulative power reaches a chosen percentage of total power (typical: 95–99%). This balances keeping sway-related energy and rejecting high‑frequency noise.\nRules of thumb - 95% is a practical starting point for many signals; 99% is more conservative (keeps more high‑frequency content). - Avoid extreme values: too low (e.g., &lt;90%) can remove meaningful data; attempting to use 100% can include noise and numerical edge cases.\nDiagnostics - Always inspect the PSD and the cumulative power plot together. The chosen cutoff should fall after the dominant low‑frequency peaks but before the broadband noise floor grows. - If the cumulative curve has no clear knee, compare several representative channels and pick a conservative compromise or use manual inspection.\nPractical steps 1. Try 95% and visually inspect filtered traces versus raw traces. 2. If excessive smoothing occurs, raise toward 97–99% and re-evaluate.\nInteractive use - The threshold slider in the notebook lets you explore how thresholds change the estimated cutoff and filtered results; use the PSD diagnostics (plot=True) to justify your selection.\nTakeaway: pick a threshold grounded in both the quantitative cumulative-power metric and the visual appearance of filtered traces — document the exact threshold in your analysis.\n\ndef filter_and_plot(threshold):\n    # Filter the EO and EC data using the selected cutoff frequency\n    df_EC_filtered = filter_timeseries_data(df_EC[['VFz']], 100, custom_cutoff_frequency=None, threshold=threshold, plot=True, nan_policy='interpolate')\n\n    # Create two separate figures for EO and EC data\n    figsize = (12, 6)\n    time_range = (15, 30)\n    time = np.linspace(0, len(df_EC_filtered)/100, len(df_EC_filtered))\n    pad = 2.0\n\n    fig_EC, axs_EC = plt.subplots(figsize=figsize)\n\n    # Plot EO data\n    col, title = ('VFz','Fz voltage')\n    axs_EC.plot(time, df_EC[['VFz']], color='tab:blue', alpha=0.8, label='EO - Original')\n    axs_EC.plot(time, df_EC_filtered, color='tab:orange', label='EO - Filtered')\n    axs_EC.legend(bbox_to_anchor=(1.1, 1), loc='upper right')\n    axs_EC.set_title(title)\n    axs_EC.set_xlim(time_range)\n\n    # Set x-axis labels and limits\n    axs_EC.set_xlabel('Time [s]')\n    # Set y-axis label for the middle subplot in both figures\n    axs_EC.set_ylabel('Voltage [V]')\n    fig_EC.align_ylabels()\n    # Adjust layout for both figures\n    plt.tight_layout(pad=pad)\n    # Adjust the layouts to avoid overlap with legends and titles\n    plt.subplots_adjust(top=0.9)\n    fig_EC.subplots_adjust(top=0.9)\n    fig_EC.suptitle('Eyes Open Data')\n    # Show plot\n    plt.show()\n\n# Create a slider for cutoff frequency\nthreshold_slider = widgets.FloatSlider(value=50, min=1, max=99.9, step=0.1, description='Power:')\n\n# Create an interactive widget\ninteractive_plot = widgets.interactive_output(filter_and_plot, {'threshold': threshold_slider})\n\n# Display the slider and the plot\nprint(\"Note: it may take a minute for the plot to update after moving the slider\")\ndisplay(threshold_slider, interactive_plot), \n\nNote: it may take a minute for the plot to update after moving the slider\n\n\n\n\n\n\n\n\n(None,)"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#convert-measured-voltages-to-forces-and-moments",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#convert-measured-voltages-to-forces-and-moments",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Convert measured voltages to forces and moments",
    "text": "Convert measured voltages to forces and moments\nThis section shows how to convert the six measured channel voltages from the force plate into physical forces and moments. We provide the inverted sensitivity matrix (B), the amplifier gain (G), and the input voltage (V_0). The conversion is applied row-wise and vectorized in the code cells that follow.\nWhat this section does - Converts raw channel voltages \\((V_{Fx}, V_{Fy}, V_{Fz}, V_{Mx}, V_{My}, V_{Mz})\\) → forces/moments \\((F_x, F_y, F_z, M_x, M_y, M_z)\\) in SI units. - Stores results as new columns in the existing dataframes for use in plotting and analysis.\nFormula (applied per time-point):\n\\[\\vec{Y} = \\frac{10^6}{G V_0} \\vec{V} B^T\\]\nWhere: - (\\(\\vec{V}\\)) is the 1×6 voltage row vector at a time point. - (\\(B\\)) is the inverted sensitivity matrix (6×6) shown below. - The factor \\(10^6\\) adjusts from volts/millivolts depending on your amplifier/device calibration (kept here to match the provided calibration constants).\nVerification checks (quick ideas) - Compare the mean of the vertical force (\\(F_z\\)) to the subject’s body weight (mass × 9.81 m/s²) — they should be close on average. - Plot raw voltages vs computed forces for one channel to confirm linear scaling. - Check for unreasonable outliers in computed forces (e.g., extremely large or NaN values) and investigate missing/erroneous voltage samples.\nUnits and scaling notes - Make sure the amplifier gain (\\(G\\)) and the input voltage (\\(V_0\\)) reflect your acquisition hardware. If your device uses millivolt outputs with a different amplifier gain, adjust the conversion factor accordingly. - Document the units (N, N·m) and any scaling factors in your analysis record so others can reproduce your results.\nTips - Keep the original voltage columns until you are comfortable with the conversion. The notebook already assigns new columns Fx, Fy, Fz, Mx, My, Mz to df_EO and df_EC. - If units or calibration constants differ for your dataset, update \\(G\\), \\(V_0\\), or \\(B\\) accordingly and re-run the conversion cell.\nWhat’s next - After conversion, visualize the force and moment time-series and inspect their PSDs to plan appropriate cutoffs for filtering.\n\n# Inverted sensitivity matrix, B\nB = np.array([\n    [2.9007,  0.0200, -0.0009, -0.0253, -0.0085,  0.0090],\n    [-0.0067, 2.9024, -0.0520, -0.0366, -0.0149, -0.0341],\n    [0.0046, -0.0229, 11.4206, -0.0055,  0.0055,  0.0026],\n    [-0.0019,  0.0035, -0.0067,  1.4559, -0.0053, -0.0028],\n    [0.0036,  0.0011, -0.0067,  0.0018,  1.1475, -0.0008],\n    [0.0037,  0.0145, -0.0032,  0.0006,  0.0076,  0.6188]\n])\n\nG = 4000 # Gain\nV_0 = 10 # Input voltage\nCF = 1e6/(G*V_0) # Conversion factor\n\n\n# Plotting function\ndef plot_data(df_EO, df_EC, df_cols, plot_titles, y_labels, time_range=(15, 30), figsize=(10, 8), dpi=200, pad=2.0):\n    \"\"\"\n    Plot signals from two dataframes (EO and EC) over a specified time range.\n\n    This function generates six subplots corresponding to force (Fx, Fy, Fz) and moment (Mx, My, Mz) signals,\n    comparing data from two different conditions (EO and EC).\n\n    Args:\n        df_EO (DataFrame): Data containing EO (Eyes Open) condition.\n        df_EC (DataFrame): Data containing EC (Eyes Closed) condition, with the same columns as df_EO.\n        df_cols (list): List of column names to plot from the dataframes.\n        plot_titles (list): List of titles for each subplot.\n        y_labels (list): List of y-axis labels for each subplot.\n        time_range (tuple, optional): Time range (start, end) in seconds for plotting. Defaults to (15, 30).\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n        dpi (int, optional): Dots per inch (DPI) setting for the figure resolution. Defaults to 200.\n        pad (float, optional): Padding for tight_layout to adjust subplot spacing. Defaults to 2.0.\n\n    Raises:\n        ValueError: If the input dataframes do not contain the required columns.\n\n    Example:\n        &gt;&gt;&gt; plot_data(df_EO, df_EC, ['Fx', 'Fy', 'Fz', 'Mx', 'My', 'Mz'], titles, labels, time_range=(10, 25))\n    \"\"\"\n    # Create subplots\n    fig, axs = plt.subplots(6, 1, sharex=True, sharey=False, dpi=dpi, figsize=figsize)\n\n    # Plot data for each subplot\n    for i, (col, title, y_label) in enumerate(zip(df_cols,\n                                         plot_titles,y_labels)):\n        axs[i].plot(df_EO['Time'], df_EO[col], color='tab:blue', alpha=1.0, label='EO')\n        axs[i].plot(df_EC['Time'], df_EC[col], color='tab:orange', alpha=1.0, label='EC')\n        axs[i].legend(bbox_to_anchor=(1.1, 1), loc='upper right')\n        axs[i].set_ylabel(y_label)\n        axs[i].set_title(title)\n\n    # Set x-axis label and limits\n    plt.xlabel('Time [s]')\n    plt.xlim(time_range)\n\n    # Adjust layout\n    plt.tight_layout(pad=pad)\n\n    # Show the plot\n    plt.show()\n\n\n# Get voltages as array-like\nV_EO = df_EO[['VFx', 'VFy', 'VFz', 'VMx', 'VMy', 'VMz']].to_numpy()\n# Compute forces and moments in a vectorized manner\nY_EO = CF*(V_EO@B.T)\n# Assign computed values back to DataFrame\ndf_EO[['Fx', 'Fy', 'Fz', 'Mx', 'My', 'Mz']] = Y_EO\n# Get voltages as array-like\nV_EC = df_EC[['VFx', 'VFy', 'VFz', 'VMx', 'VMy', 'VMz']].to_numpy()\n# Compute forces and moments in a vectorized manner\nY_EC = CF*(V_EC@B.T)\n# Assign computed values back to DataFrame\ndf_EC[['Fx', 'Fy', 'Fz', 'Mx', 'My', 'Mz']] = Y_EC\n\nY_cols = ['Fx','Fy','Fz','Mx','My','Mz']\nplot_titles = ['Fx','Fy','Fz','Mx','My','Mz']\ny_labels = ['$F_x$ [N]','$F_y$ [N]','$F_z$ [N]','$M_x$ [N-m]','$M_y$ [N-m]','$M_z$ [N-m]']\n\nplot_data(df_EO=df_EO,df_EC=df_EC,df_cols=Y_cols,plot_titles=plot_titles,y_labels=y_labels,time_range=(0,15),dpi=100,figsize=(10,8))"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#filtering-and-selecting-cutoff-frequencies-practical-guide",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#filtering-and-selecting-cutoff-frequencies-practical-guide",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Filtering and selecting cutoff frequencies — practical guide",
    "text": "Filtering and selecting cutoff frequencies — practical guide\nThis section demonstrates how different low-pass cutoff choices affect moment signals and how to apply a zero-phase Butterworth filter to all force/moment channels.\nWhat you’ll see - Example plots that compare a too-low cutoff (over‑smoothed), a too-high cutoff (insufficient noise removal), and a data-driven cutoff determined via cumulative power from the PSD. - Code that uses filter_timeseries_data(...) to compute per-channel cutoffs (or accept a single custom cutoff) and perform zero‑phase filtering with filtfilt when possible.\nHow to choose cutoffs - Too low: removes physiologically relevant sway (underestimates variability). - Too high: keeps noise that inflates spectral/variance metrics. - Data-driven: compute cutoff from a chosen cumulative power threshold (e.g., 95–99%). Use spectral_analysis(..., threshold=...) to inspect PSD/cumulative plots and select a threshold.\nRecommended filtering workflow (concise) 1. Compute PSD and cumulative power for a representative segment of each channel. 2. Pick a threshold (start at 95%), compute cutoff per channel, and inspect a few filtered traces. 3. If multiple channels consistently suggest similar cutoffs, you may choose to use a common cutoff for simplicity; otherwise preserve per-channel cutoffs. 4. Apply filtering to the full dataset and save the cutoffs used alongside filtered outputs.\nCommon practical options - Notch filtering: if mains interference (50/60 Hz) is present and strong, consider a narrow notch filter before low-pass filtering. - Detrending: remove linear drift before PSD computation if low-frequency trends dominate the spectrum. - Short recordings: filtfilt padding rules may not be satisfied; the function falls back to lfilter — document this and avoid phase-sensitive measures in that case.\nChecking results - Plot raw vs filtered traces and quantize differences (RMS reduction) to ensure you’re not removing expected signal content. - Compare sway metrics (range, speed) before and after filtering to understand the filter’s impact.\nTip - Save per-channel cutoff frequencies (and the threshold used to compute them) in a small JSON or CSV so your analysis is reproducible.\n\n# Create shortened df to only include signals between 15 and 30 seconds...\nf_c1_EC = 0.5 # (Hz) Too low cutoff frequency\nf_c2_EC = 450 # (Hz) Too high cutoff frequency\n\nfs = 1000 # ENSURE THAT THIS MATCHES THE DATA SAMPLING RATE!\nf_c3_EC = spectral_analysis(df_EC['My'],sampling_freq=fs, threshold=95)\n\nprint(type(pd.DataFrame(df_EC['My'])))\n\n# Filter data using too low of a cutoff frequency\ndf_c1 = filter_timeseries_data(pd.DataFrame(df_EC['My']),sampling_freq=fs,custom_cutoff_frequency=f_c1_EC)\n# Filter data using too high of a cutoff frequency\ndf_c2 = filter_timeseries_data(pd.DataFrame(df_EC['My']),sampling_freq=fs,custom_cutoff_frequency=f_c2_EC)\n# Filter data using an appropriate cutoff frequency\ndf_c3 = filter_timeseries_data(pd.DataFrame(df_EC['My']),sampling_freq=fs,custom_cutoff_frequency=f_c3_EC)\n\n# Plot the data showing the effects of using the different cutoff frequencies\nfig, axs = plt.subplots(3, 1, sharex=True, sharey=False, dpi=100, figsize=(10, 8))\n\ntime = df_EC['Time']\n\n# Subplot 1: too low cutoff\naxs[0].plot(time, df_EC['My'], color='gray', linestyle='-', alpha=0.8, label='Original $M_y$ (raw)')\naxs[0].plot(time, df_c1['My'], color='tab:blue', alpha=1.0, label=f'Filtered (fc={f_c1_EC:.2f} Hz)')\naxs[0].set_title(\"Cutoff Freq Too Low\")\naxs[0].legend(loc='upper right')\n\n# Subplot 2: too high cutoff\naxs[1].plot(time, df_EC['My'], color='gray', linestyle='-', alpha=0.8, label='Original $M_y$ (raw)')\naxs[1].plot(time, df_c2['My'], color='tab:green', alpha=1.0, label=f'Filtered (fc={f_c2_EC:.2f} Hz)')\naxs[1].set_title(\"Cutoff Freq Too High\")\naxs[1].legend(loc='upper right')\n\n# Subplot 3: appropriate cutoff (data-driven)\naxs[2].plot(time, df_EC['My'], color='gray', linestyle='-', alpha=0.8, label='Original $M_y$ (raw)')\naxs[2].plot(time, df_c3['My'], color='tab:orange', alpha=1.0, label=f'Filtered (fc={f_c3_EC:.2f} Hz)')\naxs[2].set_title(\"Appropriate Cutoff Frequency (spectral_analysis)\")\naxs[2].legend(loc='upper right')\n\naxs[-1].set_xlabel('Time [s]')\naxs[1].set_ylabel('Eyes Closed $M_y$ [N-m]')\n\n# Zoom to the 15-30 s window as intended (if available)\ntry:\n    axs[0].set_xlim(5, 15)\nexcept Exception:\n    pass\n\nplt.tight_layout(pad=2.0)\nplt.show()\n\ndf_EO_filtered = filter_timeseries_data(df_EO, sampling_freq=fs)\ndf_EC_filtered = filter_timeseries_data(df_EC, sampling_freq=fs)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#center-of-pressure-cop-calculation-and-stabilogram-plotting",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#center-of-pressure-cop-calculation-and-stabilogram-plotting",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Center of Pressure (CoP) — calculation and stabilogram plotting",
    "text": "Center of Pressure (CoP) — calculation and stabilogram plotting\nThis section explains what the Center of Pressure (CoP) is, why it matters for balance analysis, how to compute it from filtered force/moment signals, and how to visualize stabilograms for EO and EC conditions.\nWhat is the CoP? - The CoP is the point location on the force‑plate surface where the resultant ground reaction force acts. It is computed from vertical force and plate moments and represents the net pressure centroid under the foot/feet. - Physically, CoP tracks how the neuromuscular system shifts load to maintain balance; it is a directly measured, high‑temporal‑resolution proxy for postural control dynamics. - CoP is not the same as the body Center of Mass (CoM). CoM represents mass distribution of the body and typically requires motion capture and modeling; CoP is measured directly by the force plate.\nWhy CoP is useful - Sensitive to corrective actions: fast corrective sways and stabilizing adjustments produce characteristic CoP excursions and velocities. - Trial‑level and summary metrics derived from CoP (range, speed, excursion) are widely used to quantify postural stability and to compare conditions or groups (e.g., EO vs EC). - Easy to compute and reproducible across labs when calibration and filtering are documented.\nWhat is a stabilogram? - A stabilogram is a 2D time‑series plot of CoP coordinates (ML vs AP) that shows the trajectory of CoP over the trial. It is the primary visualization for postural sway. - Interpret visually: - Large cloud/trajectory = more sway / less stability. - Long, smooth excursions suggest slow drift; dense, jittery paths indicate higher-frequency corrective actions or noise. - Complement stabilograms with scalar summaries (range, mean speed, RMS) and spectral analysis to separate slow and fast behavior.\nComputation (per time point):\n\nGiven the measured vector \\(\\vec{Y} = (F_x, F_y, F_z, M_x, M_y, M_z)\\) and the plate shear‑center offset \\(z_0\\) (in meters), the CoP coordinates are computed as the displayed equations:\n\n\\[\n\\begin{aligned}\n\\vec{Y} &= \\big(F_x,\\, F_y,\\, F_z,\\, M_x,\\, M_y,\\, M_z\\big) \\\\\nx_{\\mathrm{CoP}} &= \\frac{M_x + z_0\\,F_y}{F_z} \\\\[6pt]\ny_{\\mathrm{CoP}} &= \\frac{z_0\\,F_x - M_y}{F_z}\n\\end{aligned}\n\\]\n\nImportant notes on signs and conventions: confirm that your plate calibration and sign convention match these formulas (Mx positive → generates positive x_CoP, etc.). Small sign flips in moments/forces will invert CoP axes.\n\nUnits and centering - Keep consistent SI units: forces in N, moments in N·m, z0 in m. Convert CoP from meters to mm for plotting (multiply by 1e3) if desired for readability. - Center stabilograms by subtracting the temporal mean from each CoP axis so the trajectory is centered at (0,0). This removes constant offsets due to subject placement.\nPractical tips and pitfalls - Handle small/zero Fz: mask, drop, or interpolate samples where Fz is near zero (division by very small values yields spurious large CoP). Consider thresholding Fz (e.g., ignore samples where Fz &lt; some small fraction of median Fz). - Filtering: compute CoP from filtered forces/moments (zero‑phase filtering preferred) to avoid adding high‑frequency noise in the stabilogram. Document filter design (type, order, cutoff). - Edge effects: filtfilt requires adequate padding—short recordings may fall back to lfilter (phase shift). Avoid phase‑sensitive measures if filtfilt fallback occurs. - Outlier checks: clip or remove sudden CoP spikes due to artifacts (e.g., transient force clipping).\nPlotting guidance - Use equal axis scales (square aspect ratio) and identical axis limits for EO and EC to allow direct visual comparison. - Add dashed lines at zero to show anatomical midlines; annotate mean CoP and origin. - For long traces, consider color‑coding by time, plotting a density/contour, or downsampling for clarity. - Overlay raw vs filtered CoP (or show both on separate panels) to confirm filtering did not remove relevant low‑frequency features.\nInterpretation (tandem stance example) - Tandem stance typically increases AP sway (narrow fore‑aft base) and may increase average CoP speed; removing vision (EC) commonly further increases excursion and speed. - Use paired comparisons (EO vs EC) and percent change together with stabilograms to report effects.\nNext steps - Compute scalar metrics from the centered CoP (AP/ML excursion ranges, mean CoP speed, mean excursion distance, std dev) and report the filter/cutoff and any sample masking thresholds used for reproducibility. - Important notes on signs and conventions: confirm that your plate calibration and sign convention match these formulas (Mx positive → generates positive x_CoP, etc.). Small sign flips in moments/forces will invert CoP axes.\nUnits and centering - Keep consistent SI units: forces in N, moments in N·m, z0 in m. Convert CoP from meters to mm for plotting (multiply by 1e3) if desired for readability. - Center stabilograms by subtracting the temporal mean from each CoP axis so the trajectory is centered at (0,0). This removes constant offsets due to subject placement.\nPractical tips and pitfalls - Handle small/zero Fz: mask, drop, or interpolate samples where Fz is near zero (division by very small values yields spurious large CoP). Consider thresholding Fz (e.g., ignore samples where Fz &lt; some small fraction of median Fz). - Filtering: compute CoP from filtered forces/moments (zero‑phase filtering preferred) to avoid adding high‑frequency noise in the stabilogram. Document filter design (type, order, cutoff). - Edge effects: filtfilt requires adequate padding—short recordings may fall back to lfilter (phase shift). Avoid phase‑sensitive measures if filtfilt fallback occurs. - Outlier checks: clip or remove sudden CoP spikes due to artifacts (e.g., transient force clipping).\nPlotting guidance - Use equal axis scales (square aspect ratio) and identical axis limits for EO and EC to allow direct visual comparison. - Add dashed lines at zero to show anatomical midlines; annotate mean CoP and origin. - For long traces, consider color‑coding by time, plotting a density/contour, or downsampling for clarity. - Overlay raw vs filtered CoP (or show both on separate panels) to confirm filtering did not remove relevant low‑frequency features.\nInterpretation (tandem stance example) - Tandem stance typically increases AP sway (narrow fore‑aft base) and may increase average CoP speed; removing vision (EC) commonly further increases excursion and speed. - Use paired comparisons (EO vs EC) and percent change together with stabilograms to report effects.\nNext steps - Compute scalar metrics from the centered CoP (AP/ML excursion ranges, mean CoP speed, mean excursion distance, std dev) and report the filter/cutoff and any sample masking thresholds used for reproducibility.\n\nz_0 = -37.645*1e-3 # (m) Shear center relative to geometric center\n\nCOPx_EO = (df_EO_filtered['Mx']+z_0*df_EO_filtered['Fy'])/df_EO_filtered['Fz']\nCOPy_EO = (df_EO_filtered['My']-z_0*df_EO_filtered['Fx'])/df_EO_filtered['Fz']\n\nCOPx_EC = (df_EC_filtered['Mx']+z_0*df_EC_filtered['Fy'])/df_EC_filtered['Fz']\nCOPy_EC = (df_EC_filtered['My']-z_0*df_EC_filtered['Fx'])/df_EC_filtered['Fz']\n\nCOPx_EO_centered = (COPx_EO - COPx_EO.mean())*1e3 # Convert to mm\nCOPy_EO_centered = (COPy_EO - COPy_EO.mean())*1e3 # Convert to mm\n\nCOPx_EC_centered = (COPx_EC - COPx_EC.mean())*1e3 # Convert to mm\nCOPy_EC_centered = (COPy_EC - COPy_EC.mean())*1e3 # Convert to mm\n\nfig, axs = plt.subplots(1, 2, dpi=100, figsize=(12,6))\n\n# Determine the maximum range for centering around (0,0)\nx_max = np.ceil(1.25*max(abs(COPx_EO_centered.min()), abs(COPx_EO_centered.max()),\n            abs(COPx_EC_centered.min()), abs(COPx_EC_centered.max())))\ny_max = np.ceil(1.25*max(abs(COPy_EO_centered.min()), abs(COPy_EO_centered.max()),\n            abs(COPy_EC_centered.min()), abs(COPy_EC_centered.max())))\n\nmax_range = max(x_max, y_max)  # Ensure square limits\n\n# First subplot (EO)\naxs[0].plot(COPx_EO_centered, COPy_EO_centered, color='tab:blue', label='EO')\naxs[0].set_xlim(-max_range, max_range)\naxs[0].set_ylim(-max_range, max_range)\naxs[0].set_xlabel('Medial/lateral CoP position [mm]')\naxs[0].set_ylabel('Anterior/posterior CoP position [mm]')\naxs[0].set_title('Eyes Open Condition')\naxs[0].axhline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[0].axvline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[0].set_aspect('equal')  # Ensures square aspect ratio\n\n# Second subplot (EC)\naxs[1].plot(COPx_EC_centered, COPy_EC_centered, color='tab:orange', label='EC')\naxs[1].set_xlim(-max_range, max_range)\naxs[1].set_ylim(-max_range, max_range)\naxs[1].set_xlabel('Medial/lateral CoP position [mm]')\naxs[1].set_ylabel('Anterior/posterior CoP position [mm]')\naxs[1].set_title('Eyes Closed Condition')\naxs[1].axhline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[1].axvline(0, color='gray', linestyle='--', linewidth=0.8)\naxs[1].set_aspect('equal')  # Ensures square aspect ratio\n\n# Set overall figure title with large font\nfig.suptitle('Stabilogram Comparison', fontsize=18, fontweight='bold')\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title\nplt.show()"
  },
  {
    "objectID": "notebooks/BalanceSignalProcessingTutorial.html#balance-metrics-definitions-computation-and-interpretation",
    "href": "notebooks/BalanceSignalProcessingTutorial.html#balance-metrics-definitions-computation-and-interpretation",
    "title": "Signal Processing for Biomechanics: Force-Plate Balance Analysis Tutorial",
    "section": "Balance metrics — definitions, computation, and interpretation",
    "text": "Balance metrics — definitions, computation, and interpretation\nThis section computes commonly used scalar summary metrics from CoP time series and explains how to interpret them in the context of balance tasks.\nMetrics computed in the notebook - AP excursion range: max(CoP_x) - min(CoP_x) (mm) - ML excursion range: max(CoP_y) - min(CoP_y) (mm) - Average CoP speed: mean( sqrt( (dx/dt)^2 + (dy/dt)^2 ) ) (mm/s) - Mean excursion distance: mean( sqrt(x^2 + y^2) ) (mm) - Std dev of excursion distance: std( sqrt(x^2 + y^2) ) (mm)\nImplementation notes - Use centered CoP (subtract temporal mean) so offsets do not inflate range measures. - Compute derivatives using successive differences divided by dt (ensure consistent units: mm vs m). - For average speed, ignore NaN or masked samples (e.g., where Fz was invalid). Consider thresholding small Fz values before computing CoP. - Document sampling rate, filter design (type, order, cutoff) and any NaN handling — metrics are sensitive to filtering and preprocessing.\nHow to interpret each metric (practical guide) - AP / ML excursion ranges - What they measure: trial-wide span of CoP in anterior–posterior and medial–lateral axes. - Larger values → greater gross excursion (more sway or larger shifts of load). Directional changes (AP vs ML) can indicate task-specific instability (e.g., tandem stance increases AP). - Caveats: ranges are sensitive to brief large transients (artifacts). Check stabilograms to confirm excursions reflect behaviour, not noise. - Average CoP speed - What it measures: time-averaged instantaneous CoP velocity; captures how actively the subject is correcting posture. - Larger speed → more frequent/rapid corrective actions or noise. Often more sensitive to subtle balance changes than range. - Caveats: speed increases with trial length variability and with higher-frequency noise; ensure consistent filtering across conditions. - Mean excursion distance - What it measures: average radial distance from center (magnitude of sway). - Larger mean distance → CoP is on average farther from center; useful summary of overall displacement. - Complement with range and speed to distinguish broad drift vs frequent small corrections. - Std dev of excursion distance - What it measures: variability of radial CoP magnitude; indicates consistency vs intermittency of sway. - Larger std → more variable control (bursty corrections or inconsistent posture).\nPractical interpretation rules of thumb - Compare within-subject, within-task (paired comparisons) whenever possible; absolute values depend on posture, footwear, surface, and trial duration. - Use percent change and paired statistics (paired t-test, Wilcoxon) for EO vs EC or pre/post designs — report effect size and confidence intervals, not only p-values. - Visual checks: always inspect stabilograms (trajectory) and raw vs filtered traces to ensure metrics reflect behavior. - Filtering matters: a more aggressive low-pass reduces speed and high-frequency variance while leaving gross ranges less affected. Report filter parameters.\nHandling artifacts and edge cases - Outliers: remove or clip transient spikes before computing range/speed. - Low/zero Fz: mask samples where Fz is near zero (division instability) or interpolate short gaps. - Short trials: filtfilt padding may fail and introduce phase shift; avoid phase-sensitive metrics if lfilter fallback occurs.\n\ndt = 1/fs # Time difference between adjacent time points\n\n# AP (Anterior-Posterior) Excursion Range\nAP_EO = COPx_EO_centered.max() - COPx_EO_centered.min()\nAP_EC = COPx_EC_centered.max() - COPx_EC_centered.min()\n\n# ML (Medio-Lateral) Excursion Range\nML_EO = COPy_EO_centered.max() - COPy_EO_centered.min()\nML_EC = COPy_EC_centered.max() - COPy_EC_centered.min()\n\n# Average CoP Speed\nAvg_CoP_Speed_EO = (np.sqrt(COPx_EO_centered.diff()**2 + COPy_EO_centered.diff()**2)/dt).mean()\nAvg_CoP_Speed_EC = (np.sqrt(COPx_EC_centered.diff()**2 + COPy_EC_centered.diff()**2)/dt).mean()\n\n# Mean Excursion Distance\nMean_Excursion_Distance_EO = np.sqrt(COPx_EO_centered**2 + COPy_EO_centered**2).mean()\nMean_Excursion_Distance_EC = np.sqrt(COPx_EC_centered**2 + COPy_EC_centered**2).mean()\n\n# Standard Deviation of Excursion Distance\nStDev_Excursion_Distance_EO = np.sqrt(COPx_EO_centered**2 + COPy_EO_centered**2).std()\nStDev_Excursion_Distance_EC = np.sqrt(COPx_EC_centered**2 + COPy_EC_centered**2).std()\n\n# Print the calculated metrics\nprint(\"Eyes Open Condition:\")\nprint(\"AP Excursion Range: {:.2f} mm\".format(AP_EO))\nprint(\"ML Excursion Range: {:.2f} mm\".format(ML_EO))\nprint(\"Average CoP Speed: {:.2f} mm/s\".format(Avg_CoP_Speed_EO))\nprint(\"Mean Excursion Distance: {:.2f} mm\".format(Mean_Excursion_Distance_EO))\nprint(\"Standard Deviation of Excursion Distance: {:.2f} mm\".format(StDev_Excursion_Distance_EO))\n\nprint(\"\\nEyes Closed Condition:\")\nprint(\"AP Excursion Range: {:.2f} mm\".format(AP_EC))\nprint(\"ML Excursion Range: {:.2f} mm\".format(ML_EC))\nprint(\"Average CoP Speed: {:.2f} mm/s\".format(Avg_CoP_Speed_EC))\nprint(\"Mean Excursion Distance: {:.2f} mm\".format(Mean_Excursion_Distance_EC))\nprint(\"Standard Deviation of Excursion Distance: {:.2f} mm\".format(StDev_Excursion_Distance_EC))\n\n# Calculate Percent change using (EC - EO) / EO * 100\nAP_percent_change = (AP_EC - AP_EO) / AP_EO * 100\nML_percent_change = (ML_EC - ML_EO) / ML_EO * 100\nAvg_CoP_Speed_percent_change = (Avg_CoP_Speed_EC - Avg_CoP_Speed_EO) / Avg_CoP_Speed_EO * 100\nMean_Excursion_Distance_percent_change = (Mean_Excursion_Distance_EC - Mean_Excursion_Distance_EO) / Mean_Excursion_Distance_EO * 100\nStDev_Excursion_Distance_percent_change = (StDev_Excursion_Distance_EC - StDev_Excursion_Distance_EO) / StDev_Excursion_Distance_EO * 100\n\n# Print the percent differences\nprint(\"\\nPercent change from Eyes Open to Eyes Closed conditions:\")\n\nprint(\"AP Excursion Range: {:+.2f} %\".format(AP_percent_change))\nprint(\"ML Excursion Range: {:+.2f} %\".format(ML_percent_change))\nprint(\"Average CoP Speed: {:+.2f} %\".format(Avg_CoP_Speed_percent_change))\nprint(\"Mean Excursion Distance: {:+.2f} %\".format(Mean_Excursion_Distance_percent_change))\nprint(\"Standard Deviation of Excursion Distance: {:+.2f} %\".format(StDev_Excursion_Distance_percent_change))\n\nEyes Open Condition:\nAP Excursion Range: 25.84 mm\nML Excursion Range: 25.82 mm\nAverage CoP Speed: 29.84 mm/s\nMean Excursion Distance: 6.92 mm\nStandard Deviation of Excursion Distance: 3.72 mm\n\nEyes Closed Condition:\nAP Excursion Range: 26.63 mm\nML Excursion Range: 32.95 mm\nAverage CoP Speed: 53.20 mm/s\nMean Excursion Distance: 7.36 mm\nStandard Deviation of Excursion Distance: 3.70 mm\n\nPercent change from Eyes Open to Eyes Closed conditions:\nAP Excursion Range: +3.08 %\nML Excursion Range: +27.60 %\nAverage CoP Speed: +78.31 %\nMean Excursion Distance: +6.33 %\nStandard Deviation of Excursion Distance: -0.64 %"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a 4th year PhD student in Mechanical Engineering at the University of Illinois Urbana-Champaign, where I conduct research in the Tissue Biomechanics Laboratory. My work has two complementary focuses: developing accessible measurement tools and applying them to evaluate exercise interventions for manual wheelchair users."
  },
  {
    "objectID": "about.html#research-focus",
    "href": "about.html#research-focus",
    "title": "About",
    "section": "Research Focus",
    "text": "Research Focus\nI develop and apply accessible technologies to solve real-world problems in wheelchair mobility:\n\nEvaluating adapted exercise modes like handcycling and suspended-wheel wheelchair attachments\nQuantifying musculoskeletal safety during exercise to balance cardiovascular benefits with injury risk\nDeveloping monitoring tools that enable personalized exercise prescription and long-term safety tracking"
  },
  {
    "objectID": "about.html#what-i-develop",
    "href": "about.html#what-i-develop",
    "title": "About",
    "section": "What I Develop",
    "text": "What I Develop\nI build open-source, low-cost systems to support this research:\n\nMachine learning algorithms that predict external loading during physical activity\nFlexible markerless motion capture pipelines with multi-camera calibration, pose estimation, and 3D triangulation"
  },
  {
    "objectID": "about.html#what-i-use",
    "href": "about.html#what-i-use",
    "title": "About",
    "section": "What I Use",
    "text": "What I Use\nI leverage existing tools and technologies to conduct my research:\n\nMachine Learning: PyTorch, TensorFlow, scikit-learn for model development\nComputer Vision: OpenCV, DeepLabCut for pose estimation, camera calibration, and 3-D reconstruction\nBiomechanics Analysis: OpenSim for musculoskeletal modeling and simulation\nWearable Sensors: Smartwatches and IMUs for field-based data collection"
  },
  {
    "objectID": "about.html#my-approach",
    "href": "about.html#my-approach",
    "title": "About",
    "section": "My Approach",
    "text": "My Approach\nMy work focuses on creating practical measurement tools that bring biomechanics monitoring out of the lab and into real-world settings. By developing accessible systems to study exercise safety and effectiveness, I aim to support the development of better physical activity guidance for manual wheelchair users and the clinicians who work with them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I’m Griffin Sipes — a fourth-year PhD student in Mechanical Engineering at the University of Illinois Urbana-Champaign."
  },
  {
    "objectID": "index.html#featured-sections",
    "href": "index.html#featured-sections",
    "title": "Welcome!",
    "section": "Featured Sections",
    "text": "Featured Sections\n\nTutorials — Step-by-step guides and teaching materials.\nProjects — Portfolio projects."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Welcome!",
    "section": "Quick Links",
    "text": "Quick Links\n\nAbout\nProjects\nTutorials"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Welcome!",
    "section": "Contact",
    "text": "Contact\nIf you’d like to get in touch, find me on"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html",
    "href": "notebooks/IntroductionImageProcessing.html",
    "title": "Interactive Introduction to Image Processing",
    "section": "",
    "text": "Open in Colab"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#why-this-matters",
    "href": "notebooks/IntroductionImageProcessing.html#why-this-matters",
    "title": "Interactive Introduction to Image Processing",
    "section": "Why this matters",
    "text": "Why this matters\nImage processing is everywhere: improving medical images, cleaning up photos, extracting measurements, and powering computer vision systems such as object detectors and trackers. This notebook focuses on the practical building blocks you can reuse across projects."
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#what-youll-learn",
    "href": "notebooks/IntroductionImageProcessing.html#what-youll-learn",
    "title": "Interactive Introduction to Image Processing",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nHow images are represented (pixels, channels, color spaces)\nBasic transforms: resizing, rotating, and cropping\nFilters: blur, sharpen, edge detection, and morphological ops\nHow to inspect image statistics using histograms\nHow to detect objects and faces using modern libraries\nPractical tips for tuning parameters and debugging outputs"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#requirements",
    "href": "notebooks/IntroductionImageProcessing.html#requirements",
    "title": "Interactive Introduction to Image Processing",
    "section": "Requirements",
    "text": "Requirements\nTo run this notebook, you need the following:\n\nPython 3.8+\nJupyter Notebook (or JupyterLab)"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#tips-before-you-start",
    "href": "notebooks/IntroductionImageProcessing.html#tips-before-you-start",
    "title": "Interactive Introduction to Image Processing",
    "section": "Tips before you start",
    "text": "Tips before you start\n\nRun cells sequentially so variables (like img) are available.\nIf an example is slow, reduce sizes (resize to 320x240) for faster iteration.\nUse the provided test_image.jpg or change the filename to your own image."
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#resources-next-steps",
    "href": "notebooks/IntroductionImageProcessing.html#resources-next-steps",
    "title": "Interactive Introduction to Image Processing",
    "section": "Resources & next steps",
    "text": "Resources & next steps\n\nOpenCV documentation: https://docs.opencv.org/\nUltralytics (YOLO) docs: https://docs.ultralytics.com/\nMediaPipe face detection: https://developers.google.com/mediapipe\n\n\nEnvironment Setup\nThis cell sets up the environment for image and video processing. It downloads and imports essential libraries:\n\nos for file operations.\ncv2 (OpenCV) for image and video processing.\nnumpy for numerical operations.\nmatplotlib.pyplot for plotting images and results.\n\nIt also defines file paths for the test image, video, and YOLO model, and prints quick checks to confirm that these files exist and that OpenCV is installed. This ensures all required resources are available before running further image processing tasks.\n\n# Install required packages\n%pip install opencv-python matplotlib numpy ultralytics mediapipe ipython\n\nimport os\nimport cv2\nimport numpy as np\nimport requests\nfrom PIL import Image\n\n# Direct download links for Box files\nIMG_URL = 'https://uofi.box.com/shared/static/4vwmy8d4zutugdq54xj0jm98y2dsv8t0.jpg'\nVIDEO_URL = 'https://uofi.box.com/shared/static/of08em0yjvobx6xoqtxzk02glq9d8e55.mp4'\nYOLO_URL = 'https://uofi.box.com/shared/static/pma36x7cge58b3i18b2o0xqlcqd38iwk.pt'\n\ndef download_file(url, save_path):\n    r = requests.get(url, stream=True)\n    r.raise_for_status()\n    with open(save_path, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n    return save_path\n\n# Download files to /content/\nimg_path = '/content/test_image.jpg'\nvideo_path = '/content/GX010881.MP4'\nyolo_path = '/content/yolo11n.pt'\n\ndownload_file(IMG_URL, img_path)\ndownload_file(VIDEO_URL, video_path)\ndownload_file(YOLO_URL, yolo_path)\n\n# Load image\nimg = cv2.imread(img_path)\nprint('Image loaded:', img is not None)\n\n# Load video\nvideo = cv2.VideoCapture(video_path)\nprint('Video loaded:', video.isOpened())\n\n# Check YOLO model file\nprint('YOLO model downloaded:', os.path.exists(yolo_path))\n\ncv2 version: 4.11.0\nimg exists: True\nvideo exists: True\nmodel file exists: True\n\n\n\n\nLoading and Displaying Images with OpenCV\nOpenCV is a powerful library for image processing in Python. To get started, you need to load an image from disk and display it. Here’s how:\n\nLoading an Image\nUse cv2.imread() to load an image from a file. The image is read as a NumPy array in BGR (Blue, Green, Red) format.\n\n# Step 1: Load and Display an Image\nimport cv2\nfrom matplotlib import pyplot as plt\n\n# Load an image from file\nimg = cv2.imread('test_image.jpg')\n\nif img is not None:\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n    plt.imshow(img_rgb)\n    plt.title('Loaded Image')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not found. Try changing the filename to an image file.')\n\n\n\n\n\n\n\n\n\n\n\nColor Spaces and Channels\nImages are made up of pixels, and each pixel can have one or more channels depending on the color space used. Understanding color spaces and channels is essential for effective image analysis and processing.\n\nWhat is a Color Space?\nA color space is a specific way of representing colors numerically. It defines how pixel values map to actual colors. Common color spaces include RGB, LAB, and HSV.\n\n\nWhat is a Channel?\nA channel is a single component of a color space. For example, in RGB, each pixel has three channels: Red, Green, and Blue. In grayscale images, there is only one channel representing intensity.\n\n\nCommon Color Spaces\n\nRGB (Red, Green, Blue):\n\nEach pixel has three channels: R, G, and B.\nUsed for display and general image processing.\nNot perceptually uniform.\n\n**LAB (Lab*):**\n\nThree channels: L (lightness), a (green–red), b (blue–yellow).\nDesigned for perceptual uniformity.\nUseful for color correction and measuring color differences.\n\nHSV (Hue, Saturation, Value):\n\nThree channels: H (hue), S (saturation), V (value/brightness).\nSeparates color information (hue) from intensity (value).\nUseful for color-based segmentation and filtering.\n\n\n\n\nWhy Channels Matter\n\nChannel manipulation: You can process each channel separately (e.g., enhance brightness, isolate colors).\nVisualization: Viewing individual channels helps understand image structure and color distribution.\nAnalysis: Some algorithms work better on specific channels (e.g., edge detection on intensity, segmentation on hue).\n\nTip:\nChoose the color space and channels that best fit your task. For example, use LAB for brightness/contrast adjustment, HSV for color segmentation, and RGB for visualization.\n\n# Show original image and individual color channels (RGB, LAB, HSV)\nif img is not None:\n    # RGB channels\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    r, g, b = img_rgb[:, :, 0], img_rgb[:, :, 1], img_rgb[:, :, 2]\n    # LAB channels\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l_lab, a_lab, b_lab = cv2.split(lab)\n    # HSV channels\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h_hsv, s_hsv, v_hsv = cv2.split(hsv)\n\n    plt.figure(figsize=(16, 8))\n    # Row 1: RGB\n    plt.subplot(3, 4, 1)\n    plt.imshow(img_rgb)\n    plt.title('RGB (original)')\n    plt.axis('off')\n    plt.subplot(3, 4, 2)\n    plt.imshow(r, cmap='Reds')\n    plt.title('Red channel')\n    plt.axis('off')\n    plt.subplot(3, 4, 3)\n    plt.imshow(g, cmap='Greens')\n    plt.title('Green channel')\n    plt.axis('off')\n    plt.subplot(3, 4, 4)\n    plt.imshow(b, cmap='Blues')\n    plt.title('Blue channel')\n    plt.axis('off')\n\n    # Row 2: LAB\n    plt.subplot(3, 4, 5)\n    plt.imshow(l_lab, cmap='gray')\n    plt.title('LAB L (Lightness)')\n    plt.axis('off')\n    plt.subplot(3, 4, 6)\n    plt.imshow(a_lab, cmap='RdYlGn')\n    plt.title('LAB a (Green-Red)')\n    plt.axis('off')\n    plt.subplot(3, 4, 7)\n    plt.imshow(b_lab, cmap='RdYlBu')\n    plt.title('LAB b (Blue-Yellow)')\n    plt.axis('off')\n    plt.subplot(3, 4, 8)\n    plt.axis('off')  # Empty for layout\n\n    # Row 3: HSV\n    plt.subplot(3, 4, 9)\n    plt.imshow(h_hsv, cmap='hsv')\n    plt.title('HSV H (Hue)')\n    plt.axis('off')\n    plt.subplot(3, 4, 10)\n    plt.imshow(s_hsv, cmap='gray')\n    plt.title('HSV S (Saturation)')\n    plt.axis('off')\n    plt.subplot(3, 4, 11)\n    plt.imshow(v_hsv, cmap='gray')\n    plt.title('HSV V (Value)')\n    plt.axis('off')\n    plt.subplot(3, 4, 12)\n    plt.axis('off')  # Empty for layout\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nGrayscale imagery — why and how\nGrayscale images use a single intensity channel, making them faster to process and easier for many algorithms (e.g., edge detection, thresholding). Converting to grayscale is a common first step in image analysis.\n\nBackground\nMost digital images are captured in color, with each pixel containing multiple values (channels) for red, green, and blue. However, many image processing tasks—such as measuring brightness, detecting edges, or segmenting objects—work best on simpler data. Grayscale images reduce complexity by representing each pixel with a single value for intensity, ranging from black (0) to white (255).\nGrayscale conversion is widely used in medical imaging, document analysis, and computer vision because it: - Removes color distractions, focusing on structure and contrast. - Speeds up processing and reduces memory usage. - Simplifies algorithms that rely on intensity rather than color.\nTip:\nStart with grayscale for tasks like thresholding, edge detection, and morphological operations. Use color only when necessary for segmentation or visualization.\n\n# Convert the loaded image to grayscale and display it\nif img is not None:\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Grayscale Image')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\nImage Resizing and Rotation\nResizing and rotating images are essential preprocessing steps in image analysis and computer vision.\nWhy resize?\n- Reduces memory and computation time, especially for large images or real-time applications. - Enables faster experimentation and model training. - Helps standardize input sizes for neural networks and algorithms.\nWhy rotate?\n- Corrects image orientation for consistent analysis. - Useful for aligning features or objects in medical, satellite, or document images.\nTip:\nWhen enlarging images, use interpolation methods (e.g., bilinear, bicubic) to avoid pixelation and preserve quality. For shrinking, simple nearest-neighbor or area interpolation is often sufficient.\nResizing and rotating are quick ways to optimize your workflow and improve downstream results.\n\n# Resize and rotate examples\nif img is not None:\n    # Use the grayscale image 'gray' if available, otherwise convert\n    small = cv2.resize(gray, (320, 240))\n    rotated = cv2.rotate(small, cv2.ROTATE_90_CLOCKWISE)\n    plt.figure(figsize=(8, 4))\n    plt.subplot(1, 2, 1)\n    plt.imshow(small, cmap='gray')\n    plt.title('Resized (320x240)')\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(rotated, cmap='gray')\n    plt.title('Rotated 90 deg')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nSmoothing / Blur — Purpose, Background, and Sharpening\nBlurring (smoothing) is a fundamental image processing technique that reduces high-frequency noise and small details. It is commonly used before edge detection, thresholding, or segmentation to suppress speckle and minor artifacts, making features easier to analyze.\n\nWhy Blur?\n\nNoise reduction: Removes random pixel fluctuations and small unwanted details.\nPreprocessing: Improves the reliability of subsequent steps like edge detection and binarization.\nVisual effect: Produces a softer, less detailed image.\n\nA Gaussian blur uses a weighted kernel (must be odd-sized, e.g., 3x3, 5x5, 11x11) to average pixel values, giving more weight to the center. Larger kernels produce stronger smoothing.\n\n\nSharpening\nSharpening enhances edges and fine details, making features stand out. It is often used after blurring or on its own to improve image clarity.\n\nHow it works: Sharpening applies a kernel that emphasizes differences between neighboring pixels, boosting contrast at edges.\nCommon method: The Laplacian or unsharp mask filter.\n\nTip:\nUse blurring to clean up noise before analysis. Use sharpening to highlight boundaries and details for visualization or feature extraction.\nExample kernels: - Gaussian blur:\n[[1, 2, 1],      [2, 4, 2],      [1, 2, 1]] / 16 - Sharpening:\n[[ 0, -1,  0],      [-1,  5, -1],      [ 0, -1,  0]]\n\nif img is not None:\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Original Grayscale')\n    plt.axis('off')\n\n    # Gaussian blur with 11x11 kernel and sigma=0\n    blurred = cv2.GaussianBlur(gray, (11, 11), 0)\n    plt.subplot(1, 3, 2)\n    plt.imshow(blurred, cmap='gray')\n    plt.title('Blurred (Gaussian)')\n    plt.axis('off')\n\n    # Sharpening kernel\n    sharpen_kernel = np.array([[0, -1, 0],\n                               [-1, 5, -1],\n                               [0, -1, 0]])\n    sharpened = cv2.filter2D(gray, -1, sharpen_kernel)\n    plt.subplot(1, 3, 3)\n    plt.imshow(sharpened, cmap='gray')\n    plt.title('Sharpened')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nHistograms — Background & Intuition\nAn image histogram is a graphical representation of the distribution of pixel intensities (brightness or color values) in an image. It helps you understand the overall exposure, contrast, and color balance at a glance.\n\nWhat does a histogram show?\n\nX-axis: Pixel intensity values (0–255 for 8-bit images).\nY-axis: Number of pixels at each intensity.\nGrayscale images: One histogram for brightness.\nColor images: Separate histograms for each channel (Red, Green, Blue).\n\n\n\nWhy are histograms useful?\n\nContrast: A wide histogram means high contrast; a narrow one means low contrast.\nBrightness: If the histogram is shifted left, the image is dark; shifted right, it’s bright.\nExposure: Peaks at the ends may indicate underexposure (too dark) or overexposure (too bright).\nColor balance: Comparing channel histograms reveals color casts or imbalances.\n\n\n\nPractical uses\n\nImage enhancement: Adjust brightness/contrast or apply histogram equalization.\nThresholding: Choose thresholds for binarization based on histogram shape.\nQuality control: Detect poor lighting or exposure problems.\n\nHistograms are a simple but powerful tool for diagnosing and improving images in any image processing workflow.\n\n# Intensity and channel histograms\nif img is not None:\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 3, 1)\n    plt.hist(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).ravel(), bins=256)\n    plt.title('Intensity histogram')\n    plt.subplot(1, 3, 2)\n    plt.hist(r.ravel(), bins=256, color='r')\n    plt.title('Red hist')\n    plt.subplot(1, 3, 3)\n    plt.hist(g.ravel(), bins=256, color='g')\n    plt.title('Green hist')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nHistogram-based Image Processing\nHistogram-based image processing uses the distribution of pixel intensities to analyze and enhance images. By examining the histogram, we can adjust brightness, contrast, and exposure, detect features, and segment regions. This approach is essential because it provides a quantitative way to understand image quality and apply targeted corrections, making images more useful for visualization and further analysis.\nCLAHE (Contrast Limited Adaptive Histogram Equalization) is an advanced method for improving image contrast, especially in images with varying lighting or local features. Unlike standard histogram equalization, which adjusts contrast globally, CLAHE works on small regions (tiles) of the image and limits amplification to avoid noise.\n\nStandard Histogram Equalization\n\nGlobal adjustment: Redistributes pixel intensities across the entire image.\nBest for: Images with uniform lighting and global low contrast.\nDrawbacks: Can over-amplify noise and create unnatural effects in areas with little variation.\n\n\n\nCLAHE\n\nLocal adjustment: Applies histogram equalization to small tiles, then combines them.\nContrast limiting: Prevents over-amplification of noise by clipping the histogram.\nBest for: Medical images, uneven lighting, or images with both bright and dark regions.\nAdvantages: Preserves local details, avoids noise amplification, and produces more natural results.\n\nSummary:\nUse standard histogram equalization for quick global contrast enhancement. Use CLAHE for images with local contrast issues, uneven illumination, or when you want to avoid boosting noise.\n\n# Histogram equalization and CLAHE comparison (grayscale, LAB luminance)\nif img is not None:\n    # Prepare processed images\n    gray_eq = cv2.equalizeHist(gray)\n    gray_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    img_clahe = gray_clahe.apply(gray)\n\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    l_eq = cv2.equalizeHist(l)\n    lab_eq = cv2.merge((l_eq, a, b))\n    lab_eq_rgb = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2RGB)\n    l_clahe = gray_clahe.apply(l)\n    lab_clahe = cv2.merge((l_clahe, a, b))\n    lab_clahe_rgb = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n\n    # Figure 1: Images (2 rows x 3 cols)\n    plt.figure(figsize=(18, 8))\n    plt.subplot(2, 3, 1)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Original Grayscale')\n    plt.axis('off')\n    plt.subplot(2, 3, 2)\n    plt.imshow(gray_eq, cmap='gray')\n    plt.title('Histogram Equalized')\n    plt.axis('off')\n    plt.subplot(2, 3, 3)\n    plt.imshow(img_clahe, cmap='gray')\n    plt.title('CLAHE Grayscale')\n    plt.axis('off')\n\n    plt.subplot(2, 3, 4)\n    plt.imshow(img_rgb)\n    plt.title('Original Color (RGB)')\n    plt.axis('off')\n    plt.subplot(2, 3, 5)\n    plt.imshow(lab_eq_rgb)\n    plt.title('Histogram Equalized LAB Luminance')\n    plt.axis('off')\n    plt.subplot(2, 3, 6)\n    plt.imshow(lab_clahe_rgb)\n    plt.title('CLAHE LAB Luminance')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n    # Figure 2: Histograms corresponding to the images (2 rows x 3 cols)\n    plt.figure(figsize=(18, 8))\n    plt.subplot(2, 3, 1)\n    plt.hist(gray.ravel(), bins=256, color='gray')\n    plt.title('Original Grayscale Hist')\n    plt.subplot(2, 3, 2)\n    plt.hist(gray_eq.ravel(), bins=256, color='blue')\n    plt.title('Hist Eq Grayscale Hist')\n    plt.subplot(2, 3, 3)\n    plt.hist(img_clahe.ravel(), bins=256, color='red')\n    plt.title('CLAHE Grayscale Hist')\n\n    plt.subplot(2, 3, 4)\n    # histogram for original LAB luminance channel\n    plt.hist(l.ravel(), bins=256, color='gray', alpha=0.7, label='Luminance')\n    plt.title('Original LAB Luminance Hist')\n    plt.ylim(0, 30000)\n    plt.subplot(2, 3, 5)\n    plt.hist(l_eq.ravel(), bins=256, color='purple')\n    plt.title('Histogram Equalized LAB Luminance')\n    plt.subplot(2, 3, 6)\n    plt.hist(l_clahe.ravel(), bins=256, color='orange')\n    plt.title('CLAHE Lab Luminance')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinarization\nBinarization is the process of converting a grayscale image into a binary image, where each pixel is either black (0) or white (255). This is done by applying a threshold: pixels above the threshold become white, and those below become black.\n\nWhy Use Binarization?\n\nSimplifies analysis: Many image processing tasks (like shape analysis, object counting, and OCR) work better on binary images.\nSeparates foreground from background: Useful for segmenting objects from their surroundings.\nPreprocessing for algorithms: Many algorithms (e.g., contour detection, morphological operations) require binary input.\n\n\n\nHow Does Binarization Work?\n\nChoose a threshold value (e.g., 100).\nCompare each pixel’s intensity to the threshold.\n\nIf the pixel value &gt; threshold, set it to 255 (white).\nIf the pixel value ≤ threshold, set it to 0 (black).\n\n\n\n\nOtsu’s Method — Automatic Threshold Selection\nOtsu’s method is a popular technique for automatically finding the optimal threshold value. It works by: - Analyzing the histogram of pixel intensities. - Finding the threshold that minimizes the variance within each class (foreground and background), or equivalently, maximizes the separation between them.\nAdvantages: - No manual tuning needed. - Works well when the image has a clear bimodal histogram (two peaks: one for background, one for foreground).\nIn practice:\nOtsu’s method is widely used for document scanning, medical imaging, and any scenario where robust, automatic binarization is needed.\n\nif img is not None:\n    # Show histogram\n    plt.figure(figsize=(10, 4))\n    plt.hist(gray.ravel(), bins=256, color='gray')\n    plt.title('Grayscale Histogram')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    # Otsu's thresholding\n    otsu_thresh, binary_img = cv2.threshold(\n        gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    plt.axvline(otsu_thresh, color='red', linestyle='dashed',\n                label=f'Otsu Threshold = {otsu_thresh:.0f}')\n    plt.legend()\n    plt.show()\n    # Show binarized image\n    plt.imshow(binary_img, cmap='gray')\n    plt.title('Binarized Image (Otsu Threshold)')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMorphological Operations\nMorphological operations are image processing techniques that probe and modify the shapes of objects in binary or grayscale images. The two most common operations are erosion and dilation.\n\nWhat is Erosion?\n\nPurpose: Erosion shrinks bright regions and removes small white noise. It is useful for eliminating tiny artifacts, separating objects that are close together, and reducing the size of foreground objects.\nHow it works: Erosion slides a small shape (called a structuring element or kernel) over the image. At each position, if all pixels under the kernel are bright (e.g., white in binary images), the output pixel remains bright; otherwise, it becomes dark. This causes boundaries of bright regions to shrink.\n\n\n\nWhat is Dilation?\n\nPurpose: Dilation expands bright regions and fills small holes or gaps. It is useful for joining broken parts of objects, making features thicker, and connecting nearby objects.\nHow it works: Dilation also slides a kernel over the image. At each position, if any pixel under the kernel is bright, the output pixel becomes bright. This causes boundaries of bright regions to grow outward.\n\n\n\nPractical Use\n\nNoise removal: Erosion followed by dilation (called opening) removes small noise while preserving object shape.\nObject joining: Dilation followed by erosion (called closing) fills small holes and connects nearby objects.\nParameter tuning: The size and shape of the kernel control the strength and direction of the effect. Larger kernels produce stronger changes.\n\nTip: Try different kernel sizes and shapes to see how they affect your image. Use erosion to clean up noise, and dilation to restore or connect features.\n\n# Erosion and dilation on binarized image\nif img is not None:\n    plt.figure(figsize=(15, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(binary_img, cmap='gray')\n    plt.title('Original Binary')\n    plt.axis('off')\n\n    # 15x15 kernel for demo. Adjust size as needed.\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n\n    eroded_bin = cv2.erode(binary_img, kernel, iterations=1)\n    dilated_bin = cv2.dilate(binary_img, kernel, iterations=1)\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(eroded_bin, cmap='gray')\n    plt.title('Erosion (15x15)')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(dilated_bin, cmap='gray')\n    plt.title('Dilation (15x15)')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nEdge Detection\nEdge detection is a fundamental technique in image processing and computer vision. Edges represent boundaries where pixel intensities change sharply, often corresponding to object outlines, texture changes, or surface discontinuities.\n\nWhy Detect Edges?\n\nObject boundaries: Edges help segment objects from the background.\nFeature extraction: Many algorithms use edges to identify shapes, corners, and regions of interest.\nImage understanding: Edges simplify images, making it easier to analyze and interpret content.\n\n\n\nHow Does Edge Detection Work?\nEdge detectors analyze local intensity changes in an image. They highlight pixels where the difference between neighboring values is large. Common approaches include: - Gradient-based methods: Compute the rate of change (gradient) in intensity. Examples: Sobel, Prewitt, Roberts. - Canny edge detector: A multi-stage algorithm that smooths the image, finds gradients, applies non-maximum suppression, and uses double thresholding to select strong and weak edges.\n\n\nCanny Edge Detector\nThe Canny method is popular because it produces clean, thin edges and reduces noise. It uses two thresholds: - Low threshold: Detects weak edges. - High threshold: Detects strong edges. Edges connected to strong edges are kept; isolated weak edges are discarded.\nTip: Adjust thresholds to control edge sensitivity. Lower values reveal more edges (including noise); higher values show only the most prominent boundaries.\nEdge detection is a key step for tasks like segmentation, tracking, and recognition.\n\n# Compare Sobel gradient (simpler) with Canny edge detection (more complex)\nif img is not None:\n    # Sobel gradient (X and Y)\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n    sobel_mag = cv2.magnitude(sobelx, sobely)\n    sobel_mag = cv2.convertScaleAbs(sobel_mag)\n\n    # Canny edge detection\n    edges_canny = cv2.Canny(gray, 200, 400)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(sobel_mag, cmap='gray')\n    plt.title('Sobel Gradient Magnitude')\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(edges_canny, cmap='gray')\n    plt.title('Canny Edge Detection')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nObject Detection and Face Detection\nModern image processing leverages deep learning models to automatically detect and localize objects and faces in images and videos. Two popular approaches are YOLO (You Only Look Once) for general object detection and MediaPipe Face Detection for fast, lightweight face localization.\n\nYOLO Object Detection\n\nYOLO is a family of real-time object detectors that predict bounding boxes and class labels for multiple objects in a single pass through the image.\nHow it works: YOLO divides the image into a grid and simultaneously predicts bounding boxes and class probabilities for each cell.\nAdvantages: Fast, accurate, and suitable for real-time applications such as video analysis, robotics, and surveillance.\nUse cases: Detecting people, vehicles, animals, and everyday objects in images or video frames.\n\n\n# Step 3: Object Detection with YOLO\nfrom ultralytics import YOLO\n\n# The YOLO model loaded above (yolo_model) is a pre-trained deep learning object detector from the Ultralytics YOLO family.\n# YOLO (\"You Only Look Once\") models are designed for real-time object detection in images and videos.\n# They predict bounding boxes and class labels for multiple objects in a single forward pass.\n# The model file 'yolo11n.pt' is a specific version/size of YOLO.\n\nyolo_model = YOLO('yolo11n.pt')\n\n# Load the first frame of the video file and use it as the image\nvideo_path = 'GX010881.MP4'  # Change to your video file name if needed\ncap = cv2.VideoCapture(video_path)\nret, frame = cap.read()\ncap.release()\nif ret:\n    img = frame\n    # Run YOLO detection on the first frame\n    results = yolo_model(img)\n    img_detected = img.copy()\n    for result in results:\n        for box in result.boxes:\n            class_id = int(box.cls[0])\n            confidence = float(box.conf[0])\n            class_name = yolo_model.names[class_id]\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\n            if confidence &gt; 0.2:\n                cv2.rectangle(img_detected, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(img_detected, f'{class_name} {confidence:.2f}',\n                            (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n    img_detected_rgb = cv2.cvtColor(img_detected, cv2.COLOR_BGR2RGB)\n    plt.imshow(img_detected_rgb)\n    plt.title('YOLO Object Detection (First Video Frame)')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Could not load first frame from video.')\n\n\n\nMediaPipe Face Detection\n\nMediaPipe is a framework from Google for building cross-platform ML pipelines. Its face detection model is designed for real-time applications and works well on both close-up and wide-angle scenes.\nHow it works: The model uses machine learning to find faces and estimate their bounding boxes, returning confidence scores for each detection.\nAdvantages: Lightweight, fast, and robust to variations in pose and lighting.\nUse cases: Face tracking, privacy blurring, selfie enhancement, and interactive applications.\n\n\n# Step 4: Face Detection with MediaPipe\nimport mediapipe as mp\n\n# The MediaPipe Face Detection model is a lightweight, real-time face detector from Google.\n# It uses machine learning to find faces and estimate their bounding boxes in images or video frames.\n# The model is fast and works well for both close-up selfies and wider scenes, depending on the model_selection parameter.\n# It outputs detection objects with bounding box coordinates and confidence scores.\nmp_face_detection = mp.solutions.face_detection\n\nif img is not None:\n    # Tip: model_selection=0 is tuned for close-up faces (e.g., selfies); model_selection=1 is for wider scenes.\n    # Tip: increase min_detection_confidence (0.0-1.0) to reduce false positives at the cost of missing faint faces.\n    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        results = face_detection.process(img_rgb)\n        img_faces = img_rgb.copy()\n        h_img, w_img = img_faces.shape[:2]\n        if results.detections:\n            for detection in results.detections:\n                bboxC = detection.location_data.relative_bounding_box\n                x = int(bboxC.xmin * w_img)\n                y = int(bboxC.ymin * h_img)\n                w = int(bboxC.width * w_img)\n                h = int(bboxC.height * h_img)\n                cv2.rectangle(img_faces, (x, y),\n                              (x + w, y + h), (255, 0, 0), 2)\n            plt.imshow(img_faces)\n            plt.title('MediaPipe Face Detection')\n            plt.axis('off')\n            plt.show()\n            # Note: each detection object has a score (detection.score). You can filter detections by inspecting that value.\n            print(f'Faces detected: {len(results.detections)}')\n        else:\n            print('No faces detected.')\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\nVideo Processing: Object and Face Detection with Privacy Blurring\nThe next cell defines and runs a function called process_video that processes a video file frame-by-frame. For each frame, it performs:\n\nObject Detection (YOLO):\nUses the YOLO model to detect people in the frame and draws green rectangles around them.\nFace Detection (MediaPipe):\nUses MediaPipe to detect faces, draws blue rectangles around each detected face, and applies a blur to the face region for privacy protection.\nInteractive Display:\nThe processed frames are displayed interactively in the notebook, allowing you to watch the detection and blurring in real time.\nRobust Face Tracking:\nIf a face is missed in a frame, the function keeps the last known face location for a few frames to maintain privacy blurring even during brief detection failures.\n\nThis workflow demonstrates how to combine deep learning-based object detection and lightweight face detection for privacy-aware video analysis in Python.\n\nfrom IPython.display import display, clear_output\nimport contextlib\nimport os\nfrom ultralytics import YOLO\n\n\ndef _get_valid_ksize(fw, fh, base=51):\n    ksize = min(fw, fh, base)\n    ksize = max(ksize, 3)\n    if ksize % 2 == 0:\n        ksize += 1\n    return int(ksize)\n\n\ndef process_video(video_path, max_frames=30, max_missed=5):\n    yolo_model = YOLO('yolo11n.pt')\n    mp_face_detection = mp.solutions.face_detection\n    cap = cv2.VideoCapture(video_path)\n    last_face_boxes = []\n    missed_frames = 0\n    frames_shown = 0\n    fig, ax = plt.subplots(figsize=(10, 6))\n    img_plot = None\n    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n        while cap.isOpened() and frames_shown &lt; max_frames:\n            ret, frame = cap.read()\n            if not ret:\n                print('End of video or cannot read the frame.')\n                break\n            # YOLO person detection (suppress output)\n            with open(os.devnull, 'w') as fnull, contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n                results = yolo_model(frame, verbose=False)\n            for result in results:\n                for box in result.boxes:\n                    class_id = int(box.cls[0])\n                    confidence = float(box.conf[0])\n                    class_name = yolo_model.names[class_id]\n                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n                    if confidence &gt; 0.5 and class_name == 'person':\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (0, 255, 0), 2)\n                        cv2.putText(frame, f'Person {confidence:.2f}', (\n                            x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n            # MediaPipe face detection\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            results = face_detection.process(rgb_frame)\n            h_img, w_img = frame.shape[:2]\n            current_face_boxes = []\n            if results.detections:\n                for detection in results.detections:\n                    bboxC = detection.location_data.relative_bounding_box\n                    x = int(bboxC.xmin * w_img)\n                    y = int(bboxC.ymin * h_img)\n                    w = int(bboxC.width * w_img)\n                    h = int(bboxC.height * h_img)\n                    x1 = max(x, 0)\n                    y1 = max(y, 0)\n                    x2 = min(x + w, w_img)\n                    y2 = min(y + h, h_img)\n                    current_face_boxes.append((x1, y1, x2, y2))\n            if current_face_boxes:\n                last_face_boxes = current_face_boxes\n                missed_frames = 0\n            else:\n                missed_frames += 1\n            # Draw blue rectangle, add text, and blur face for each detected face\n            if last_face_boxes and missed_frames &lt;= max_missed:\n                for (x1, y1, x2, y2) in last_face_boxes:\n                    face_roi = frame[y1:y2, x1:x2]\n                    fh, fw = face_roi.shape[:2]\n                    ksize = _get_valid_ksize(\n                        fw, fh, base=51 + 10 * missed_frames)\n                    if face_roi.size &gt; 0 and ksize &gt; 1:\n                        blurred_face = cv2.GaussianBlur(\n                            face_roi, (ksize, ksize), 0)\n                        frame[y1:y2, x1:x2] = blurred_face\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (255, 0, 0), 2)\n                        cv2.putText(frame, 'Face', (x1, y1-10),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n            # Update the image plot\n            clear_output(wait=True)\n            if img_plot is None:\n                img_plot = ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                ax.axis('off')\n            else:\n                img_plot.set_data(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                fig.canvas.draw_idle()\n            display(plt.gcf())\n            frames_shown += 1\n    cap.release()\n\n\nprocess_video('test_video.MP4')  # Change to your video file name"
  },
  {
    "objectID": "projects.html#evaluation-of-adapted-exercise-modes",
    "href": "projects.html#evaluation-of-adapted-exercise-modes",
    "title": "Projects",
    "section": "Evaluation of Adapted Exercise Modes",
    "text": "Evaluation of Adapted Exercise Modes\n\n\nConducting community-based evaluations of adapted exercise modes using wearable sensors and musculoskeletal modeling to assess musculoskeletal safety and participant enjoyment.\nData collection combines graded exercise testing on a custom wheelchair ergometer (following ACSM protocols) with personalized workouts performed in community settings.\nThe study recruits participants across a broad age range and varied activity backgrounds (athletic and non‑athletic) to improve generalizability.\nPrimary outcomes include shoulder loading and propulsion metrics derived from musculoskeletal modeling and sensor data, alongside self-reported enjoyment to compare exercise modes.\nPresentation: Evaluation of propulsion biomechanics during exercise with a suspended-wheel everyday wheelchair (American Society of Biomechanics Annual Meeting, 2024)"
  },
  {
    "objectID": "projects.html#smartwatch-based-wheelchair-propulsion-monitoring",
    "href": "projects.html#smartwatch-based-wheelchair-propulsion-monitoring",
    "title": "Projects",
    "section": "Smartwatch-Based Wheelchair Propulsion Monitoring",
    "text": "Smartwatch-Based Wheelchair Propulsion Monitoring\n\nDeveloping a novel method to predict manual wheelchair propulsion kinetics using only smartwatch sensor data. This work addresses the limitations of gold-standard devices (e.g., the SMARTWheel), which are no longer widely available and are restricted to laboratory use. The system aims to predict all six components of hand reaction loads (three forces and three moments) and to enable calculation of common wheelchair propulsion metrics. This pilot study is evaluating whether consumer-grade wearables can provide accurate, field-deployable measures of biomechanical loading to support injury prevention and rehabilitation for manual wheelchair users.\n\nPresentation: Predicting manual wheelchair propulsion kinetics using smartwatch data: a pilot study (MRC Annual Symposium, Washington University in St. Louis, 2025)"
  },
  {
    "objectID": "projects.html#handcycling-force-prediction-from-wearable-sensors",
    "href": "projects.html#handcycling-force-prediction-from-wearable-sensors",
    "title": "Projects",
    "section": "Handcycling Force Prediction from Wearable Sensors",
    "text": "Handcycling Force Prediction from Wearable Sensors\n\nInvestigated the feasibility of predicting continuous hand reaction forces during handcycling using only arm segment kinematics. This work compared multiple machine learning architectures including temporal convolutional networks (TCN), residual networks, and ensemble methods against traditional statistical approaches. The TCN model achieved correlation coefficients up to r=0.97 for in-plane forces, enabling replacement of instrumented crank handles in future studies.\n\nPublication: Kinematics-Based Predictions of External Loads during Handcycling (Sensors, 2024)\nPresentation: Estimating hand reaction forces from arm segment accelerations during handcycle propulsion using machine learning (Congress of the International Society of Biomechanics, 2023)"
  },
  {
    "objectID": "projects.html#flexible-multi-camera-markerless-motion-capture-pipeline",
    "href": "projects.html#flexible-multi-camera-markerless-motion-capture-pipeline",
    "title": "Projects",
    "section": "Flexible Multi-Camera Markerless Motion Capture Pipeline",
    "text": "Flexible Multi-Camera Markerless Motion Capture Pipeline\n\nDeveloping a scalable motion capture system that works with any N≥2 consumer cameras to achieve research-grade 3D motion tracking. The complete pipeline includes:\n\nCamera Calibration: Estimating intrinsic parameters for each camera using Zhang’s method with planar calibration targets\nLens Distortion Correction: Applying radial and tangential distortion models to correct image artifacts\nCamera Pose Estimation: Solving the Perspective-n-Point (PnP) problem to determine extrinsic camera parameters\nKeypoint Tracking: Using deep learning-based keypoint detection (DeepLabCut) to track anatomical landmarks across multiple views\n3D Reconstruction: Triangulating 2D keypoints into accurate 3D coordinates using multi-view geometric constraints"
  },
  {
    "objectID": "projects.html#machine-learning-for-equine-ground-reaction-force-estimation",
    "href": "projects.html#machine-learning-for-equine-ground-reaction-force-estimation",
    "title": "Projects",
    "section": "Machine Learning for Equine Ground Reaction Force Estimation",
    "text": "Machine Learning for Equine Ground Reaction Force Estimation\n\n\nEstimated equine ground reaction forces using kinematic and anthropometric data with machine learning models.\nDeveloped statistical approaches to predict loading during walking and trotting, providing non-invasive methods for veterinary biomechanical assessment.\nPublication: Statistical approaches for estimating forelimb ground reaction forces in foals during walking and trotting (Journal of Biomechanics, 2025 — Submitted)"
  },
  {
    "objectID": "projects.html#finite-element-analysis-of-equine-bone-fracture-risk",
    "href": "projects.html#finite-element-analysis-of-equine-bone-fracture-risk",
    "title": "Projects",
    "section": "Finite Element Analysis of Equine Bone Fracture Risk",
    "text": "Finite Element Analysis of Equine Bone Fracture Risk\n\n\nAnalyzed high fracture risk regions of equine bone using subject-specific finite element models.\nCreated detailed models from CT data to understand stress distribution in fracture-prone areas of the third metacarpal.\nPresentation: Bone quality differences in fracture-prone regions of the equine third metacarpal (Orthopaedic Research Society, 2021)"
  }
]