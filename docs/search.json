[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Interactive Introduction to Image Processing"
  },
  {
    "objectID": "tutorials.html#tutorials",
    "href": "tutorials.html#tutorials",
    "title": "Tutorials",
    "section": "",
    "text": "Interactive Introduction to Image Processing"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No projects found."
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "Projects",
    "section": "",
    "text": "No projects found."
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html",
    "href": "notebooks/IntroductionImageProcessing.html",
    "title": "Interactive Introduction to Image Processing",
    "section": "",
    "text": "Open in Colab"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#why-this-matters",
    "href": "notebooks/IntroductionImageProcessing.html#why-this-matters",
    "title": "Interactive Introduction to Image Processing",
    "section": "Why this matters",
    "text": "Why this matters\nImage processing is everywhere: improving medical images, cleaning up photos, extracting measurements, and powering computer vision systems such as object detectors and trackers. This notebook focuses on the practical building blocks you can reuse across projects."
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#what-youll-learn",
    "href": "notebooks/IntroductionImageProcessing.html#what-youll-learn",
    "title": "Interactive Introduction to Image Processing",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nHow images are represented (pixels, channels, color spaces)\nBasic transforms: resizing, rotating, and cropping\nFilters: blur, sharpen, edge detection, and morphological ops\nHow to inspect image statistics using histograms\nHow to detect objects and faces using modern libraries\nPractical tips for tuning parameters and debugging outputs"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#requirements",
    "href": "notebooks/IntroductionImageProcessing.html#requirements",
    "title": "Interactive Introduction to Image Processing",
    "section": "Requirements",
    "text": "Requirements\nTo run this notebook, you need the following:\n\nPython 3.8+\nJupyter Notebook (or JupyterLab)"
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#tips-before-you-start",
    "href": "notebooks/IntroductionImageProcessing.html#tips-before-you-start",
    "title": "Interactive Introduction to Image Processing",
    "section": "Tips before you start",
    "text": "Tips before you start\n\nRun cells sequentially so variables (like img) are available.\nIf an example is slow, reduce sizes (resize to 320x240) for faster iteration.\nUse the provided test_image.jpg or change the filename to your own image."
  },
  {
    "objectID": "notebooks/IntroductionImageProcessing.html#resources-next-steps",
    "href": "notebooks/IntroductionImageProcessing.html#resources-next-steps",
    "title": "Interactive Introduction to Image Processing",
    "section": "Resources & next steps",
    "text": "Resources & next steps\n\nOpenCV documentation: https://docs.opencv.org/\nUltralytics (YOLO) docs: https://docs.ultralytics.com/\nMediaPipe face detection: https://developers.google.com/mediapipe\n\n\nEnvironment Setup\nThis cell sets up the environment for image and video processing. It downloads and imports essential libraries:\n\nos for file operations.\ncv2 (OpenCV) for image and video processing.\nnumpy for numerical operations.\nmatplotlib.pyplot for plotting images and results.\n\nIt also defines file paths for the test image, video, and YOLO model, and prints quick checks to confirm that these files exist and that OpenCV is installed. This ensures all required resources are available before running further image processing tasks.\n\n# Install required packages\n%pip install opencv-python matplotlib numpy ultralytics mediapipe ipython\n\nimport os\nimport cv2\nimport numpy as np\nimport requests\nfrom PIL import Image\n\n# Direct download links for Box files\nIMG_URL = 'https://uofi.box.com/shared/static/4vwmy8d4zutugdq54xj0jm98y2dsv8t0.jpg'\nVIDEO_URL = 'https://uofi.box.com/shared/static/of08em0yjvobx6xoqtxzk02glq9d8e55.mp4'\nYOLO_URL = 'https://uofi.box.com/shared/static/pma36x7cge58b3i18b2o0xqlcqd38iwk.pt'\n\ndef download_file(url, save_path):\n    r = requests.get(url, stream=True)\n    r.raise_for_status()\n    with open(save_path, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            f.write(chunk)\n    return save_path\n\n# Download files to /content/\nimg_path = '/content/test_image.jpg'\nvideo_path = '/content/GX010881.MP4'\nyolo_path = '/content/yolo11n.pt'\n\ndownload_file(IMG_URL, img_path)\ndownload_file(VIDEO_URL, video_path)\ndownload_file(YOLO_URL, yolo_path)\n\n# Load image\nimg = cv2.imread(img_path)\nprint('Image loaded:', img is not None)\n\n# Load video\nvideo = cv2.VideoCapture(video_path)\nprint('Video loaded:', video.isOpened())\n\n# Check YOLO model file\nprint('YOLO model downloaded:', os.path.exists(yolo_path))\n\ncv2 version: 4.11.0\nimg exists: True\nvideo exists: True\nmodel file exists: True\n\n\n\n\nLoading and Displaying Images with OpenCV\nOpenCV is a powerful library for image processing in Python. To get started, you need to load an image from disk and display it. Here’s how:\n\nLoading an Image\nUse cv2.imread() to load an image from a file. The image is read as a NumPy array in BGR (Blue, Green, Red) format.\n\n# Step 1: Load and Display an Image\nimport cv2\nfrom matplotlib import pyplot as plt\n\n# Load an image from file\nimg = cv2.imread('test_image.jpg')\n\nif img is not None:\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n    plt.imshow(img_rgb)\n    plt.title('Loaded Image')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not found. Try changing the filename to an image file.')\n\n\n\n\n\n\n\n\n\n\n\nColor Spaces and Channels\nImages are made up of pixels, and each pixel can have one or more channels depending on the color space used. Understanding color spaces and channels is essential for effective image analysis and processing.\n\nWhat is a Color Space?\nA color space is a specific way of representing colors numerically. It defines how pixel values map to actual colors. Common color spaces include RGB, LAB, and HSV.\n\n\nWhat is a Channel?\nA channel is a single component of a color space. For example, in RGB, each pixel has three channels: Red, Green, and Blue. In grayscale images, there is only one channel representing intensity.\n\n\nCommon Color Spaces\n\nRGB (Red, Green, Blue):\n\nEach pixel has three channels: R, G, and B.\nUsed for display and general image processing.\nNot perceptually uniform.\n\n**LAB (Lab*):**\n\nThree channels: L (lightness), a (green–red), b (blue–yellow).\nDesigned for perceptual uniformity.\nUseful for color correction and measuring color differences.\n\nHSV (Hue, Saturation, Value):\n\nThree channels: H (hue), S (saturation), V (value/brightness).\nSeparates color information (hue) from intensity (value).\nUseful for color-based segmentation and filtering.\n\n\n\n\nWhy Channels Matter\n\nChannel manipulation: You can process each channel separately (e.g., enhance brightness, isolate colors).\nVisualization: Viewing individual channels helps understand image structure and color distribution.\nAnalysis: Some algorithms work better on specific channels (e.g., edge detection on intensity, segmentation on hue).\n\nTip:\nChoose the color space and channels that best fit your task. For example, use LAB for brightness/contrast adjustment, HSV for color segmentation, and RGB for visualization.\n\n# Show original image and individual color channels (RGB, LAB, HSV)\nif img is not None:\n    # RGB channels\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    r, g, b = img_rgb[:, :, 0], img_rgb[:, :, 1], img_rgb[:, :, 2]\n    # LAB channels\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l_lab, a_lab, b_lab = cv2.split(lab)\n    # HSV channels\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h_hsv, s_hsv, v_hsv = cv2.split(hsv)\n\n    plt.figure(figsize=(16, 8))\n    # Row 1: RGB\n    plt.subplot(3, 4, 1)\n    plt.imshow(img_rgb)\n    plt.title('RGB (original)')\n    plt.axis('off')\n    plt.subplot(3, 4, 2)\n    plt.imshow(r, cmap='Reds')\n    plt.title('Red channel')\n    plt.axis('off')\n    plt.subplot(3, 4, 3)\n    plt.imshow(g, cmap='Greens')\n    plt.title('Green channel')\n    plt.axis('off')\n    plt.subplot(3, 4, 4)\n    plt.imshow(b, cmap='Blues')\n    plt.title('Blue channel')\n    plt.axis('off')\n\n    # Row 2: LAB\n    plt.subplot(3, 4, 5)\n    plt.imshow(l_lab, cmap='gray')\n    plt.title('LAB L (Lightness)')\n    plt.axis('off')\n    plt.subplot(3, 4, 6)\n    plt.imshow(a_lab, cmap='RdYlGn')\n    plt.title('LAB a (Green-Red)')\n    plt.axis('off')\n    plt.subplot(3, 4, 7)\n    plt.imshow(b_lab, cmap='RdYlBu')\n    plt.title('LAB b (Blue-Yellow)')\n    plt.axis('off')\n    plt.subplot(3, 4, 8)\n    plt.axis('off')  # Empty for layout\n\n    # Row 3: HSV\n    plt.subplot(3, 4, 9)\n    plt.imshow(h_hsv, cmap='hsv')\n    plt.title('HSV H (Hue)')\n    plt.axis('off')\n    plt.subplot(3, 4, 10)\n    plt.imshow(s_hsv, cmap='gray')\n    plt.title('HSV S (Saturation)')\n    plt.axis('off')\n    plt.subplot(3, 4, 11)\n    plt.imshow(v_hsv, cmap='gray')\n    plt.title('HSV V (Value)')\n    plt.axis('off')\n    plt.subplot(3, 4, 12)\n    plt.axis('off')  # Empty for layout\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nGrayscale imagery — why and how\nGrayscale images use a single intensity channel, making them faster to process and easier for many algorithms (e.g., edge detection, thresholding). Converting to grayscale is a common first step in image analysis.\n\nBackground\nMost digital images are captured in color, with each pixel containing multiple values (channels) for red, green, and blue. However, many image processing tasks—such as measuring brightness, detecting edges, or segmenting objects—work best on simpler data. Grayscale images reduce complexity by representing each pixel with a single value for intensity, ranging from black (0) to white (255).\nGrayscale conversion is widely used in medical imaging, document analysis, and computer vision because it: - Removes color distractions, focusing on structure and contrast. - Speeds up processing and reduces memory usage. - Simplifies algorithms that rely on intensity rather than color.\nTip:\nStart with grayscale for tasks like thresholding, edge detection, and morphological operations. Use color only when necessary for segmentation or visualization.\n\n# Convert the loaded image to grayscale and display it\nif img is not None:\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Grayscale Image')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\nImage Resizing and Rotation\nResizing and rotating images are essential preprocessing steps in image analysis and computer vision.\nWhy resize?\n- Reduces memory and computation time, especially for large images or real-time applications. - Enables faster experimentation and model training. - Helps standardize input sizes for neural networks and algorithms.\nWhy rotate?\n- Corrects image orientation for consistent analysis. - Useful for aligning features or objects in medical, satellite, or document images.\nTip:\nWhen enlarging images, use interpolation methods (e.g., bilinear, bicubic) to avoid pixelation and preserve quality. For shrinking, simple nearest-neighbor or area interpolation is often sufficient.\nResizing and rotating are quick ways to optimize your workflow and improve downstream results.\n\n# Resize and rotate examples\nif img is not None:\n    # Use the grayscale image 'gray' if available, otherwise convert\n    small = cv2.resize(gray, (320, 240))\n    rotated = cv2.rotate(small, cv2.ROTATE_90_CLOCKWISE)\n    plt.figure(figsize=(8, 4))\n    plt.subplot(1, 2, 1)\n    plt.imshow(small, cmap='gray')\n    plt.title('Resized (320x240)')\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(rotated, cmap='gray')\n    plt.title('Rotated 90 deg')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nSmoothing / Blur — Purpose, Background, and Sharpening\nBlurring (smoothing) is a fundamental image processing technique that reduces high-frequency noise and small details. It is commonly used before edge detection, thresholding, or segmentation to suppress speckle and minor artifacts, making features easier to analyze.\n\nWhy Blur?\n\nNoise reduction: Removes random pixel fluctuations and small unwanted details.\nPreprocessing: Improves the reliability of subsequent steps like edge detection and binarization.\nVisual effect: Produces a softer, less detailed image.\n\nA Gaussian blur uses a weighted kernel (must be odd-sized, e.g., 3x3, 5x5, 11x11) to average pixel values, giving more weight to the center. Larger kernels produce stronger smoothing.\n\n\nSharpening\nSharpening enhances edges and fine details, making features stand out. It is often used after blurring or on its own to improve image clarity.\n\nHow it works: Sharpening applies a kernel that emphasizes differences between neighboring pixels, boosting contrast at edges.\nCommon method: The Laplacian or unsharp mask filter.\n\nTip:\nUse blurring to clean up noise before analysis. Use sharpening to highlight boundaries and details for visualization or feature extraction.\nExample kernels: - Gaussian blur:\n[[1, 2, 1],      [2, 4, 2],      [1, 2, 1]] / 16 - Sharpening:\n[[ 0, -1,  0],      [-1,  5, -1],      [ 0, -1,  0]]\n\nif img is not None:\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Original Grayscale')\n    plt.axis('off')\n\n    # Gaussian blur with 11x11 kernel and sigma=0\n    blurred = cv2.GaussianBlur(gray, (11, 11), 0)\n    plt.subplot(1, 3, 2)\n    plt.imshow(blurred, cmap='gray')\n    plt.title('Blurred (Gaussian)')\n    plt.axis('off')\n\n    # Sharpening kernel\n    sharpen_kernel = np.array([[0, -1, 0],\n                               [-1, 5, -1],\n                               [0, -1, 0]])\n    sharpened = cv2.filter2D(gray, -1, sharpen_kernel)\n    plt.subplot(1, 3, 3)\n    plt.imshow(sharpened, cmap='gray')\n    plt.title('Sharpened')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nHistograms — Background & Intuition\nAn image histogram is a graphical representation of the distribution of pixel intensities (brightness or color values) in an image. It helps you understand the overall exposure, contrast, and color balance at a glance.\n\nWhat does a histogram show?\n\nX-axis: Pixel intensity values (0–255 for 8-bit images).\nY-axis: Number of pixels at each intensity.\nGrayscale images: One histogram for brightness.\nColor images: Separate histograms for each channel (Red, Green, Blue).\n\n\n\nWhy are histograms useful?\n\nContrast: A wide histogram means high contrast; a narrow one means low contrast.\nBrightness: If the histogram is shifted left, the image is dark; shifted right, it’s bright.\nExposure: Peaks at the ends may indicate underexposure (too dark) or overexposure (too bright).\nColor balance: Comparing channel histograms reveals color casts or imbalances.\n\n\n\nPractical uses\n\nImage enhancement: Adjust brightness/contrast or apply histogram equalization.\nThresholding: Choose thresholds for binarization based on histogram shape.\nQuality control: Detect poor lighting or exposure problems.\n\nHistograms are a simple but powerful tool for diagnosing and improving images in any image processing workflow.\n\n# Intensity and channel histograms\nif img is not None:\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 3, 1)\n    plt.hist(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).ravel(), bins=256)\n    plt.title('Intensity histogram')\n    plt.subplot(1, 3, 2)\n    plt.hist(r.ravel(), bins=256, color='r')\n    plt.title('Red hist')\n    plt.subplot(1, 3, 3)\n    plt.hist(g.ravel(), bins=256, color='g')\n    plt.title('Green hist')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nHistogram-based Image Processing\nHistogram-based image processing uses the distribution of pixel intensities to analyze and enhance images. By examining the histogram, we can adjust brightness, contrast, and exposure, detect features, and segment regions. This approach is essential because it provides a quantitative way to understand image quality and apply targeted corrections, making images more useful for visualization and further analysis.\nCLAHE (Contrast Limited Adaptive Histogram Equalization) is an advanced method for improving image contrast, especially in images with varying lighting or local features. Unlike standard histogram equalization, which adjusts contrast globally, CLAHE works on small regions (tiles) of the image and limits amplification to avoid noise.\n\nStandard Histogram Equalization\n\nGlobal adjustment: Redistributes pixel intensities across the entire image.\nBest for: Images with uniform lighting and global low contrast.\nDrawbacks: Can over-amplify noise and create unnatural effects in areas with little variation.\n\n\n\nCLAHE\n\nLocal adjustment: Applies histogram equalization to small tiles, then combines them.\nContrast limiting: Prevents over-amplification of noise by clipping the histogram.\nBest for: Medical images, uneven lighting, or images with both bright and dark regions.\nAdvantages: Preserves local details, avoids noise amplification, and produces more natural results.\n\nSummary:\nUse standard histogram equalization for quick global contrast enhancement. Use CLAHE for images with local contrast issues, uneven illumination, or when you want to avoid boosting noise.\n\n# Histogram equalization and CLAHE comparison (grayscale, LAB luminance)\nif img is not None:\n    # Prepare processed images\n    gray_eq = cv2.equalizeHist(gray)\n    gray_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    img_clahe = gray_clahe.apply(gray)\n\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    l_eq = cv2.equalizeHist(l)\n    lab_eq = cv2.merge((l_eq, a, b))\n    lab_eq_rgb = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2RGB)\n    l_clahe = gray_clahe.apply(l)\n    lab_clahe = cv2.merge((l_clahe, a, b))\n    lab_clahe_rgb = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n\n    # Figure 1: Images (2 rows x 3 cols)\n    plt.figure(figsize=(18, 8))\n    plt.subplot(2, 3, 1)\n    plt.imshow(gray, cmap='gray')\n    plt.title('Original Grayscale')\n    plt.axis('off')\n    plt.subplot(2, 3, 2)\n    plt.imshow(gray_eq, cmap='gray')\n    plt.title('Histogram Equalized')\n    plt.axis('off')\n    plt.subplot(2, 3, 3)\n    plt.imshow(img_clahe, cmap='gray')\n    plt.title('CLAHE Grayscale')\n    plt.axis('off')\n\n    plt.subplot(2, 3, 4)\n    plt.imshow(img_rgb)\n    plt.title('Original Color (RGB)')\n    plt.axis('off')\n    plt.subplot(2, 3, 5)\n    plt.imshow(lab_eq_rgb)\n    plt.title('Histogram Equalized LAB Luminance')\n    plt.axis('off')\n    plt.subplot(2, 3, 6)\n    plt.imshow(lab_clahe_rgb)\n    plt.title('CLAHE LAB Luminance')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n    # Figure 2: Histograms corresponding to the images (2 rows x 3 cols)\n    plt.figure(figsize=(18, 8))\n    plt.subplot(2, 3, 1)\n    plt.hist(gray.ravel(), bins=256, color='gray')\n    plt.title('Original Grayscale Hist')\n    plt.subplot(2, 3, 2)\n    plt.hist(gray_eq.ravel(), bins=256, color='blue')\n    plt.title('Hist Eq Grayscale Hist')\n    plt.subplot(2, 3, 3)\n    plt.hist(img_clahe.ravel(), bins=256, color='red')\n    plt.title('CLAHE Grayscale Hist')\n\n    plt.subplot(2, 3, 4)\n    # histogram for original LAB luminance channel\n    plt.hist(l.ravel(), bins=256, color='gray', alpha=0.7, label='Luminance')\n    plt.title('Original LAB Luminance Hist')\n    plt.ylim(0, 30000)\n    plt.subplot(2, 3, 5)\n    plt.hist(l_eq.ravel(), bins=256, color='purple')\n    plt.title('Histogram Equalized LAB Luminance')\n    plt.subplot(2, 3, 6)\n    plt.hist(l_clahe.ravel(), bins=256, color='orange')\n    plt.title('CLAHE Lab Luminance')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinarization\nBinarization is the process of converting a grayscale image into a binary image, where each pixel is either black (0) or white (255). This is done by applying a threshold: pixels above the threshold become white, and those below become black.\n\nWhy Use Binarization?\n\nSimplifies analysis: Many image processing tasks (like shape analysis, object counting, and OCR) work better on binary images.\nSeparates foreground from background: Useful for segmenting objects from their surroundings.\nPreprocessing for algorithms: Many algorithms (e.g., contour detection, morphological operations) require binary input.\n\n\n\nHow Does Binarization Work?\n\nChoose a threshold value (e.g., 100).\nCompare each pixel’s intensity to the threshold.\n\nIf the pixel value &gt; threshold, set it to 255 (white).\nIf the pixel value ≤ threshold, set it to 0 (black).\n\n\n\n\nOtsu’s Method — Automatic Threshold Selection\nOtsu’s method is a popular technique for automatically finding the optimal threshold value. It works by: - Analyzing the histogram of pixel intensities. - Finding the threshold that minimizes the variance within each class (foreground and background), or equivalently, maximizes the separation between them.\nAdvantages: - No manual tuning needed. - Works well when the image has a clear bimodal histogram (two peaks: one for background, one for foreground).\nIn practice:\nOtsu’s method is widely used for document scanning, medical imaging, and any scenario where robust, automatic binarization is needed.\n\nif img is not None:\n    # Show histogram\n    plt.figure(figsize=(10, 4))\n    plt.hist(gray.ravel(), bins=256, color='gray')\n    plt.title('Grayscale Histogram')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    # Otsu's thresholding\n    otsu_thresh, binary_img = cv2.threshold(\n        gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    plt.axvline(otsu_thresh, color='red', linestyle='dashed',\n                label=f'Otsu Threshold = {otsu_thresh:.0f}')\n    plt.legend()\n    plt.show()\n    # Show binarized image\n    plt.imshow(binary_img, cmap='gray')\n    plt.title('Binarized Image (Otsu Threshold)')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMorphological Operations\nMorphological operations are image processing techniques that probe and modify the shapes of objects in binary or grayscale images. The two most common operations are erosion and dilation.\n\nWhat is Erosion?\n\nPurpose: Erosion shrinks bright regions and removes small white noise. It is useful for eliminating tiny artifacts, separating objects that are close together, and reducing the size of foreground objects.\nHow it works: Erosion slides a small shape (called a structuring element or kernel) over the image. At each position, if all pixels under the kernel are bright (e.g., white in binary images), the output pixel remains bright; otherwise, it becomes dark. This causes boundaries of bright regions to shrink.\n\n\n\nWhat is Dilation?\n\nPurpose: Dilation expands bright regions and fills small holes or gaps. It is useful for joining broken parts of objects, making features thicker, and connecting nearby objects.\nHow it works: Dilation also slides a kernel over the image. At each position, if any pixel under the kernel is bright, the output pixel becomes bright. This causes boundaries of bright regions to grow outward.\n\n\n\nPractical Use\n\nNoise removal: Erosion followed by dilation (called opening) removes small noise while preserving object shape.\nObject joining: Dilation followed by erosion (called closing) fills small holes and connects nearby objects.\nParameter tuning: The size and shape of the kernel control the strength and direction of the effect. Larger kernels produce stronger changes.\n\nTip: Try different kernel sizes and shapes to see how they affect your image. Use erosion to clean up noise, and dilation to restore or connect features.\n\n# Erosion and dilation on binarized image\nif img is not None:\n    plt.figure(figsize=(15, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(binary_img, cmap='gray')\n    plt.title('Original Binary')\n    plt.axis('off')\n\n    # 15x15 kernel for demo. Adjust size as needed.\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n\n    eroded_bin = cv2.erode(binary_img, kernel, iterations=1)\n    dilated_bin = cv2.dilate(binary_img, kernel, iterations=1)\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(eroded_bin, cmap='gray')\n    plt.title('Erosion (15x15)')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(dilated_bin, cmap='gray')\n    plt.title('Dilation (15x15)')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nEdge Detection\nEdge detection is a fundamental technique in image processing and computer vision. Edges represent boundaries where pixel intensities change sharply, often corresponding to object outlines, texture changes, or surface discontinuities.\n\nWhy Detect Edges?\n\nObject boundaries: Edges help segment objects from the background.\nFeature extraction: Many algorithms use edges to identify shapes, corners, and regions of interest.\nImage understanding: Edges simplify images, making it easier to analyze and interpret content.\n\n\n\nHow Does Edge Detection Work?\nEdge detectors analyze local intensity changes in an image. They highlight pixels where the difference between neighboring values is large. Common approaches include: - Gradient-based methods: Compute the rate of change (gradient) in intensity. Examples: Sobel, Prewitt, Roberts. - Canny edge detector: A multi-stage algorithm that smooths the image, finds gradients, applies non-maximum suppression, and uses double thresholding to select strong and weak edges.\n\n\nCanny Edge Detector\nThe Canny method is popular because it produces clean, thin edges and reduces noise. It uses two thresholds: - Low threshold: Detects weak edges. - High threshold: Detects strong edges. Edges connected to strong edges are kept; isolated weak edges are discarded.\nTip: Adjust thresholds to control edge sensitivity. Lower values reveal more edges (including noise); higher values show only the most prominent boundaries.\nEdge detection is a key step for tasks like segmentation, tracking, and recognition.\n\n# Compare Sobel gradient (simpler) with Canny edge detection (more complex)\nif img is not None:\n    # Sobel gradient (X and Y)\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n    sobel_mag = cv2.magnitude(sobelx, sobely)\n    sobel_mag = cv2.convertScaleAbs(sobel_mag)\n\n    # Canny edge detection\n    edges_canny = cv2.Canny(gray, 200, 400)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(sobel_mag, cmap='gray')\n    plt.title('Sobel Gradient Magnitude')\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(edges_canny, cmap='gray')\n    plt.title('Canny Edge Detection')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\n\n\n\n\n\n\n\nObject Detection and Face Detection\nModern image processing leverages deep learning models to automatically detect and localize objects and faces in images and videos. Two popular approaches are YOLO (You Only Look Once) for general object detection and MediaPipe Face Detection for fast, lightweight face localization.\n\nYOLO Object Detection\n\nYOLO is a family of real-time object detectors that predict bounding boxes and class labels for multiple objects in a single pass through the image.\nHow it works: YOLO divides the image into a grid and simultaneously predicts bounding boxes and class probabilities for each cell.\nAdvantages: Fast, accurate, and suitable for real-time applications such as video analysis, robotics, and surveillance.\nUse cases: Detecting people, vehicles, animals, and everyday objects in images or video frames.\n\n\n# Step 3: Object Detection with YOLO\nfrom ultralytics import YOLO\n\n# The YOLO model loaded above (yolo_model) is a pre-trained deep learning object detector from the Ultralytics YOLO family.\n# YOLO (\"You Only Look Once\") models are designed for real-time object detection in images and videos.\n# They predict bounding boxes and class labels for multiple objects in a single forward pass.\n# The model file 'yolo11n.pt' is a specific version/size of YOLO.\n\nyolo_model = YOLO('yolo11n.pt')\n\n# Load the first frame of the video file and use it as the image\nvideo_path = 'GX010881.MP4'  # Change to your video file name if needed\ncap = cv2.VideoCapture(video_path)\nret, frame = cap.read()\ncap.release()\nif ret:\n    img = frame\n    # Run YOLO detection on the first frame\n    results = yolo_model(img)\n    img_detected = img.copy()\n    for result in results:\n        for box in result.boxes:\n            class_id = int(box.cls[0])\n            confidence = float(box.conf[0])\n            class_name = yolo_model.names[class_id]\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\n            if confidence &gt; 0.2:\n                cv2.rectangle(img_detected, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(img_detected, f'{class_name} {confidence:.2f}',\n                            (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n    img_detected_rgb = cv2.cvtColor(img_detected, cv2.COLOR_BGR2RGB)\n    plt.imshow(img_detected_rgb)\n    plt.title('YOLO Object Detection (First Video Frame)')\n    plt.axis('off')\n    plt.show()\nelse:\n    print('Could not load first frame from video.')\n\n\n\nMediaPipe Face Detection\n\nMediaPipe is a framework from Google for building cross-platform ML pipelines. Its face detection model is designed for real-time applications and works well on both close-up and wide-angle scenes.\nHow it works: The model uses machine learning to find faces and estimate their bounding boxes, returning confidence scores for each detection.\nAdvantages: Lightweight, fast, and robust to variations in pose and lighting.\nUse cases: Face tracking, privacy blurring, selfie enhancement, and interactive applications.\n\n\n# Step 4: Face Detection with MediaPipe\nimport mediapipe as mp\n\n# The MediaPipe Face Detection model is a lightweight, real-time face detector from Google.\n# It uses machine learning to find faces and estimate their bounding boxes in images or video frames.\n# The model is fast and works well for both close-up selfies and wider scenes, depending on the model_selection parameter.\n# It outputs detection objects with bounding box coordinates and confidence scores.\nmp_face_detection = mp.solutions.face_detection\n\nif img is not None:\n    # Tip: model_selection=0 is tuned for close-up faces (e.g., selfies); model_selection=1 is for wider scenes.\n    # Tip: increase min_detection_confidence (0.0-1.0) to reduce false positives at the cost of missing faint faces.\n    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        results = face_detection.process(img_rgb)\n        img_faces = img_rgb.copy()\n        h_img, w_img = img_faces.shape[:2]\n        if results.detections:\n            for detection in results.detections:\n                bboxC = detection.location_data.relative_bounding_box\n                x = int(bboxC.xmin * w_img)\n                y = int(bboxC.ymin * h_img)\n                w = int(bboxC.width * w_img)\n                h = int(bboxC.height * h_img)\n                cv2.rectangle(img_faces, (x, y),\n                              (x + w, y + h), (255, 0, 0), 2)\n            plt.imshow(img_faces)\n            plt.title('MediaPipe Face Detection')\n            plt.axis('off')\n            plt.show()\n            # Note: each detection object has a score (detection.score). You can filter detections by inspecting that value.\n            print(f'Faces detected: {len(results.detections)}')\n        else:\n            print('No faces detected.')\nelse:\n    print('Image not loaded. Run the first cell.')\n\n\n\n\nVideo Processing: Object and Face Detection with Privacy Blurring\nThe next cell defines and runs a function called process_video that processes a video file frame-by-frame. For each frame, it performs:\n\nObject Detection (YOLO):\nUses the YOLO model to detect people in the frame and draws green rectangles around them.\nFace Detection (MediaPipe):\nUses MediaPipe to detect faces, draws blue rectangles around each detected face, and applies a blur to the face region for privacy protection.\nInteractive Display:\nThe processed frames are displayed interactively in the notebook, allowing you to watch the detection and blurring in real time.\nRobust Face Tracking:\nIf a face is missed in a frame, the function keeps the last known face location for a few frames to maintain privacy blurring even during brief detection failures.\n\nThis workflow demonstrates how to combine deep learning-based object detection and lightweight face detection for privacy-aware video analysis in Python.\n\nfrom IPython.display import display, clear_output\nimport contextlib\nimport os\nfrom ultralytics import YOLO\n\n\ndef _get_valid_ksize(fw, fh, base=51):\n    ksize = min(fw, fh, base)\n    ksize = max(ksize, 3)\n    if ksize % 2 == 0:\n        ksize += 1\n    return int(ksize)\n\n\ndef process_video(video_path, max_frames=30, max_missed=5):\n    yolo_model = YOLO('yolo11n.pt')\n    mp_face_detection = mp.solutions.face_detection\n    cap = cv2.VideoCapture(video_path)\n    last_face_boxes = []\n    missed_frames = 0\n    frames_shown = 0\n    fig, ax = plt.subplots(figsize=(10, 6))\n    img_plot = None\n    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n        while cap.isOpened() and frames_shown &lt; max_frames:\n            ret, frame = cap.read()\n            if not ret:\n                print('End of video or cannot read the frame.')\n                break\n            # YOLO person detection (suppress output)\n            with open(os.devnull, 'w') as fnull, contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n                results = yolo_model(frame, verbose=False)\n            for result in results:\n                for box in result.boxes:\n                    class_id = int(box.cls[0])\n                    confidence = float(box.conf[0])\n                    class_name = yolo_model.names[class_id]\n                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n                    if confidence &gt; 0.5 and class_name == 'person':\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (0, 255, 0), 2)\n                        cv2.putText(frame, f'Person {confidence:.2f}', (\n                            x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n            # MediaPipe face detection\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            results = face_detection.process(rgb_frame)\n            h_img, w_img = frame.shape[:2]\n            current_face_boxes = []\n            if results.detections:\n                for detection in results.detections:\n                    bboxC = detection.location_data.relative_bounding_box\n                    x = int(bboxC.xmin * w_img)\n                    y = int(bboxC.ymin * h_img)\n                    w = int(bboxC.width * w_img)\n                    h = int(bboxC.height * h_img)\n                    x1 = max(x, 0)\n                    y1 = max(y, 0)\n                    x2 = min(x + w, w_img)\n                    y2 = min(y + h, h_img)\n                    current_face_boxes.append((x1, y1, x2, y2))\n            if current_face_boxes:\n                last_face_boxes = current_face_boxes\n                missed_frames = 0\n            else:\n                missed_frames += 1\n            # Draw blue rectangle, add text, and blur face for each detected face\n            if last_face_boxes and missed_frames &lt;= max_missed:\n                for (x1, y1, x2, y2) in last_face_boxes:\n                    face_roi = frame[y1:y2, x1:x2]\n                    fh, fw = face_roi.shape[:2]\n                    ksize = _get_valid_ksize(\n                        fw, fh, base=51 + 10 * missed_frames)\n                    if face_roi.size &gt; 0 and ksize &gt; 1:\n                        blurred_face = cv2.GaussianBlur(\n                            face_roi, (ksize, ksize), 0)\n                        frame[y1:y2, x1:x2] = blurred_face\n                        cv2.rectangle(frame, (x1, y1),\n                                      (x2, y2), (255, 0, 0), 2)\n                        cv2.putText(frame, 'Face', (x1, y1-10),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n            # Update the image plot\n            clear_output(wait=True)\n            if img_plot is None:\n                img_plot = ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                ax.axis('off')\n            else:\n                img_plot.set_data(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                fig.canvas.draw_idle()\n            display(plt.gcf())\n            frames_shown += 1\n    cap.release()\n\n\nprocess_video('test_video.MP4')  # Change to your video file name"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Griffin C. Sipes",
    "section": "",
    "text": "PhD student in Mechanical Engineering (UIUC). My biomechanics research focuses on using wearable sensors and computer vision for analyzing adapted exercise."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Griffin C. Sipes",
    "section": "",
    "text": "PhD student in Mechanical Engineering (UIUC). My biomechanics research focuses on using wearable sensors and computer vision for analyzing adapted exercise."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Griffin C. Sipes",
    "section": "Quick Links",
    "text": "Quick Links\n\nCV (PDF): add files/CV.pdf and I will link it here.\nProjects: projects.qmd and tutorials.qmd (auto-generated lists)\nNotebooks: notebooks/ (open in Colab when available)\nPublications: add publications.qmd or references.bib to populate."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Griffin C. Sipes",
    "section": "Contact",
    "text": "Contact\n\nEmail: gsipes2@illinois.edu\nGitHub: https://github.com/gsipes2"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Open IntroductionImageProcessing in Colab\n\nThis site can include Jupyter notebooks (.ipynb) in two common ways.\n\nRender the notebook as a standalone page using Quarto\n\nPlace notebooks in a notebooks/ folder (e.g. notebooks/example.ipynb).\nRender it with quarto render notebooks/example.ipynb which will produce an HTML page in _site/ (or the site output folder).\nLink to the produced HTML from your site navigation or pages.\n\nInclude notebook content inside a .qmd page (convert or execute)\n\nYou can include the notebook output by converting it to Quarto-compatible content or by embedding the HTML output.\nExample: in a .qmd file you can use an iframe to embed the rendered notebook HTML:\n\n\n&lt;iframe src=\"notebooks/example.html\" style=\"width:100%;height:800px;border:0\"&gt;&lt;/iframe&gt;\nTips:\n\nUse quarto render to convert .ipynb to HTML (Quarto will execute notebooks when rendering if execution is enabled).\nEnsure any data or resources used by the notebook are committed or in paths resolvable from the site root.\nIf you prefer dynamic execution during site builds, enable execute: true in the notebook’s YAML or in _quarto.yml.\n\nIf you want, I can also add a small example notebooks/example.ipynb and a build script to automatically render notebooks during site build. title: Notebooks title: Notebooks format: html\n\n\nOpen IntroductionImageProcessing in Colab Launch on Binder Open in JupyterLite (if available)\n\n\n\nOpen IntroductionImageProcessing in Colab\n\nThis site can include Jupyter notebooks (.ipynb) in two common ways.\n\nRender the notebook as a standalone page using Quarto\n\nPlace notebooks in a notebooks/ folder (e.g. notebooks/example.ipynb).\nRender it with quarto render notebooks/example.ipynb which will produce an HTML page in _site/ (or the site output folder).\nLink to the produced HTML from your site navigation or pages.\n\nInclude notebook content inside a .qmd page (convert or execute)\n\nYou can include the notebook output by converting it to Quarto-compatible content or by embedding the HTML output.\nExample: in a .qmd file you can use an iframe to embed the rendered notebook HTML:\n\n\n&lt;iframe src=\"notebooks/example.html\" style=\"width:100%;height:800px;border:0\"&gt;&lt;/iframe&gt;\nTips:\n\nUse quarto render to convert .ipynb to HTML (Quarto will execute notebooks when rendering if execution is enabled).\nEnsure any data or resources used by the notebook are committed or in paths resolvable from the site root.\nIf you prefer dynamic execution during site builds, enable execute: true in the notebook’s YAML or in _quarto.yml.\n\nIf you want, I can also add a small example notebooks/example.ipynb and a build script to automatically render notebooks during site build."
  },
  {
    "objectID": "README-PORTFOLIO.html",
    "href": "README-PORTFOLIO.html",
    "title": "Griffin Sipes's Portfolio",
    "section": "",
    "text": "Quick Next Steps — Portfolio Site\n\nPreview locally:\n\nquarto preview\n\nBuild site:\n\nquarto render\n\nAdd a CV:\n\nPlace CV.pdf in files/ and update index.qmd link to files/CV.pdf.\n\nAdd ORCID / social links:\n\nEdit index.qmd and add badges or links.\n\nProjects and Notebooks:\n\nUpdate tutorials.qmd, projects.qmd, or add project .qmd pages.\nNotebooks live in notebooks/ and can be opened in Colab where available.\n\nPublish via GitHub Pages (current repo uses docs/):\n\nIf you pushed _site/ to docs/, set Pages source to main / docs in repository settings.\n\n\nIf you want, I can add a projects grid, publication bibliography, or a downloadable CV and wire up social badges."
  }
]